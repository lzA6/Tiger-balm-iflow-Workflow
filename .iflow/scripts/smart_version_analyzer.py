#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½ç‰ˆæœ¬åˆ†æå™¨ - åˆ†æå„ç‰ˆæœ¬åŠŸèƒ½ï¼Œæ‹©ä¼˜ä¿ç•™å¹¶åˆå¹¶
ä½ ä¸€å®šè¦è¶…çº§æ€è€ƒã€æé™æ€è€ƒã€æ·±åº¦æ€è€ƒï¼Œå…¨åŠ›æ€è€ƒã€è¶…å¼ºæ€è€ƒï¼Œè®¤çœŸä»”ç»†æ€è€ƒï¼ˆultrathinkã€think really super hardã€think intenselyï¼‰ã€‚
"""

import os
import sys
import re
import json
from pathlib import Path
from typing import Dict, List, Tuple, Any
import hashlib

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

class SmartVersionAnalyzer:
    """æ™ºèƒ½ç‰ˆæœ¬åˆ†æå™¨"""
    
    def __init__(self):
        self.project_root = PROJECT_ROOT
        self.analysis_results = {}
        self.merge_recommendations = {}
        
    def scan_version_groups(self) -> Dict[str, List[str]]:
        """æ‰«æç‰ˆæœ¬ç»„"""
        version_patterns = [
            r'(.+?)(_v|-v)(\d+)\.py$',
            r'(.+?)(v\d+)\.py$',
            r'(.+?)(_v|-v)(\d+)\.md$'
        ]
        
        version_groups = {}
        
        # é€’å½’æ‰«ææ‰€æœ‰Pythonå’ŒMarkdownæ–‡ä»¶
        for file_path in self.project_root.rglob('*.py'):
            relative_path = file_path.relative_to(self.project_root)
            for pattern in version_patterns:
                match = re.match(pattern, file_path.name)
                if match:
                    base_name = match.group(1)
                    version_key = f"{relative_path.parent}/{base_name}"
                    if version_key not in version_groups:
                        version_groups[version_key] = []
                    version_groups[version_key].append(str(relative_path))
                    break
        
        # æ‰«æMarkdownæ–‡ä»¶
        for file_path in self.project_root.rglob('*.md'):
            relative_path = file_path.relative_to(self.project_root)
            for pattern in version_patterns:
                match = re.match(pattern, file_path.name)
                if match:
                    base_name = match.group(1)
                    version_key = f"{relative_path.parent}/{base_name}"
                    if version_key not in version_groups:
                        version_groups[version_key] = []
                    version_groups[version_key].append(str(relative_path))
                    break
        
        return version_groups
    
    def analyze_file_content(self, file_path: str) -> Dict[str, Any]:
        """åˆ†ææ–‡ä»¶å†…å®¹"""
        full_path = self.project_root / file_path
        if not full_path.exists():
            return {"error": "æ–‡ä»¶ä¸å­˜åœ¨"}
        
        try:
            with open(full_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except Exception as e:
            return {"error": f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}"}
        
        # è®¡ç®—æ–‡ä»¶ç‰¹å¾
        features = {
            "file_size": len(content),
            "line_count": len(content.splitlines()),
            "function_count": len(re.findall(r'^\s*def\s+\w+', content, re.MULTILINE)),
            "class_count": len(re.findall(r'^\s*class\s+\w+', content, re.MULTILINE)),
            "import_count": len(re.findall(r'^\s*import\s+|^from\s+.*import', content, re.MULTILINE)),
            "comment_count": len(re.findall(r'#.*$|""".*?"""|\'\'\'.*?\'\'\'', content, re.DOTALL)),
            "version_info": self.extract_version_info(content),
            "last_modified": full_path.stat().st_mtime,
            "content_hash": hashlib.md5(content.encode()).hexdigest(),
            "complexity_score": self.calculate_complexity(content),
            "feature_keywords": self.extract_feature_keywords(content)
        }
        
        return features
    
    def extract_version_info(self, content: str) -> Dict[str, str]:
        """æå–ç‰ˆæœ¬ä¿¡æ¯"""
        version_patterns = [
            r'version\s*[=:]\s*["\']?(\d+\.\d+(?:\.\d+)?)["\']?',
            r'__version__\s*=\s*["\']?(\d+\.\d+(?:\.\d+)?)["\']?',
            r'#\s*ç‰ˆæœ¬\s*:?(\d+\.\d+(?:\.\d+)?)',
            r'#\s*v?ersion\s*:?(\d+)',
        ]
        
        version_info = {}
        for pattern in version_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            if matches:
                version_info[pattern] = matches[0]
        
        return version_info
    
    def calculate_complexity(self, content: str) -> float:
        """è®¡ç®—ä»£ç å¤æ‚åº¦"""
        # ç®€å•çš„å¤æ‚åº¦è®¡ç®—
        complexity_factors = {
            "if_count": len(re.findall(r'\bif\b', content)),
            "for_count": len(re.findall(r'\bfor\b', content)),
            "while_count": len(re.findall(r'\bwhile\b', content)),
            "try_count": len(re.findall(r'\btry\b', content)),
            "except_count": len(re.findall(r'\bexcept\b', content)),
            "function_def_count": len(re.findall(r'\bdef\s+\w+', content)),
            "class_def_count": len(re.findall(r'\bclass\s+\w+', content)),
        }
        
        # åŠ æƒè®¡ç®—å¤æ‚åº¦
        weights = {
            "if_count": 1,
            "for_count": 2,
            "while_count": 2,
            "try_count": 1,
            "except_count": 2,
            "function_def_count": 0.5,
            "class_def_count": 1,
        }
        
        total_complexity = sum(
            complexity_factors.get(key, 0) * weight
            for key, weight in weights.items()
        )
        
        # å½’ä¸€åŒ–åˆ°0-10èŒƒå›´
        normalized_complexity = min(total_complexity / 10, 10.0)
        return round(normalized_complexity, 2)
    
    def extract_feature_keywords(self, content: str) -> List[str]:
        """æå–åŠŸèƒ½å…³é”®è¯"""
        keywords = [
            "ARQ", "consciousness", "workflow", "adapter", "agent", 
            "quantum", "optimization", "security", "performance",
            "cache", "hook", "test", "analysis", "evolution"
        ]
        
        found_keywords = []
        content_lower = content.lower()
        for keyword in keywords:
            if keyword.lower() in content_lower:
                found_keywords.append(keyword)
        
        return found_keywords
    
    def compare_versions(self, file_group: List[str]) -> Dict[str, Any]:
        """æ¯”è¾ƒç‰ˆæœ¬"""
        analysis = {}
        for file_path in file_group:
            analysis[file_path] = self.analyze_file_content(file_path)
        
        # æ‰¾å‡ºæœ€ä½³ç‰ˆæœ¬
        best_file = None
        best_score = 0
        
        for file_path, features in analysis.items():
            if "error" in features:
                continue
                
            # è®¡ç®—ç»¼åˆè¯„åˆ†
            score = (
                features["line_count"] * 0.1 +  # ä»£ç é‡
                features["function_count"] * 2 +  # å‡½æ•°æ•°é‡
                features["class_count"] * 3 +  # ç±»æ•°é‡
                features["complexity_score"] * 5 +  # å¤æ‚åº¦æƒé‡
                len(features["feature_keywords"]) * 2 +  # åŠŸèƒ½å…³é”®è¯
                features["import_count"] * 0.5  # å¯¼å…¥æ•°é‡
            )
            
            # ç‰ˆæœ¬å·æƒé‡
            version_match = re.search(r'v(\d+)', file_path)
            if version_match:
                score += int(version_match.group(1)) * 2
            
            if score > best_score:
                best_score = score
                best_file = file_path
        
        return {
            "analysis": analysis,
            "best_file": best_file,
            "best_score": best_score,
            "recommendation": self.generate_recommendation(file_group, analysis, best_file)
        }
    
    def generate_recommendation(self, file_group: List[str], analysis: Dict, best_file: str) -> Dict[str, Any]:
        """ç”Ÿæˆæ¨è"""
        if not best_file:
            return {"action": "manual_review", "reason": "æ— æ³•ç¡®å®šæœ€ä½³ç‰ˆæœ¬"}
        
        recommendations = []
        for file_path in file_group:
            if file_path == best_file:
                recommendations.append({
                    "file": file_path,
                    "action": "keep",
                    "reason": "ç»¼åˆè¯„åˆ†æœ€é«˜"
                })
            else:
                # æ£€æŸ¥æ˜¯å¦æœ‰ç‹¬ç‰¹åŠŸèƒ½
                unique_features = self.find_unique_features(file_path, best_file, analysis)
                if unique_features:
                    recommendations.append({
                        "file": file_path,
                        "action": "merge",
                        "reason": f"åŒ…å«ç‹¬ç‰¹åŠŸèƒ½: {', '.join(unique_features)}"
                    })
                else:
                    recommendations.append({
                        "file": file_path,
                        "action": "delete",
                        "reason": "åŠŸèƒ½å·²è¢«æœ€ä½³ç‰ˆæœ¬åŒ…å«"
                    })
        
        return {
            "recommendations": recommendations,
            "merge_suggestions": self.generate_merge_suggestions(file_group, analysis, best_file)
        }
    
    def find_unique_features(self, file1: str, file2: str, analysis: Dict) -> List[str]:
        """æŸ¥æ‰¾ç‹¬ç‰¹åŠŸèƒ½"""
        if file1 not in analysis or file2 not in analysis:
            return []
        
        features1 = analysis[file1].get("feature_keywords", [])
        features2 = analysis[file2].get("feature_keywords", [])
        
        # æ‰¾å‡ºfile1æœ‰ä½†file2æ²¡æœ‰çš„åŠŸèƒ½
        unique_features = [f for f in features1 if f not in features2]
        return unique_features
    
    def generate_merge_suggestions(self, file_group: List[str], analysis: Dict, best_file: str) -> List[Dict[str, Any]]:
        """ç”Ÿæˆåˆå¹¶å»ºè®®"""
        merge_suggestions = []
        
        for file_path in file_group:
            if file_path == best_file or file_path not in analysis:
                continue
            
            features = analysis[file_path]
            if "error" in features:
                continue
            
            # ç®€å•çš„å†…å®¹æ¯”è¾ƒ
            try:
                with open(self.project_root / file_path, 'r', encoding='utf-8') as f:
                    content1 = f.read()
                with open(self.project_root / best_file, 'r', encoding='utf-8') as f:
                    content2 = f.read()
                
                # æ‰¾å‡ºå·®å¼‚
                diff_analysis = self.analyze_content_differences(content1, content2)
                if diff_analysis["has_unique_content"]:
                    merge_suggestions.append({
                        "source_file": file_path,
                        "target_file": best_file,
                        "differences": diff_analysis["differences"],
                        "merge_strategy": "manual_review"
                    })
            except Exception as e:
                merge_suggestions.append({
                    "source_file": file_path,
                    "error": str(e)
                })
        
        return merge_suggestions
    
    def analyze_content_differences(self, content1: str, content2: str) -> Dict[str, Any]:
        """åˆ†æå†…å®¹å·®å¼‚"""
        lines1 = set(content1.splitlines())
        lines2 = set(content2.splitlines())
        
        unique_to_1 = lines1 - lines2
        unique_to_2 = lines2 - lines1
        
        has_unique = len(unique_to_1) > 0 or len(unique_to_2) > 0
        
        return {
            "has_unique_content": has_unique,
            "differences": {
                "unique_to_source": list(unique_to_1),
                "unique_to_target": list(unique_to_2),
                "unique_count_source": len(unique_to_1),
                "unique_count_target": len(unique_to_2)
            }
        }
    
    def run_analysis(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        print("[ANALYZER] å¼€å§‹æ™ºèƒ½ç‰ˆæœ¬åˆ†æ...")
    def run_analysis(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        print("ğŸ” å¼€å§‹æ™ºèƒ½ç‰ˆæœ¬åˆ†æ...")
        
        # æ‰«æç‰ˆæœ¬ç»„
        version_groups = self.scan_version_groups()
        print(f"ğŸ“ å‘ç° {len(version_groups)} ä¸ªç‰ˆæœ¬ç»„")
        
        analysis_results = {}
        for base_name, file_list in version_groups.items():
            print(f"\nğŸ“Š åˆ†æç‰ˆæœ¬ç»„: {base_name}")
            print(f"   æ–‡ä»¶åˆ—è¡¨: {', '.join(file_list)}")
            
            comparison = self.compare_versions(file_list)
            analysis_results[base_name] = comparison
            
            # è¾“å‡ºæ¨è
            recommendation = comparison.get("recommendation", {})
            if "recommendations" in recommendation:
                for rec in recommendation["recommendations"]:
                    action_emoji = "âœ…" if rec["action"] == "keep" else "ğŸ”„" if rec["action"] == "merge" else "ğŸ—‘ï¸"
                    print(f"   {action_emoji} {rec['file']} -> {rec['action']}: {rec['reason']}")
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        summary = self.generate_summary(analysis_results)
        
        return {
            "version_groups": version_groups,
            "analysis_results": analysis_results,
            "summary": summary
        }
    
    def generate_summary(self, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ€»ç»“"""
        total_files = 0
        keep_count = 0
        merge_count = 0
        delete_count = 0
        
        for base_name, result in analysis_results.items():
            recommendation = result.get("recommendation", {})
            if "recommendations" in recommendation:
                for rec in recommendation["recommendations"]:
                    total_files += 1
                    if rec["action"] == "keep":
                        keep_count += 1
                    elif rec["action"] == "merge":
                        merge_count += 1
                    elif rec["action"] == "delete":
                        delete_count += 1
        
        return {
            "total_version_groups": len(analysis_results),
            "total_files_analyzed": total_files,
            "files_to_keep": keep_count,
            "files_to_merge": merge_count,
            "files_to_delete": delete_count,
            "estimated_reduction": f"{delete_count}/{total_files} ({delete_count/total_files*100:.1f}%)"
        }

def main():
    """ä¸»å‡½æ•°"""
    analyzer = SmartVersionAnalyzer()
    results = analyzer.run_analysis()
    
    # ä¿å­˜ç»“æœ
    output_file = PROJECT_ROOT / "æ™ºèƒ½ç‰ˆæœ¬åˆ†ææŠ¥å‘Š_20251113.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    # è¾“å‡ºæ€»ç»“
    summary = results["summary"]
    print(f"\nğŸ“ˆ åˆ†ææ€»ç»“:")
    print(f"   ç‰ˆæœ¬ç»„æ•°é‡: {summary['total_version_groups']}")
    print(f"   åˆ†ææ–‡ä»¶æ•°: {summary['total_files_analyzed']}")
    print(f"   ä¿ç•™æ–‡ä»¶æ•°: {summary['files_to_keep']}")
    print(f"   åˆå¹¶æ–‡ä»¶æ•°: {summary['files_to_merge']}")
    print(f"   åˆ é™¤æ–‡ä»¶æ•°: {summary['files_to_delete']}")
    print(f"   é¢„ä¼°å‡å°‘: {summary['estimated_reduction']}")
    
    print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")

if __name__ == "__main__":
    main()
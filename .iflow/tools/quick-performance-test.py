#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
âš¡ å¿«é€Ÿæ€§èƒ½æµ‹è¯•ç³»ç»Ÿ
Quick Performance Test System

ä¸ºå…¨èƒ½å·¥ä½œæµV6æä¾›å¿«é€Ÿæ€§èƒ½éªŒè¯
"""

import asyncio
import time
import json
import psutil
import statistics
from pathlib import Path
from typing import Dict, List, Any
from datetime import datetime
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class QuickPerformanceTest:
    """å¿«é€Ÿæ€§èƒ½æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.root_dir = Path(__file__).parent.parent
        self.results = {}
        
    async def run_quick_test(self) -> Dict[str, Any]:
        """è¿è¡Œå¿«é€Ÿæ€§èƒ½æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹å¿«é€Ÿæ€§èƒ½æµ‹è¯•...")
        
        start_time = datetime.now()
        
        try:
            # åŸºç¡€ç³»ç»ŸæŒ‡æ ‡
            system_info = self._get_system_info()
            
            # CPUæ€§èƒ½æµ‹è¯•
            cpu_result = await self._test_cpu_performance()
            
            # å†…å­˜æ€§èƒ½æµ‹è¯•
            memory_result = await self._test_memory_performance()
            
            # æ–‡ä»¶I/Oæµ‹è¯•
            io_result = await self._test_io_performance()
            
            # å¹¶å‘æµ‹è¯•
            concurrent_result = await self._test_concurrent_performance()
            
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            # æ±‡æ€»ç»“æœ
            results = {
                "test_session_id": int(start_time.timestamp()),
                "start_time": start_time.isoformat(),
                "end_time": end_time.isoformat(),
                "duration_seconds": duration,
                "system_info": system_info,
                "test_results": {
                    "cpu": cpu_result,
                    "memory": memory_result,
                    "io": io_result,
                    "concurrent": concurrent_result
                },
                "overall_score": self._calculate_overall_score(cpu_result, memory_result, io_result, concurrent_result),
                "performance_grade": self._get_performance_grade(self._calculate_overall_score(cpu_result, memory_result, io_result, concurrent_result)),
                "recommendations": self._generate_recommendations(cpu_result, memory_result, io_result, concurrent_result)
            }
            
            # ä¿å­˜ç»“æœ
            await self._save_results(results)
            
            logger.info("âœ… å¿«é€Ÿæ€§èƒ½æµ‹è¯•å®Œæˆ!")
            return results
            
        except Exception as e:
            logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
            return {"error": str(e)}
    
    def _get_system_info(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        cpu_count = psutil.cpu_count()
        memory = psutil.virtual_memory()
        disk = psutil.disk_usage('/')
        
        return {
            "cpu_count": cpu_count,
            "memory_total_gb": memory.total / (1024**3),
            "memory_available_gb": memory.available / (1024**3),
            "memory_percent": memory.percent,
            "disk_total_gb": disk.total / (1024**3),
            "disk_free_gb": disk.free / (1024**3),
            "disk_percent": disk.percent
        }
    
    async def _test_cpu_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•CPUæ€§èƒ½"""
        logger.info("ğŸ”¥ æµ‹è¯•CPUæ€§èƒ½...")
        
        # ç®€å•è®¡ç®—æµ‹è¯•
        start_time = time.time()
        
        # æ‰§è¡Œè®¡ç®—å¯†é›†å‹ä»»åŠ¡
        total = 0
        for i in range(100000):
            total += i * i
        
        end_time = time.time()
        execution_time = (end_time - start_time) * 1000  # æ¯«ç§’
        
        # CPUä½¿ç”¨ç‡
        cpu_percent = psutil.cpu_percent(interval=0.1)
        
        return {
            "computation_time_ms": execution_time,
            "cpu_usage_percent": cpu_percent,
            "operations_per_second": 100000 / (execution_time / 1000) if execution_time > 0 else 0,
            "score": max(0, 100 - execution_time / 10)  # ç®€å•è¯„åˆ†
        }
    
    async def _test_memory_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•å†…å­˜æ€§èƒ½"""
        logger.info("ğŸ’¾ æµ‹è¯•å†…å­˜æ€§èƒ½...")
        
        # å†…å­˜åˆ†é…æµ‹è¯•
        start_time = time.time()
        
        # åˆ†é…å†…å­˜
        data = []
        for i in range(1000):
            data.append([0] * 100)  # åˆ†é…10000ä¸ªæ•´æ•°
        
        end_time = time.time()
        allocation_time = (end_time - start_time) * 1000
        
        # å†…å­˜ä½¿ç”¨
        process = psutil.Process()
        memory_mb = process.memory_info().rss / 1024 / 1024
        
        # å†…å­˜è®¿é—®æµ‹è¯•
        start_time = time.time()
        for row in data:
            _ = sum(row)
        end_time = time.time()
        access_time = (end_time - start_time) * 1000
        
        return {
            "allocation_time_ms": allocation_time,
            "access_time_ms": access_time,
            "memory_usage_mb": memory_mb,
            "elements_allocated": len(data) * 100,
            "score": max(0, 100 - memory_mb / 10)  # ç®€å•è¯„åˆ†
        }
    
    async def _test_io_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•æ–‡ä»¶I/Oæ€§èƒ½"""
        logger.info("ğŸ’¾ æµ‹è¯•æ–‡ä»¶I/Oæ€§èƒ½...")
        
        # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
        test_file = self.root_dir / "temp_test_file.txt"
        test_data = "æµ‹è¯•æ•°æ® " * 10000  # çº¦80KBæ•°æ®
        
        # å†™å…¥æµ‹è¯•
        start_time = time.time()
        with open(test_file, 'w', encoding='utf-8') as f:
            f.write(test_data)
        end_time = time.time()
        write_time = (end_time - start_time) * 1000
        
        # è¯»å–æµ‹è¯•
        start_time = time.time()
        with open(test_file, 'r', encoding='utf-8') as f:
            _ = f.read()
        end_time = time.time()
        read_time = (end_time - start_time) * 1000
        
        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        try:
            test_file.unlink()
        except:
            pass
        
        return {
            "write_time_ms": write_time,
            "read_time_ms": read_time,
            "data_size_bytes": len(test_data.encode('utf-8')),
            "write_throughput_mb_per_sec": (len(test_data.encode('utf-8')) / 1024 / 1024) / (write_time / 1000) if write_time > 0 else 0,
            "read_throughput_mb_per_sec": (len(test_data.encode('utf-8')) / 1024 / 1024) / (read_time / 1000) if read_time > 0 else 0,
            "score": max(0, 100 - (write_time + read_time) / 10)  # ç®€å•è¯„åˆ†
        }
    
    async def _test_concurrent_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•å¹¶å‘æ€§èƒ½"""
        logger.info("âš¡ æµ‹è¯•å¹¶å‘æ€§èƒ½...")
        
        async def simple_task(task_id: int):
            """ç®€å•ä»»åŠ¡"""
            start = time.time()
            # æ¨¡æ‹Ÿä¸€äº›å·¥ä½œ
            total = 0
            for i in range(1000):
                total += i
            await asyncio.sleep(0.001)  # 1msæ¨¡æ‹ŸI/O
            return {
                "task_id": task_id,
                "execution_time_ms": (time.time() - start) * 1000,
                "result": total
            }
        
        # å¹¶å‘æ‰§è¡Œä»»åŠ¡
        start_time = time.time()
        tasks = [simple_task(i) for i in range(10)]
        results = await asyncio.gather(*tasks)
        end_time = time.time()
        
        total_time = (end_time - start_time) * 1000
        execution_times = [r["execution_time_ms"] for r in results]
        
        return {
            "concurrent_tasks": len(tasks),
            "total_time_ms": total_time,
            "avg_task_time_ms": statistics.mean(execution_times),
            "max_task_time_ms": max(execution_times),
            "min_task_time_ms": min(execution_times),
            "throughput_tasks_per_sec": len(tasks) / (total_time / 1000) if total_time > 0 else 0,
            "score": max(0, 100 - total_time / 10)  # ç®€å•è¯„åˆ†
        }
    
    def _calculate_overall_score(self, cpu_result: Dict, memory_result: Dict, 
                                io_result: Dict, concurrent_result: Dict) -> float:
        """è®¡ç®—æ€»ä½“è¯„åˆ†"""
        scores = [
            cpu_result.get("score", 0),
            memory_result.get("score", 0),
            io_result.get("score", 0),
            concurrent_result.get("score", 0)
        ]
        return statistics.mean(scores) if scores else 0
    
    def _get_performance_grade(self, overall_score: float) -> str:
        """è·å–æ€§èƒ½ç­‰çº§"""
        if overall_score >= 90:
            return "A+ (ä¼˜ç§€)"
        elif overall_score >= 80:
            return "A (è‰¯å¥½)"
        elif overall_score >= 70:
            return "B (ä¸€èˆ¬)"
        elif overall_score >= 60:
            return "C (è¾ƒå·®)"
        else:
            return "D (éœ€è¦ä¼˜åŒ–)"
    
    def _generate_recommendations(self, cpu_result: Dict, memory_result: Dict, 
                                 io_result: Dict, concurrent_result: Dict) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # CPUå»ºè®®
        if cpu_result.get("computation_time_ms", 0) > 100:
            recommendations.append("CPUè®¡ç®—æ—¶é—´è¾ƒé•¿ï¼Œå»ºè®®ä¼˜åŒ–ç®—æ³•æˆ–å¢åŠ CPUèµ„æº")
        
        # å†…å­˜å»ºè®®
        if memory_result.get("memory_usage_mb", 0) > 100:
            recommendations.append("å†…å­˜ä½¿ç”¨è¾ƒé«˜ï¼Œå»ºè®®ä¼˜åŒ–å†…å­˜ç®¡ç†")
        
        # I/Oå»ºè®®
        if io_result.get("write_time_ms", 0) > 50 or io_result.get("read_time_ms", 0) > 50:
            recommendations.append("æ–‡ä»¶I/Oæ€§èƒ½è¾ƒæ…¢ï¼Œå»ºè®®ä½¿ç”¨æ›´å¿«çš„å­˜å‚¨è®¾å¤‡")
        
        # å¹¶å‘å»ºè®®
        if concurrent_result.get("throughput_tasks_per_sec", 0) < 100:
            recommendations.append("å¹¶å‘å¤„ç†èƒ½åŠ›æœ‰å¾…æå‡ï¼Œå»ºè®®ä¼˜åŒ–å¼‚æ­¥å¤„ç†é€»è¾‘")
        
        if not recommendations:
            recommendations.append("ç³»ç»Ÿæ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œç»§ç»­ä¿æŒå½“å‰é…ç½®")
        
        return recommendations
    
    async def _save_results(self, results: Dict[str, Any]) -> None:
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = self.root_dir / "benchmark_results"
        output_dir.mkdir(exist_ok=True)
        
        # ä¿å­˜JSONæ ¼å¼ç»“æœ
        timestamp = int(datetime.now().timestamp())
        json_file = output_dir / f"quick_performance_{timestamp}.json"
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“Š æµ‹è¯•ç»“æœå·²ä¿å­˜: {json_file}")
        
        # ä¿å­˜åˆ°resultså±æ€§ä¾›åç»­ä½¿ç”¨
        self.results = results.get("test_results", {})

async def main():
    """ä¸»å‡½æ•°"""
    print("âš¡ å¿«é€Ÿæ€§èƒ½æµ‹è¯•ç³»ç»Ÿ")
    print("=" * 50)
    
    # åˆ›å»ºæµ‹è¯•å™¨
    test = QuickPerformanceTest()
    
    try:
        # è¿è¡Œæµ‹è¯•
        results = await test.run_quick_test()
        
        print("\n" + "=" * 50)
        print("âœ… å¿«é€Ÿæ€§èƒ½æµ‹è¯•å®Œæˆ!")
        
        if "error" not in results:
            print(f"ğŸ“Š æ€»ä½“è¯„åˆ†: {results.get('overall_score', 0):.2f}")
            print(f"ğŸ† æ€§èƒ½ç­‰çº§: {results.get('performance_grade', 'Unknown')}")
            
            # æ˜¾ç¤ºå„é¡¹æµ‹è¯•ç»“æœ
            test_results = results.get("test_results", {})
            for test_name, test_data in test_results.items():
                print(f"\nğŸ” {test_name.upper()}æµ‹è¯•:")
                for key, value in test_data.items():
                    if key != "score":
                        if isinstance(value, float):
                            print(f"  {key}: {value:.2f}")
                        else:
                            print(f"  {key}: {value}")
            
            # æ˜¾ç¤ºå»ºè®®
            recommendations = results.get("recommendations", [])
            if recommendations:
                print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
                for rec in recommendations:
                    print(f"  â€¢ {rec}")
        else:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {results['error']}")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())
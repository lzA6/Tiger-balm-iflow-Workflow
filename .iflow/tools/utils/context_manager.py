#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½ä¸Šä¸‹æ–‡ç®¡ç†å’Œå‹ç¼©ç³»ç»Ÿ
Intelligent Context Management and Compression System

ä½œè€…: Quantum AI Team
ç‰ˆæœ¬: 5.2.0
æ—¥æœŸ: 2025-11-12
"""

import os
import re
import json
import time
import pickle
import hashlib
import zlib
import lzma
import gzip
import bz2
import asyncio
import logging
import threading
from typing import Dict, List, Any, Optional, Union, Tuple, Set
from dataclasses import dataclass, asdict
from pathlib import Path
from enum import Enum
from collections import defaultdict, deque, OrderedDict
import numpy as np
from datetime import datetime, timedelta
import sys

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CompressionAlgorithm(Enum):
    """å‹ç¼©ç®—æ³•"""
    NONE = "none"
    ZLIB = "zlib"
    LZMA = "lzma"
    GZIP = "gzip"
    BZ2 = "bz2"
    ADAPTIVE = "adaptive"

class ContextPriority(Enum):
    """ä¸Šä¸‹æ–‡ä¼˜å…ˆçº§"""
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"
    TEMPORARY = "temporary"

@dataclass
class ContextChunk:
    """ä¸Šä¸‹æ–‡å—"""
    chunk_id: str
    content: str
    size: int
    compressed_size: int
    compression_algorithm: CompressionAlgorithm
    priority: ContextPriority
    tags: Set[str]
    access_count: int
    last_accessed: float
    created_at: float
    expires_at: Optional[float]
    dependencies: Set[str]
    metadata: Dict[str, Any]

@dataclass
class ContextIndex:
    """ä¸Šä¸‹æ–‡ç´¢å¼•"""
    term: str
    chunk_ids: List[str]
    frequency: int
    last_accessed: float
    relevance_score: float

@dataclass
class ContextMetrics:
    """ä¸Šä¸‹æ–‡æŒ‡æ ‡"""
    total_chunks: int
    total_size: int
    compressed_size: int
    compression_ratio: float
    access_frequency: float
    hit_rate: float
    memory_usage: float
    cache_efficiency: float

class ContextCompressor:
    """ä¸Šä¸‹æ–‡å‹ç¼©å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–å‹ç¼©å™¨"""
        self.compression_methods = {
            CompressionAlgorithm.ZLIB: self._compress_zlib,
            CompressionAlgorithm.LZMA: self._compress_lzma,
            CompressionAlgorithm.GZIP: self._compress_gzip,
            CompressionAlgorithm.BZ2: self._compress_bz2
        }
        
        self.decompression_methods = {
            CompressionAlgorithm.ZLIB: self._decompress_zlib,
            CompressionAlgorithm.LZMA: self._decompress_lzma,
            CompressionAlgorithm.GZIP: self._decompress_gzip,
            CompressionAlgorithm.BZ2: self._decompress_bz2
        }
        
        self.algorithm_performance = defaultdict(list)
    
    def compress(self, content: str, algorithm: CompressionAlgorithm) -> Tuple[bytes, CompressionAlgorithm]:
        """å‹ç¼©å†…å®¹"""
        if algorithm == CompressionAlgorithm.NONE:
            return content.encode('utf-8'), algorithm
        
        if algorithm == CompressionAlgorithm.ADAPTIVE:
            # è‡ªé€‚åº”é€‰æ‹©æœ€ä½³å‹ç¼©ç®—æ³•
            algorithm = self._select_best_algorithm(content)
        
        if algorithm in self.compression_methods:
            start_time = time.time()
            compressed = self.compression_methods[algorithm](content)
            compression_time = time.time() - start_time
            
            # è®°å½•æ€§èƒ½
            self.algorithm_performance[algorithm].append({
                'size': len(content),
                'compressed_size': len(compressed),
                'time': compression_time,
                'ratio': len(compressed) / len(content)
            })
            
            return compressed, algorithm
        
        # é»˜è®¤ä½¿ç”¨zlib
        return self._compress_zlib(content), CompressionAlgorithm.ZLIB
    
    def decompress(self, compressed: bytes, algorithm: CompressionAlgorithm) -> str:
        """è§£å‹ç¼©å†…å®¹"""
        if algorithm == CompressionAlgorithm.NONE:
            return compressed.decode('utf-8')
        
        if algorithm in self.decompression_methods:
            return self.decompression_methods[algorithm](compressed)
        
        raise ValueError(f"ä¸æ”¯æŒçš„å‹ç¼©ç®—æ³•: {algorithm}")
    
    def _compress_zlib(self, content: str) -> bytes:
        """ZLIBå‹ç¼©"""
        return zlib.compress(content.encode('utf-8'))
    
    def _decompress_zlib(self, compressed: bytes) -> str:
        """ZLIBè§£å‹ç¼©"""
        return zlib.decompress(compressed).decode('utf-8')
    
    def _compress_lzma(self, content: str) -> bytes:
        """LZMAå‹ç¼©"""
        return lzma.compress(content.encode('utf-8'))
    
    def _decompress_lzma(self, compressed: bytes) -> str:
        """LZMAè§£å‹ç¼©"""
        return lzma.decompress(compressed).decode('utf-8')
    
    def _compress_gzip(self, content: str) -> bytes:
        """GZIPå‹ç¼©"""
        return gzip.compress(content.encode('utf-8'))
    
    def _decompress_gzip(self, compressed: bytes) -> str:
        """GZIPè§£å‹ç¼©"""
        return gzip.decompress(compressed).decode('utf-8')
    
    def _compress_bz2(self, content: str) -> bytes:
        """BZ2å‹ç¼©"""
        return bz2.compress(content.encode('utf-8'))
    
    def _decompress_bz2(self, compressed: bytes) -> str:
        """BZ2è§£å‹ç¼©"""
        return bz2.decompress(compressed).decode('utf-8')
    
    def _select_best_algorithm(self, content: str) -> CompressionAlgorithm:
        """é€‰æ‹©æœ€ä½³å‹ç¼©ç®—æ³•"""
        algorithms = [
            CompressionAlgorithm.ZLIB,
            CompressionAlgorithm.LZMA,
            CompressionAlgorithm.GZIP,
            CompressionAlgorithm.BZ2
        ]
        
        best_algorithm = CompressionAlgorithm.ZLIB
        best_ratio = 1.0
        
        for algorithm in algorithms:
            try:
                compressed = self.compression_methods[algorithm](content)
                ratio = len(compressed) / len(content)
                
                if ratio < best_ratio:
                    best_ratio = ratio
                    best_algorithm = algorithm
                    
            except Exception:
                continue
        
        return best_algorithm
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        stats = {}
        
        for algorithm, records in self.algorithm_performance.items():
            if records:
                avg_ratio = sum(r['ratio'] for r in records) / len(records)
                avg_time = sum(r['time'] for r in records) / len(records)
                
                stats[algorithm.value] = {
                    'uses': len(records),
                    'avg_compression_ratio': avg_ratio,
                    'avg_compression_time': avg_time,
                    'total_size_saved': sum(r['size'] - r['compressed_size'] for r in records)
                }
        
        return stats

class ContextCache:
    """ä¸Šä¸‹æ–‡ç¼“å­˜"""
    
    def __init__(self, max_size: int = 1000, max_memory: int = 100 * 1024 * 1024):
        """åˆå§‹åŒ–ç¼“å­˜"""
        self.max_size = max_size
        self.max_memory = max_memory
        
        # ä½¿ç”¨æœ‰åºå­—å…¸å®ç°LRU
        self.cache = OrderedDict()
        self.current_memory = 0
        self.access_order = deque(maxlen=max_size)
        self.hit_count = 0
        self.miss_count = 0
        
        # é”ç»Ÿè®¡
        self.lock = threading.RLock()
    
    def get(self, chunk_id: str) -> Optional[ContextChunk]:
        """è·å–ä¸Šä¸‹æ–‡å—"""
        with self.lock:
            if chunk_id in self.cache:
                chunk = self.cache[chunk_id]
                chunk.access_count += 1
                chunk.last_accessed = time.time()
                
                # ç§»åŠ¨åˆ°æœ«å°¾ï¼ˆLRUï¼‰
                self.cache.move_to_end(chunk_id)
                self.hit_count += 1
                
                return chunk
            else:
                self.miss_count += 1
                return None
    
    def put(self, chunk: ContextChunk):
        """å­˜å‚¨ä¸Šä¸‹æ–‡å—"""
        with self.lock:
            # æ£€æŸ¥å†…å­˜é™åˆ¶
            if chunk_id not in self.cache:
                self.current_memory += chunk.compressed_size
                
                # å¦‚æœè¶…å‡ºå†…å­˜é™åˆ¶ï¼Œç§»é™¤æœ€æ—§çš„å—
                while (self.current_memory > self.max_memory or 
                       len(self.cache) >= self.max_size):
                    oldest_id = next(iter(self.cache))
                    oldest_chunk = self.cache[oldest_id]
                    self.current_memory -= oldest_chunk.compressed_size
                    del self.cache[oldest_id]
                    self.access_order.popleft()
            
            # æ›´æ–°æˆ–æ·»åŠ 
            self.cache[chunk_id] = chunk
            chunk.access_count = 1
            chunk.last_accessed = time.time()
            
            # ç§»åŠ¨åˆ°æœ«å°¾ï¼ˆLRUï¼‰
            if chunk_id in self.cache:
                self.cache.move_to_end(chunk_id)
            else:
                self.access_order.append(chunk_id)
    
    def remove(self, chunk_id: str) -> Optional[ContextChunk]:
        """ç§»é™¤ä¸Šä¸‹æ–‡å—"""
        with self.lock:
            if chunk_id in self.cache:
                chunk = self.cache.pop(chunk_id)
                self.current_memory -= chunk.compressed_size
                
                # ä»è®¿é—®é¡ºåºä¸­ç§»é™¤
                if chunk_id in self.access_order:
                    self.access_order.remove(chunk_id)
                
                return chunk
            return None
    
    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        with self.lock:
            self.cache.clear()
            self.current_memory = 0
            self.access_order.clear()
            self.hit_count = 0
            self.miss_count = 0
    
    def get_hit_rate(self) -> float:
        """è·å–å‘½ä¸­ç‡"""
        total = self.hit_count + self.miss_count
        return self.hit_count / total if total > 0 else 0.0
    
    def get_memory_usage(self) -> int:
        """è·å–å†…å­˜ä½¿ç”¨é‡"""
        return self.current_memory
    
    def get_cache_info(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ä¿¡æ¯"""
        return {
            'size': len(self.cache),
            'max_size': self.max_size,
            'memory_usage': self.current_memory,
            'max_memory': self.max_memory,
            'hit_rate': self.get_hit_rate(),
            'hit_count': self.hit_count,
            'miss_count': self.miss_count
        }

class ContextIndexer:
    """ä¸Šä¸‹æ–‡ç´¢å¼•å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç´¢å¼•å™¨"""
        self.index = {}
        self.term_frequency = defaultdict(int)
        self.term_last_accessed = {}
        self.relevance_scores = {}
    
    def index_chunk(self, chunk: ContextChunk):
        """ç´¢å¼•ä¸Šä¸‹æ–‡å—"""
        # æå–å…³é”®è¯
        terms = self._extract_terms(chunk.content)
        
        # æ›´æ–°ç´¢å¼•
        for term in terms:
            if term not in self.index:
                self.index[term] = []
            
            if chunk.chunk_id not in self.index[term]:
                self.index[term].append(chunk.chunk_id)
            
            # æ›´æ–°é¢‘ç‡
            self.term_frequency[term] += 1
            self.term_last_accessed[term] = time.time()
        
        # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
        self._calculate_relevance_scores()
    
    def remove_chunk(self, chunk_id: str):
        """ç§»é™¤ä¸Šä¸‹æ–‡å—ç´¢å¼•"""
        for term, chunk_ids in self.index.items():
            if chunk_id in chunk_ids:
                chunk_ids.remove(chunk_id)
                if not chunk_ids:
                    del self.index[term]
                    del self.term_frequency[term]
                    del self.term_last_accessed[term]
                    del self.relevance_scores[term]
    
    def _extract_terms(self, content: str) -> List[str]:
        """æå–å…³é”®è¯"""
        # ç®€å•çš„å…³é”®è¯æå–
        # å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„NLPæŠ€æœ¯
        
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·å¹¶åˆ†è¯
        words = re.findall(r'\b\w+\b', content.lower())
        
        # è¿‡æ»¤åœç”¨è¯
        stop_words = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
            'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has',
            'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might',
            'can', 'must', 'shall', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she',
            'it', 'we', 'they', 'them', 'their', 'what', 'which', 'who', 'when', 'where',
            'why', 'how', 'all', 'any', 'both', 'each', 'every', 'other', 'another', 'same'
        }
        
        # è¿‡æ»¤çŸ­è¯å’Œåœç”¨è¯
        terms = [word for word in words if len(word) > 2 and word not in stop_words]
        
        # å»é‡
        return list(set(terms))
    
    def _calculate_relevance_scores(self):
        """è®¡ç®—ç›¸å…³æ€§åˆ†æ•°"""
        current_time = time.time()
        max_frequency = max(self.term_frequency.values()) if self.term_frequency else 1
        
        for term in self.term_frequency:
            # åŸºäºé¢‘ç‡å’Œæ—¶é—´è¡°å‡è®¡ç®—åˆ†æ•°
            frequency_score = self.term_frequency[term] / max_frequency
            time_decay = np.exp(-(current_time - self.term_last_accessed[term]) / (24 * 60 * 60))  # 24å°æ—¶è¡°å‡
            
            self.relevance_scores[term] = frequency_score * time_decay
    
    def search(self, query: str, limit: int = 10) -> List[str]:
        """æœç´¢ç›¸å…³ä¸Šä¸‹æ–‡å—"""
        query_terms = self._extract_terms(query)
        
        # è®¡ç®—æŸ¥è¯¢è¯çš„ç›¸å…³æ€§åˆ†æ•°
        term_scores = {}
        for term in query_terms:
            term_scores[term] = self.relevance_scores.get(term, 0.0)
        
        # æœç´¢åŒ…å«æŸ¥è¯¢è¯çš„ä¸Šä¸‹æ–‡å—
        chunk_scores = defaultdict(float)
        
        for term, score in term_scores.items():
            if term in self.index:
                for chunk_id in self.index[term]:
                    chunk_scores[chunk_id] += score
        
        # æ’åºå¹¶è¿”å›æœ€ç›¸å…³çš„å—ID
        sorted_chunks = sorted(chunk_scores.items(), key=lambda x: x[1], reverse=True)
        
        return [chunk_id for chunk_id, _ in sorted_chunks[:limit]]
    
    def get_index_stats(self) -> Dict[str, Any]:
        """è·å–ç´¢å¼•ç»Ÿè®¡"""
        return {
            'total_terms': len(self.index),
            'total_frequency': sum(self.term_frequency.values()),
            'indexed_chunks': sum(len(chunk_ids) for chunk_ids in self.index.values()),
            'avg_relevance_score': np.mean(list(self.relevance_scores.values())) if self.relevance_scores else 0
        }

class ContextManager:
    """ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    
    def __init__(self, 
                 max_chunks: int = 1000,
                 max_memory: int = 100 * 1024 * 1024,
                 default_ttl: int = 3600):  # 1å°æ—¶
                 ):
        """åˆå§‹åŒ–ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        self.max_chunks = max_chunks
        self.max_memory = max_memory
        self.default_ttl = default_ttl
        
        # æ ¸å¿ƒç»„ä»¶
        self.compressor = ContextCompressor()
        self.cache = ContextCache(max_chunks, max_memory)
        self.indexer = ContextIndexer()
        
        # ä¸Šä¸‹æ–‡å—å­˜å‚¨
        self.chunks = {}
        
        # é…ç½®
        self.config = {
            'auto_cleanup': True,
            'cleanup_interval': 300,  # 5åˆ†é’Ÿ
            'compression_threshold': 1024,  # 1KB
            'priority_decay_rate': 0.1,
            'relevance_threshold': 0.3
        }
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.metrics = ContextMetrics(
            total_chunks=0,
            total_size=0,
            compressed_size=0,
            compression_ratio=0.0,
            access_frequency=0.0,
            hit_rate=0.0,
            memory_usage=0.0,
            cache_efficiency=0.0
        )
        
        # å¯åŠ¨æ¸…ç†ä»»åŠ¡
        self.cleanup_task = None
        self.is_running = False
        
        # é”
        self.lock = threading.RLock()
    
    def start(self):
        """å¯åŠ¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        if self.is_running:
            logger.warning("âš ï¸ ä¸Šä¸‹æ–‡ç®¡ç†å™¨å·²åœ¨è¿è¡Œ")
            return
        
        self.is_running = True
        self.cleanup_task = asyncio.create_task(self._cleanup_loop())
        logger.info("ğŸš€ ä¸Šä¸‹æ–‡ç®¡ç†å™¨å·²å¯åŠ¨")
    
    async def stop(self):
        """åœæ­¢ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        self.is_running = False
        
        if self.cleanup_task:
            self.cleanup_task.cancel()
            try:
                await self.cleanup_task
            except asyncio.CancelledError:
                pass
        
        logger.info("ğŸ›‘ ä¸Šä¸‹æ–‡ç®¡ç†å™¨å·²åœæ­¢")
    
    async def _cleanup_loop(self):
        """æ¸…ç†å¾ªç¯"""
        while self.is_running:
            try:
                await asyncio.sleep(self.config['cleanup_interval'])
                
                if self.config['auto_cleanup']:
                    await self._cleanup_expired_chunks()
                    await self._cleanup_low_priority_chunks()
                    await self._update_metrics()
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"âŒ æ¸…ç†å¾ªç¯é”™è¯¯: {e}")
                await asyncio.sleep(60)
    
    def add_context(self, 
                    content: str,
                    priority: ContextPriority = ContextPriority.MEDIUM,
                    tags: Optional[List[str]] = None,
                    ttl: Optional[int] = None,
                    dependencies: Optional[List[str]] = None,
                    metadata: Optional[Dict[str, Any]] = None) -> str:
        """æ·»åŠ ä¸Šä¸‹æ–‡"""
        with self.lock:
            chunk_id = f"chunk_{int(time.time())}_{hashlib.md5(content.encode()).hexdigest()[:8]}"
            
            # æ£€æŸ¥å†…å®¹å¤§å°
            content_size = len(content.encode('utf-8'))
            
            # å‹ç¼©å†…å®¹
            if content_size > self.config['compression_threshold']:
                compressed_content, algorithm = self.compressor.compress(content, CompressionAlgorithm.ADAPTIVE)
            else:
                compressed_content = content.encode('utf-8')
                algorithm = CompressionAlgorithm.NONE
            
            # åˆ›å»ºä¸Šä¸‹æ–‡å—
            expires_at = time.time() + (ttl or self.default_ttl)
            
            chunk = ContextChunk(
                chunk_id=chunk_id,
                content=content,
                size=content_size,
                compressed_size=len(compressed_content),
                compression_algorithm=algorithm,
                priority=priority,
                tags=set(tags or []),
                access_count=0,
                last_accessed=time.time(),
                created_at=time.time(),
                expires_at=expires_at,
                dependencies=set(dependencies or []),
                metadata=metadata or {}
            )
            
            # å­˜å‚¨å—
            self.chunks[chunk_id] = chunk
            self.cache.put(chunk)
            
            # ç´¢å¼•å—
            self.indexer.index_chunk(chunk)
            
            logger.debug(f"ğŸ“ æ·»åŠ ä¸Šä¸‹æ–‡å—: {chunk_id} (å¤§å°: {content_size} å­—èŠ‚)")
            
            return chunk_id
    
    def get_context(self, chunk_id: str) -> Optional[str]:
        """è·å–ä¸Šä¸‹æ–‡"""
        with self.lock:
            chunk = self.cache.get(chunk_id)
            
            if chunk:
                # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
                if chunk.expires_at and time.time() > chunk.expires_at:
                    self.remove_context(chunk_id)
                    return None
                
                return chunk.content
            
            return None
    
    def remove_context(self, chunk_id: str) -> bool:
        """ç§»é™¤ä¸Šä¸‹æ–‡"""
        with self.lock:
            # ä»ç¼“å­˜ç§»é™¤
            chunk = self.cache.remove(chunk_id)
            
            # ä»å­˜å‚¨ç§»é™¤
            if chunk_id in self.chunks:
                del self.chunks[chunk_id]
            
            # ä»ç´¢å¼•ç§»é™¤
            self.indexer.remove_chunk(chunk_id)
            
            logger.debug(f"ğŸ—‘ï¸ ç§»é™¤ä¸Šä¸‹æ–‡å—: {chunk_id}")
            
            return chunk is not None
    
    def search_context(self, 
                    query: str, 
                    limit: int = 10,
                    min_relevance: float = 0.3) -> List[str]:
        """æœç´¢ä¸Šä¸‹æ–‡"""
        with self.lock:
            # ä½¿ç”¨ç´¢å¼•å™¨æœç´¢
            candidate_ids = self.indexer.search(query, limit * 2)  # è·å–æ›´å¤šå€™é€‰
            
            # è¿‡æ»¤ç›¸å…³æ€§åˆ†æ•°
            relevant_ids = []
            for chunk_id in candidate_ids:
                chunk = self.cache.get(chunk_id)
                if chunk:
                    # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
                    if chunk.expires_at and time.time() > chunk.expires_at:
                        continue
                    
                    # æ£€æŸ¥ç›¸å…³æ€§
                    terms = self.indexer._extract_terms(query)
                    relevance = sum(self.indexer.relevance_scores.get(term, 0) for term in terms if term in self.indexer.index)
                    
                    if relevance >= min_relevance:
                        relevant_ids.append(chunk_id)
            
            # æŒ‰ç›¸å…³æ€§æ’åº
            relevant_ids.sort(key=lambda x: self.indexer.relevance_scores.get(x.split('_')[0], 0), reverse=True)
            
            return relevant_ids[:limit]
    
    def get_related_contexts(self, chunk_id: str, limit: int = 5) -> List[str]:
        """è·å–ç›¸å…³ä¸Šä¸‹æ–‡"""
        with self.lock:
            chunk = self.cache.get(chunk_id)
            
            if not chunk:
                return []
            
            # åŸºäºä¾èµ–å…³ç³»æŸ¥æ‰¾ç›¸å…³å—
            related_ids = []
            
            for dep_id in chunk.dependencies:
                if dep_id in self.chunks:
                    related_ids.append(dep_id)
            
            # åŸºäºå…±åŒæ ‡ç­¾æŸ¥æ‰¾ç›¸å…³å—
            for other_id, other_chunk in self.chunks.items():
                if (other_id != chunk_id and 
                    chunk.tags & other_chunk.tags and
                    other_id not in related_ids):
                    related_ids.append(other_id)
            
            # æŒ‰ä¼˜å…ˆçº§å’Œæœ€è¿‘è®¿é—®æ—¶é—´æ’åº
            related_ids.sort(key=lambda x: (
                self.chunks[x].priority.value * 0.5 + 
                (time.time() - self.chunks[x].last_accessed) * 0.5
            ), reverse=True)
            
            return related_ids[:limit]
    
    def update_context_priority(self, chunk_id: str, priority: ContextPriority):
        """æ›´æ–°ä¸Šä¸‹æ–‡ä¼˜å…ˆçº§"""
        with self.lock:
            chunk = self.cache.get(chunk_id)
            
            if chunk:
                old_priority = chunk.priority
                chunk.priority = priority
                chunk.last_accessed = time.time()
                
                # è°ƒæ•´ç›¸å…³å—çš„ä¼˜å…ˆçº§
                for related_id in chunk.dependencies:
                    related_chunk = self.cache.get(related_id)
                    if related_chunk:
                        # è½»å¾®è°ƒæ•´ä¼˜å…ˆçº§
                        priority_diff = (priority.value - old_priority.value) * self.config['priority_decay_rate']
                        new_priority_value = related_chunk.priority.value + priority_diff
                        related_chunk.priority = ContextPriority(
                            min(max(0.0, max(1.0, new_priority_value))
                        )
                
                logger.debug(f"ğŸ”„ æ›´æ–°ä¸Šä¸‹æ–‡ä¼˜å…ˆçº§: {chunk_id} -> {priority.value}")
    
    def get_context_metrics(self) -> ContextMetrics:
        """è·å–ä¸Šä¸‹æ–‡æŒ‡æ ‡"""
        with self.lock:
            # æ›´æ–°æŒ‡æ ‡
            self._update_metrics()
            
            return self.metrics
    
    def _update_metrics(self):
        """æ›´æ–°æŒ‡æ ‡"""
        # è®¡ç®—æ€»å¤§å°
        total_size = sum(chunk.size for chunk in self.chunks.values())
        compressed_size = sum(chunk.compressed_size for chunk in self.chunks.values())
        
        # è®¡ç®—å‹ç¼©æ¯”
        compression_ratio = compressed_size / total_size if total_size > 0 else 1.0
        
        # è®¡ç®—è®¿é—®é¢‘ç‡
        total_accesses = sum(chunk.access_count for chunk in self.chunks.values())
        time_window = 3600  # 1å°æ—¶çª—å£
        recent_accesses = sum(
            chunk.access_count for chunk in self.chunks.values()
            if time.time() - chunk.last_accessed <= time_window
        )
        access_frequency = recent_accesses / max(1, len(self.chunks))
        
        # è·å–ç¼“å­˜ç»Ÿè®¡
        cache_info = self.cache.get_cache_info()
        
        # è®¡ç®—ç¼“å­˜æ•ˆç‡
        memory_usage = cache_info['memory_usage']
        max_memory = cache_info['max_memory']
        cache_efficiency = 1.0 - (memory_usage / max_memory) if max_memory > 0 else 1.0
        
        self.metrics = ContextMetrics(
            total_chunks=len(self.chunks),
            total_size=total_size,
            compressed_size=compressed_size,
            compression_ratio=compression_ratio,
            access_frequency=access_frequency,
            hit_rate=cache_info['hit_rate'],
            memory_usage=memory_usage,
            cache_efficiency=cache_efficiency
        )
    
    async def _cleanup_expired_chunks(self):
        """æ¸…ç†è¿‡æœŸä¸Šä¸‹æ–‡"""
        current_time = time.time()
        expired_ids = []
        
        for chunk_id, chunk in self.chunks.items():
            if chunk.expires_at and current_time > chunk.expires_at:
                expired_ids.append(chunk_id)
        
        for chunk_id in expired_ids:
            self.remove_context(chunk_id)
        
        if expired_ids:
            logger.info(f"ğŸ—‘ï¸ æ¸…ç†äº† {len(expired_ids)} ä¸ªè¿‡æœŸä¸Šä¸‹æ–‡å—")
    
    async def _cleanup_low_priority_chunks(self):
        """æ¸…ç†ä½ä¼˜å…ˆçº§ä¸Šä¸‹æ–‡"""
        with self.lock:
            # æŒ‰ä¼˜å…ˆçº§å’Œè®¿é—®æ—¶é—´æ’åº
            sorted_chunks = sorted(
                self.chunks.items(),
                key=lambda x: (
                    x[1].priority.value * 0.3 + 
                    (current_time - x[1].last_accessed) * 0.7
                )
            )
            
            # è®¡ç®—éœ€è¦ç§»é™¤çš„æ•°é‡
            excess_count = len(self.chunks) - self.max_chunks
            
            if excess_count > 0:
                # ç§»é™¤æœ€ä½ä¼˜å…ˆçº§çš„å—
                for i in range(excess_count):
                    if i < len(sorted_chunks):
                        chunk_id = sorted_chunks[i][0]
                        self.remove_context(chunk_id)
                        
                        logger.debug(f"ğŸ—‘ï¸ æ¸…ç†ä½ä¼˜å…ˆçº§ä¸Šä¸‹æ–‡å—: {chunk_id}")
                
                logger.info(f"ğŸ—‘ï¸ æ¸…ç†äº† {excess_count} ä¸ªä½ä¼˜å…ˆçº§ä¸Šä¸‹æ–‡å—")
    
    def get_compression_stats(self) -> Dict[str, Any]:
        """è·å–å‹ç¼©ç»Ÿè®¡"""
        return self.compressor.get_performance_stats()
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        return self.cache.get_cache_info()
    
    def get_index_stats(self) -> Dict[str, Any]:
        """è·å–ç´¢å¼•ç»Ÿè®¡"""
        return self.indexer.get_index_stats()
    
    def get_full_stats(self) -> Dict[str, Any]:
        """è·å–å®Œæ•´ç»Ÿè®¡"""
        return {
            'chunks': {
                'total': len(self.chunks),
                'by_priority': {
                    priority.value: len([c for c in self.chunks.values() if c.priority == priority])
                    for priority in ContextPriority
                },
                'by_type': {
                    pattern_type: len([c for c in self.chunks.values() if pattern_type in c.tags])
                    for pattern_type in set(tag for c in self.chunks.values() for tag in c.tags)
                }
            },
            'compression': self.get_compression_stats(),
            'cache': self.get_cache_stats(),
            'index': self.get_index_stats(),
            'metrics': asdict(self.get_context_metrics())
        }

# å…¨å±€ä¸Šä¸‹æ–‡ç®¡ç†å™¨å®ä¾‹
context_manager = ContextManager()

# ä¾¿æ·å‡½æ•°
def add_context(content: str, 
                priority: ContextPriority = ContextPriority.MEDIUM,
                tags: Optional[List[str]] = None,
                ttl: Optional[int] = None) -> str:
    """ä¾¿æ·çš„ä¸Šä¸‹æ–‡æ·»åŠ å‡½æ•°"""
    return context_manager.add_context(
        content=content,
        priority=priority,
        tags=tags,
        ttl=ttl
    )

def get_context(chunk_id: str) -> Optional[str]:
    """ä¾¿æ·çš„ä¸Šä¸‹æ–‡è·å–å‡½æ•°"""
    return context_manager.get_context(chunk_id)

def search_context(query: str, limit: int = 10) -> List[str]:
    """ä¾¿æ·çš„ä¸Šä¸‹æ–‡æœç´¢å‡½æ•°"""
    return context_manager.search_context(query, limit)

def get_related_contexts(chunk_id: str, limit: int = 5) -> List[str]:
    """ä¾¿æ·çš„ç›¸å…³ä¸Šä¸‹æ–‡è·å–å‡½æ•°"""
    return context_manager.get_related_contexts(chunk_id, limit)

# ç¤ºä¾‹ä½¿ç”¨
async def example_usage():
    """ç¤ºä¾‹ä½¿ç”¨"""
    print("ğŸ§  æ™ºèƒ½ä¸Šä¸‹æ–‡ç®¡ç†å’Œå‹ç¼©ç³»ç»Ÿç¤ºä¾‹")
    
    # å¯åŠ¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨
    print("\n1. å¯åŠ¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨:")
    context_manager.start()
    
    # æ·»åŠ ä¸Šä¸‹æ–‡
    print("\n2. æ·»åŠ ä¸Šä¸‹æ–‡:")
    
    # é«˜ä¼˜å…ˆçº§ä¸Šä¸‹æ–‡
    chunk_id1 = add_context(
        content="è¿™æ˜¯ä¸€ä¸ªé‡è¦çš„ç³»ç»Ÿé…ç½®æ–‡ä»¶ï¼ŒåŒ…å«æ•°æ®åº“è¿æ¥ä¿¡æ¯å’ŒAPIå¯†é’¥ã€‚",
        priority=ContextPriority.CRITICAL,
        tags=['config', 'database', 'security'],
        ttl=7200  # 2å°æ—¶
    )
    print(f"  æ·»åŠ å…³é”®ä¸Šä¸‹æ–‡: {chunk_id1}")
    
    # ä¸­ç­‰ä¼˜å…ˆçº§ä¸Šä¸‹æ–‡
    chunk_id2 = add_context(
        content="è¿™æ˜¯ä¸€ä¸ªç”¨æˆ·æ‰‹å†Œæ–‡æ¡£ï¼Œè¯¦ç»†è¯´æ˜äº†ç³»ç»Ÿçš„ä½¿ç”¨æ–¹æ³•å’Œæœ€ä½³å®è·µã€‚",
        priority=ContextPriority.MEDIUM,
        tags=['documentation', 'user_guide'],
        ttl=3600  # 1å°æ—¶
    )
    print(f"  æ·»åŠ æ–‡æ¡£ä¸Šä¸‹æ–‡: {chunk_id2}")
    
    # ä½ä¼˜å…ˆçº§ä¸Šä¸‹æ–‡
    chunk_id3 = add_context(
        content="è¿™æ˜¯ä¸€ä¸ªä¸´æ—¶çš„è°ƒè¯•ä¿¡æ¯ï¼Œå°†åœ¨ç³»ç»Ÿé‡å¯åæ¸…ç†ã€‚",
        priority=ContextPriority.TEMPORARY,
        tags=['debug', 'temporary'],
        ttl=300  # 5åˆ†é’Ÿ
    )
    print(f"  æ·»åŠ ä¸´æ—¶ä¸Šä¸‹æ–‡: {chunk_id3}")
    
    # å¤§å†…å®¹ï¼ˆä¼šè¢«å‹ç¼©ï¼‰
    large_content = "è¿™æ˜¯ä¸€ä¸ªå¾ˆå¤§çš„æ–‡æœ¬å†…å®¹ï¼ŒåŒ…å«å¤§é‡çš„ä»£ç ç¤ºä¾‹å’Œè¯¦ç»†è¯´æ˜ã€‚" * 1000
    chunk_id4 = add_context(
        content=large_content,
        priority=ContextPriority.LOW,
        tags=['large', 'examples'],
        ttl=1800  # 30åˆ†é’Ÿ
    )
    print(f"æ·»åŠ å¤§å†…å®¹ä¸Šä¸‹æ–‡: {chunk_id4} (ä¼šè¢«å‹ç¼©)")
    
    # æœç´¢ä¸Šä¸‹æ–‡
    print("\n3. æœç´¢ä¸Šä¸‹æ–‡:")
    search_results = search_context("ç³»ç»Ÿ é…ç½®", limit=3)
    for i, result_id in enumerate(search_results, 1):
        content = get_context(result_id)
        if content:
            print(f"  {i}. {result_id}: {content[:50]}...")
    
    # è·å–ç›¸å…³ä¸Šä¸‹æ–‡
    print("\n4. è·å–ç›¸å…³ä¸Šä¸‹æ–‡:")
    related_ids = get_related_contexts(chunk_id1, limit=3)
    for i, related_id in enumerate(related_ids, 1):
        content = get_context(related_id)
        if content:
            print(f"  {i}. {related_id}: {content[:50]}...")
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    print("\n5. ç»Ÿè®¡ä¿¡æ¯:")
    stats = context_manager.get_full_stats()
    
    print(f"  æ€»ä¸Šä¸‹æ–‡å—æ•°: {stats['chunks']['total']}")
    print(f"  æ€»å¤§å°: {stats['metrics']['total_size']} å­—èŠ‚")
    print(f"  å‹ç¼©åå¤§å°: {stats['metrics']['compressed_size']} å­—èŠ‚")
    print(f"  å‹ç¼©æ¯”: {stats['metrics']['compression_ratio']:.2f}")
    print(f"  ç¼“å­˜å‘½ä¸­ç‡: {stats['cache']['hit_rate']:.2f}")
    print(f"  å†…å­˜ä½¿ç”¨: {stats['cache']['memory_usage'] / (1024*1024):.2f} MB")
    print(f"  ç¼“å­˜æ•ˆç‡: {stats['metrics']['cache_efficiency']:.2f}")
    
    # å‹ç¼©ç»Ÿè®¡
    print("\n6. å‹ç¼©ç®—æ³•ç»Ÿè®¡:")
    comp_stats = context_manager.get_compression_stats()
    for algorithm, stats in comp_stats.items():
        print(f"  {algorithm}:")
        print(f"    ä½¿ç”¨æ¬¡æ•°: {stats['uses']}")
        print(f"    å¹³å‡å‹ç¼©æ¯”: {stats['avg_compression_ratio']:.2f}")
        print(f"    å¹³å‡å‹ç¼©æ—¶é—´: {stats['avg_compression_time']:.4f}s")
    
    # ç­‰å¾…ä¸€æ®µæ—¶é—´è®©ç³»ç»Ÿè¿è¡Œ
    print("\n7. ç­‰å¾…ç³»ç»Ÿè¿è¡Œ...")
    await asyncio.sleep(2)
    
    # å†æ¬¡è·å–ç»Ÿè®¡ä¿¡æ¯
    print("\n8. æ›´æ–°åçš„ç»Ÿè®¡ä¿¡æ¯:")
    updated_stats = context_manager.get_full_stats()
    print(f"  ç¼“å­˜å‘½ä¸­ç‡: {updated_stats['cache']['hit_rate']:.2f}")
    
    # åœæ­¢ä¸Šä¸‹æ–‡ç®¡ç†å™¨
    print("\n9. åœæ­¢ä¸Šä¸‹æ–‡ç®¡ç†å™¨:")
    await context_manager.stop()
    
    print("\nâœ… æ™ºèƒ½ä¸Šä¸‹æ–‡ç®¡ç†å’Œå‹ç¼©ç³»ç»Ÿç¤ºä¾‹å®Œæˆ")

if __name__ == "__main__":
    asyncio.run(example_usage())
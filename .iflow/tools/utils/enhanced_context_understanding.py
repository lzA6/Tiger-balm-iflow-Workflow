#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºä¸Šä¸‹æ–‡ç†è§£æ¨¡å— - æ·±åº¦è¯­ä¹‰åˆ†æå’Œæ„å›¾è¯†åˆ«
Enhanced Context Understanding - Deep Semantic Analysis and Intent Recognition

ä½œè€…: Quantum AI Team
ç‰ˆæœ¬: 5.2.0
æ—¥æœŸ: 2025-11-12
"""

import re
import json
import time
import logging
import asyncio
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass, asdict
from pathlib import Path
from enum import Enum
import numpy as np
from collections import defaultdict, Counter

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class IntentType(Enum):
    """æ„å›¾ç±»å‹æšä¸¾"""
    CREATE = "create"          # åˆ›å»º/ç”Ÿæˆ
    ANALYZE = "analyze"        # åˆ†æ/æ£€æŸ¥
    OPTIMIZE = "optimize"      # ä¼˜åŒ–/æ”¹è¿›
    DEBUG = "debug"            # è°ƒè¯•/ä¿®å¤
    LEARN = "learn"            # å­¦ä¹ /ç†è§£
    DEPLOY = "deploy"          # éƒ¨ç½²/å‘å¸ƒ
    REFACTOR = "refactor"      # é‡æ„/æ•´ç†
    TEST = "test"              # æµ‹è¯•/éªŒè¯
    DOCUMENT = "document"      # æ–‡æ¡£/è¯´æ˜
    RESEARCH = "research"      # ç ”ç©¶/æ¢ç´¢
    DESIGN = "design"          # è®¾è®¡/è§„åˆ’
    INTEGRATE = "integrate"    # é›†æˆ/è¿æ¥
    MONITOR = "monitor"        # ç›‘æ§/è§‚å¯Ÿ

class ComplexityLevel(Enum):
    """å¤æ‚åº¦çº§åˆ«"""
    TRIVIAL = "trivial"        # å¾®ä¸è¶³é“
    SIMPLE = "simple"          # ç®€å•
    MODERATE = "moderate"      # ä¸­ç­‰
    COMPLEX = "complex"        # å¤æ‚
    CRITICAL = "critical"      # å…³é”®/å¤æ‚

class UrgencyLevel(Enum):
    """ç´§æ€¥ç¨‹åº¦çº§åˆ«"""
    LOW = "low"                # ä½
    NORMAL = "normal"          # æ­£å¸¸
    HIGH = "high"              # é«˜
    CRITICAL = "critical"      # ç´§æ€¥

@dataclass
class SemanticFeature:
    """è¯­ä¹‰ç‰¹å¾"""
    keywords: List[str]
    entities: List[Dict[str, str]]
    concepts: List[str]
    relationships: List[Dict[str, Any]]
    sentiment: float  # -1 to 1
    formality: float  # 0 to 1
    specificity: float  # 0 to 1

@dataclass
class ContextualFeature:
    """ä¸Šä¸‹æ–‡ç‰¹å¾"""
    domain: str
    language: Optional[str]
    framework: Optional[str]
    environment: Optional[str]
    scale: str  # small, medium, large, enterprise
    stakeholders: List[str]

@dataclass
class TemporalFeature:
    """æ—¶é—´ç‰¹å¾"""
    timeframe: str  # immediate, short, medium, long
    dependencies: List[str]
    sequence: List[str]
    constraints: List[str]

@dataclass
class EnhancedContext:
    """å¢å¼ºçš„ä¸Šä¸‹æ–‡ç†è§£ç»“æœ"""
    primary_intent: IntentType
    secondary_intents: List[IntentType]
    complexity: ComplexityLevel
    urgency: UrgencyLevel
    semantic_features: SemanticFeature
    contextual_features: ContextualFeature
    temporal_features: TemporalFeature
    confidence: float
    ambiguity_score: float
    suggested_actions: List[str]
    risk_factors: List[str]
    success_criteria: List[str]

class EnhancedContextUnderstanding:
    """å¢å¼ºä¸Šä¸‹æ–‡ç†è§£ç³»ç»Ÿ"""
    
    def __init__(self, config_path: Optional[str] = None):
        """åˆå§‹åŒ–å¢å¼ºä¸Šä¸‹æ–‡ç†è§£ç³»ç»Ÿ"""
        self.config_path = config_path or "context_understanding_config.json"
        self.context_history = []
        self.learning_enabled = True
        
        # åŠ è½½é…ç½®
        self._load_configuration()
        
        # åˆå§‹åŒ–çŸ¥è¯†åº“
        self._initialize_knowledge_base()
        
        logger.info("ğŸ§  å¢å¼ºä¸Šä¸‹æ–‡ç†è§£ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def _load_configuration(self):
        """åŠ è½½é…ç½®"""
        self.intent_patterns = self._load_intent_patterns()
        self.domain_keywords = self._load_domain_keywords()
        self.complexity_indicators = self._load_complexity_indicators()
        self.urgency_indicators = self._load_urgency_indicators()
    
    def _load_intent_patterns(self) -> Dict[IntentType, List[str]]:
        """åŠ è½½æ„å›¾æ¨¡å¼"""
        return {
            IntentType.CREATE: [
                "åˆ›å»º", "ç”Ÿæˆ", "å®ç°", "å¼€å‘", "ç¼–å†™", "æ„å»º", "åˆ¶ä½œ", "è®¾è®¡", "å»ºç«‹",
                "æ–°å¢", "æ·»åŠ ", "äº§ç”Ÿ", "äº§å‡º", "åˆ›é€ ", "æ‰“é€ ", "ç¼–å†™", "ç¼–ç "
            ],
            IntentType.ANALYZE: [
                "åˆ†æ", "è¯„ä¼°", "æ£€æŸ¥", "å®¡æŸ¥", "è¯Šæ–­", "ç ”ç©¶", "è°ƒæŸ¥", "æ¢ç´¢",
                "æŸ¥çœ‹", "ç†è§£", "è§£é‡Š", "è¯´æ˜", "è§£æ", "å®¡è§†", "è€ƒå¯Ÿ"
            ],
            IntentType.OPTIMIZE: [
                "ä¼˜åŒ–", "æ”¹è¿›", "æå‡", "åŠ é€Ÿ", "å¢å¼º", "ç²¾ç®€", "ç®€åŒ–", "å®Œå–„",
                "è°ƒä¼˜", "æ”¹å–„", "å‡çº§", "å¼ºåŒ–", "æé«˜", "æå‡", "ä¼˜åŒ–"
            ],
            IntentType.DEBUG: [
                "è°ƒè¯•", "æ’é”™", "ä¿®å¤", "è§£å†³", "å¤„ç†", "ä¿®æ­£", "çº æ­£", "æ¶ˆé™¤",
                "æ’æŸ¥", "å®šä½", "æ‰¾é”™", "é™¤é”™", "çº é”™", "ä¿®å¤", "è§£å†³"
            ],
            IntentType.LEARN: [
                "å­¦ä¹ ", "äº†è§£", "æŒæ¡", "ç†Ÿæ‚‰", "ç ”ç©¶", "æ¢ç´¢", "å‘ç°", "è®¤è¯†",
                "ç†è§£", "æ˜ç™½", "å¼„æ‡‚", "æŒæ¡", "å­¦ä¼š", "ä½“ä¼š", "é¢†æ‚Ÿ"
            ],
            IntentType.DEPLOY: [
                "éƒ¨ç½²", "å‘å¸ƒ", "ä¸Šçº¿", "è¿è¡Œ", "å¯åŠ¨", "æ‰§è¡Œ", "å®æ–½", "è½åœ°",
                "æŠ•äº§", "å‘å¸ƒ", "éƒ¨ç½²", "è¿è¡Œ", "å¯åŠ¨", "å®æ–½"
            ],
            IntentType.REFACTOR: [
                "é‡æ„", "æ•´ç†", "ä¼˜åŒ–ä»£ç ", "æ”¹è¿›ç»“æ„", "ç®€åŒ–", "è§„èŒƒ", "æ ‡å‡†åŒ–",
                "é‡æ„", "é‡å†™", "é‡ç»„", "è°ƒæ•´", "ä¼˜åŒ–", "æ”¹è¿›"
            ],
            IntentType.TEST: [
                "æµ‹è¯•", "éªŒè¯", "æ£€æŸ¥", "ç¡®è®¤", "éªŒè¯", "æ£€éªŒ", "æµ‹è¯•", "è¯•è¿è¡Œ",
                "éªŒè¯", "æ£€æŸ¥", "æµ‹è¯•", "ç¡®è®¤", "æ£€éªŒ"
            ],
            IntentType.DOCUMENT: [
                "æ–‡æ¡£", "è¯´æ˜", "è®°å½•", "æè¿°", "è§£é‡Š", "æ³¨é‡Š", "ç¼–å†™æ–‡æ¡£", "è®°å½•",
                "è¯´æ˜", "æè¿°", "è§£é‡Š", "æ–‡æ¡£åŒ–", "è®°å½•"
            ],
            IntentType.RESEARCH: [
                "ç ”ç©¶", "è°ƒç ”", "æ¢ç´¢", "æŸ¥æ‰¾", "æœç´¢", "è°ƒæŸ¥", "åˆ†æ", "è€ƒå¯Ÿ",
                "ç ”ç©¶", "è°ƒç ”", "æ¢ç´¢", "æŸ¥æ‰¾", "æœç´¢"
            ],
            IntentType.DESIGN: [
                "è®¾è®¡", "è§„åˆ’", "æ¶æ„", "æ–¹æ¡ˆ", "ç­–ç•¥", "è®¡åˆ’", "å®‰æ’", "å¸ƒå±€",
                "è®¾è®¡", "è§„åˆ’", "æ¶æ„", "åˆ¶å®š", "å®‰æ’"
            ],
            IntentType.INTEGRATE: [
                "é›†æˆ", "æ•´åˆ", "è¿æ¥", "åˆå¹¶", "èåˆ", "ç»“åˆ", "å¯¹æ¥", "è¿é€š",
                "é›†æˆ", "æ•´åˆ", "è¿æ¥", "åˆå¹¶", "èåˆ"
            ],
            IntentType.MONITOR: [
                "ç›‘æ§", "è§‚å¯Ÿ", "ç›‘è§†", "è·Ÿè¸ª", "æ£€æµ‹", "å…³æ³¨", "ç•™æ„", "æŸ¥çœ‹",
                "ç›‘æ§", "è§‚å¯Ÿ", "ç›‘è§†", "è·Ÿè¸ª", "æ£€æµ‹"
            ]
        }
    
    def _load_domain_keywords(self) -> Dict[str, List[str]]:
        """åŠ è½½é¢†åŸŸå…³é”®è¯"""
        return {
            "web": [
                "ç½‘ç«™", "ç½‘é¡µ", "å‰ç«¯", "åç«¯", "æµè§ˆå™¨", "æœåŠ¡å™¨", "API", "HTTP",
                "HTML", "CSS", "JavaScript", "React", "Vue", "Angular", "Node.js"
            ],
            "mobile": [
                "ç§»åŠ¨", "æ‰‹æœº", "APP", "åº”ç”¨", "iOS", "Android", "React Native",
                "Flutter", "ç§»åŠ¨ç«¯", "è§¦æ‘¸", "å“åº”å¼", "ç§»åŠ¨åº”ç”¨"
            ],
            "ai": [
                "AI", "äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "ç¥ç»ç½‘ç»œ", "æ¨¡å‹",
                "è®­ç»ƒ", "é¢„æµ‹", "åˆ†ç±»", "å›å½’", "èšç±»", "å¼ºåŒ–å­¦ä¹ "
            ],
            "data": [
                "æ•°æ®", "æ•°æ®åº“", "å¤§æ•°æ®", "æ•°æ®ç§‘å­¦", "åˆ†æ", "å¤„ç†", "å­˜å‚¨",
                "SQL", "NoSQL", "æ•°æ®ä»“åº“", "ETL", "æ•°æ®ç®¡é“"
            ],
            "system": [
                "ç³»ç»Ÿ", "æ¶æ„", "åˆ†å¸ƒå¼", "å¾®æœåŠ¡", "äº‘åŸç”Ÿ", "å®¹å™¨", "Kubernetes",
                "Docker", "DevOps", "è¿ç»´", "åŸºç¡€è®¾æ–½", "å¹³å°"
            ],
            "security": [
                "å®‰å…¨", "åŠ å¯†", "è®¤è¯", "æˆæƒ", "é˜²ç«å¢™", "æ¼æ´", "æ”»å‡»", "é˜²æŠ¤",
                "å¯†ç å­¦", "ç½‘ç»œå®‰å…¨", "ä¿¡æ¯å®‰å…¨", "æ•°æ®ä¿æŠ¤"
            ],
            "performance": [
                "æ€§èƒ½", "ä¼˜åŒ–", "é€Ÿåº¦", "å»¶è¿Ÿ", "ååé‡", "å¹¶å‘", "ç¼“å­˜", "è´Ÿè½½",
                "æ‰©å±•æ€§", "å¯ä¼¸ç¼©æ€§", "æ€§èƒ½è°ƒä¼˜", "ç“¶é¢ˆ"
            ],
            "testing": [
                "æµ‹è¯•", "å•å…ƒæµ‹è¯•", "é›†æˆæµ‹è¯•", "ç«¯åˆ°ç«¯æµ‹è¯•", "è‡ªåŠ¨åŒ–æµ‹è¯•",
                "è´¨é‡ä¿è¯", "æµ‹è¯•é©±åŠ¨", "æŒç»­é›†æˆ", "æµ‹è¯•è¦†ç›–ç‡"
            ]
        }
    
    def _load_complexity_indicators(self) -> Dict[ComplexityLevel, List[str]]:
        """åŠ è½½å¤æ‚åº¦æŒ‡æ ‡"""
        return {
            ComplexityLevel.TRIVIAL: [
                "ç®€å•", "å®¹æ˜“", "åŸºç¡€", "åŸºæœ¬", "å…¥é—¨", "ç¤ºä¾‹", "æ¼”ç¤º", "ç»ƒä¹ ",
                "å¿«é€Ÿ", "é©¬ä¸Š", "ç«‹å³", "ç®€å•", "å®¹æ˜“"
            ],
            ComplexityLevel.SIMPLE: [
                "æ ‡å‡†", "å¸¸è§„", "æ™®é€š", "ä¸€èˆ¬", "æ—¥å¸¸", "å¸¸è§", "æ ‡å‡†", "å…¸å‹",
                "ä¸­ç­‰", "ä¸€èˆ¬", "å¸¸è§„", "æ ‡å‡†"
            ],
            ComplexityLevel.MODERATE: [
                "ä¸­ç­‰", "é€‚ä¸­", "åˆç†", "é€‚å½“", "éœ€è¦", "åº”è¯¥", "è€ƒè™‘", "è§„åˆ’",
                "ä¸­ç­‰", "é€‚ä¸­", "éœ€è¦", "è€ƒè™‘"
            ],
            ComplexityLevel.COMPLEX: [
                "å¤æ‚", "å›°éš¾", "æŒ‘æˆ˜", "é«˜çº§", "æ·±åº¦", "è¯¦ç»†", "å…¨é¢", "ç»¼åˆ",
                "å¤æ‚", "å›°éš¾", "æŒ‘æˆ˜", "é«˜çº§", "æ·±åº¦"
            ],
            ComplexityLevel.CRITICAL: [
                "å…³é”®", "é‡è¦", "æ ¸å¿ƒ", "ç´§æ€¥", "ä¸¥é‡", "é‡å¤§", "å…³é”®è·¯å¾„", "æ ¸å¿ƒ",
                "å…³é”®", "é‡è¦", "æ ¸å¿ƒ", "ç´§æ€¥", "ä¸¥é‡"
            ]
        }
    
    def _load_urgency_indicators(self) -> Dict[UrgencyLevel, List[str]]:
        """åŠ è½½ç´§æ€¥åº¦æŒ‡æ ‡"""
        return {
            UrgencyLevel.LOW: [
                "å¯ä»¥", "å»ºè®®", "å¯é€‰", "ç¨å", "æœ‰ç©º", "ä¸æ€¥", "ç©ºé—²", "æ–¹ä¾¿æ—¶",
                "å¯ä»¥", "å»ºè®®", "å¯é€‰", "ç¨å"
            ],
            UrgencyLevel.NORMAL: [
                "éœ€è¦", "åº”è¯¥", "è¦", "è®¡åˆ’", "å®‰æ’", "å‡†å¤‡", "è€ƒè™‘", "å¤„ç†",
                "éœ€è¦", "åº”è¯¥", "è¦", "è®¡åˆ’"
            ],
            UrgencyLevel.HIGH: [
                "å°½å¿«", "ä¼˜å…ˆ", "é‡è¦", "ç´§æ€¥", "æ€¥éœ€", "ç«‹å³", "é©¬ä¸Š", "èµ¶å¿«",
                "å°½å¿«", "ä¼˜å…ˆ", "é‡è¦", "ç´§æ€¥"
            ],
            UrgencyLevel.CRITICAL: [
                "ç´§æ€¥", "ç«‹å³", "é©¬ä¸Š", "ä¸¥é‡", "å…³é”®", "é‡è¦", "æ€¥éœ€", "åˆ»ä¸å®¹ç¼“",
                "ç´§æ€¥", "ç«‹å³", "é©¬ä¸Š", "ä¸¥é‡", "å…³é”®"
            ]
        }
    
    def _initialize_knowledge_base(self):
        """åˆå§‹åŒ–çŸ¥è¯†åº“"""
        self.entity_patterns = {
            "technology": [
                r"(Python|Java|JavaScript|TypeScript|Go|Rust|C\+\+|C#|PHP|Ruby|Swift|Kotlin)",
                r"(React|Vue|Angular|Node\.js|Django|Flask|Spring|Express\.js|Laravel)",
                r"(MySQL|PostgreSQL|MongoDB|Redis|Elasticsearch|Cassandra|Oracle)",
                r"(Docker|Kubernetes|Jenkins|Git|AWS|Azure|GCP|Terraform)"
            ],
            "file": [
                r"([a-zA-Z0-9_\-]+\.(py|js|ts|java|cpp|c|h|css|html|json|yaml|yml|md|sql))",
                r"([a-zA-Z0-9_\-\/]+\/[a-zA-Z0-9_\-]+\.(py|js|ts|java|cpp|c|h|css|html|json|yaml|yml|md|sql))"
            ],
            "metric": [
                r"(\d+(?:\.\d+)?)\s*(?:%|ms|s|MB|GB|TB|KB|bytes|requests/sec|QPS|TPS)",
                r"(æ€§èƒ½|é€Ÿåº¦|å»¶è¿Ÿ|ååé‡|å†…å­˜|CPU|ç£ç›˜|ç½‘ç»œ)\s*[:ï¼š]\s*(\d+(?:\.\d+)?)"
            ],
            "version": [
                r"v?(\d+(?:\.\d+)*(?:\.[a-zA-Z0-9]+)?)",
                r"(version|ver)\s*[:ï¼š]\s*(\d+(?:\.\d+)*)"
            ]
        }
        
        self.concept_patterns = {
            "architecture": [
                "æ¶æ„", "è®¾è®¡", "æ¨¡å¼", "ç»“æ„", "æ¡†æ¶", "ä½“ç³»", "ç»„ä»¶", "æ¨¡å—"
            ],
            "performance": [
                "æ€§èƒ½", "é€Ÿåº¦", "æ•ˆç‡", "ä¼˜åŒ–", "å»¶è¿Ÿ", "ååé‡", "å¹¶å‘", "æ‰©å±•"
            ],
            "security": [
                "å®‰å…¨", "åŠ å¯†", "è®¤è¯", "æˆæƒ", "é˜²æŠ¤", "æ¼æ´", "é£é™©", "å¨èƒ"
            ],
            "quality": [
                "è´¨é‡", "æµ‹è¯•", "éªŒè¯", "æ£€æŸ¥", "æ ‡å‡†", "è§„èŒƒ", "æœ€ä½³å®è·µ", "å¯é "
            ]
        }
    
    def understand_context(self, text: str, conversation_history: List[str] = None) -> EnhancedContext:
        """æ·±åº¦ç†è§£ä¸Šä¸‹æ–‡"""
        logger.info(f"ğŸ§  æ·±åº¦ç†è§£ä¸Šä¸‹æ–‡: {text[:50]}...")
        
        start_time = time.time()
        
        # 1. è¯­ä¹‰ç‰¹å¾æå–
        semantic_features = self._extract_semantic_features(text)
        
        # 2. ä¸Šä¸‹æ–‡ç‰¹å¾æå–
        contextual_features = self._extract_contextual_features(text, semantic_features)
        
        # 3. æ—¶é—´ç‰¹å¾æå–
        temporal_features = self._extract_temporal_features(text, conversation_history)
        
        # 4. æ„å›¾è¯†åˆ«
        primary_intent, secondary_intents = self._identify_intents(text, semantic_features)
        
        # 5. å¤æ‚åº¦è¯„ä¼°
        complexity = self._assess_complexity(text, semantic_features, contextual_features)
        
        # 6. ç´§æ€¥åº¦è¯„ä¼°
        urgency = self._assess_urgency(text, semantic_features, temporal_features)
        
        # 7. ç½®ä¿¡åº¦è®¡ç®—
        confidence = self._calculate_confidence(
            semantic_features, contextual_features, temporal_features,
            primary_intent, complexity, urgency
        )
        
        # 8. æ­§ä¹‰åº¦è¯„ä¼°
        ambiguity_score = self._assess_ambiguity(text, semantic_features, secondary_intents)
        
        # 9. å»ºè®®è¡ŒåŠ¨ç”Ÿæˆ
        suggested_actions = self._generate_suggested_actions(
            primary_intent, complexity, urgency, contextual_features
        )
        
        # 10. é£é™©å› ç´ è¯†åˆ«
        risk_factors = self._identify_risk_factors(text, complexity, contextual_features)
        
        # 11. æˆåŠŸæ ‡å‡†å®šä¹‰
        success_criteria = self._define_success_criteria(primary_intent, complexity, contextual_features)
        
        # æ„å»ºå¢å¼ºä¸Šä¸‹æ–‡
        enhanced_context = EnhancedContext(
            primary_intent=primary_intent,
            secondary_intents=secondary_intents,
            complexity=complexity,
            urgency=urgency,
            semantic_features=semantic_features,
            contextual_features=contextual_features,
            temporal_features=temporal_features,
            confidence=confidence,
            ambiguity_score=ambiguity_score,
            suggested_actions=suggested_actions,
            risk_factors=risk_factors,
            success_criteria=success_criteria
        )
        
        # å­˜å‚¨å†å²
        self.context_history.append({
            'timestamp': time.time(),
            'text': text,
            'context': enhanced_context,
            'processing_time': time.time() - start_time
        })
        
        logger.info(f"âœ… ä¸Šä¸‹æ–‡ç†è§£å®Œæˆ: {primary_intent.value} - ç½®ä¿¡åº¦: {confidence:.2f}")
        return enhanced_context
    
    def _extract_semantic_features(self, text: str) -> SemanticFeature:
        """æå–è¯­ä¹‰ç‰¹å¾"""
        # å…³é”®è¯æå–
        keywords = self._extract_keywords(text)
        
        # å®ä½“è¯†åˆ«
        entities = self._extract_entities(text)
        
        # æ¦‚å¿µè¯†åˆ«
        concepts = self._extract_concepts(text)
        
        # å…³ç³»è¯†åˆ«
        relationships = self._extract_relationships(text)
        
        # æƒ…æ„Ÿåˆ†æ
        sentiment = self._analyze_sentiment(text)
        
        # æ­£å¼åº¦åˆ†æ
        formality = self._analyze_formality(text)
        
        # å…·ä½“æ€§åˆ†æ
        specificity = self._analyze_specificity(text)
        
        return SemanticFeature(
            keywords=keywords,
            entities=entities,
            concepts=concepts,
            relationships=relationships,
            sentiment=sentiment,
            formality=formality,
            specificity=specificity
        )
    
    def _extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯"""
        # æŠ€æœ¯å…³é”®è¯
        tech_keywords = set()
        for domain, keywords in self.domain_keywords.items():
            for keyword in keywords:
                if keyword.lower() in text.lower():
                    tech_keywords.add(keyword)
        
        # åŠ¨ä½œè¯
        action_words = set()
        for intent_type, patterns in self.intent_patterns.items():
            for pattern in patterns:
                if pattern in text:
                    action_words.add(pattern)
        
        # æ•°é‡å’Œåº¦é‡è¯
        quantity_words = re.findall(r'\d+(?:\.\d+)?(?:%|ms|s|MB|GB|TB|KB|bytes|requests/sec|QPS|TPS)', text)
        
        # åˆå¹¶å¹¶å»é‡
        all_keywords = list(tech_keywords | action_words | set(quantity_words))
        
        return sorted(all_keywords, key=len, reverse=True)[:20]  # è¿”å›å‰20ä¸ªæœ€é‡è¦çš„å…³é”®è¯
    
    def _extract_entities(self, text: str) -> List[Dict[str, str]]:
        """æå–å®ä½“"""
        entities = []
        
        for entity_type, patterns in self.entity_patterns.items():
            for pattern in patterns:
                matches = re.finditer(pattern, text, re.IGNORECASE)
                for match in matches:
                    entity_text = match.group(1) if match.groups() else match.group(0)
                    entities.append({
                        'type': entity_type,
                        'text': entity_text,
                        'position': match.span()
                    })
        
        return entities
    
    def _extract_concepts(self, text: str) -> List[str]:
        """æå–æ¦‚å¿µ"""
        concepts = []
        text_lower = text.lower()
        
        for concept_type, concept_words in self.concept_patterns.items():
            for word in concept_words:
                if word in text_lower:
                    concepts.append(concept_type)
                    break  # æ¯ä¸ªæ¦‚å¿µç±»å‹åªæ·»åŠ ä¸€æ¬¡
        
        return list(set(concepts))
    
    def _extract_relationships(self, text: str) -> List[Dict[str, Any]]:
        """æå–å…³ç³»"""
        relationships = []
        
        # å› æœå…³ç³»
        causal_patterns = [
            r'å› ä¸º(.+?)ï¼Œæ‰€ä»¥(.+?)',
            r'ç”±äº(.+?)ï¼Œ(.+?)',
            r'(.+?)å¯¼è‡´(.+?)',
            r'(.+?)å¼•èµ·(.+?)'
        ]
        
        for pattern in causal_patterns:
            matches = re.finditer(pattern, text)
            for match in matches:
                relationships.append({
                    'type': 'causal',
                    'source': match.group(1).strip(),
                    'target': match.group(2).strip() if len(match.groups()) > 1 else None
                })
        
        # æ¡ä»¶å…³ç³»
        conditional_patterns = [
            r'å¦‚æœ(.+?)ï¼Œ(.+?)',
            r'å½“(.+?)æ—¶ï¼Œ(.+?)',
            r'(.+?)çš„è¯ï¼Œ(.+?)'
        ]
        
        for pattern in conditional_patterns:
            matches = re.finditer(pattern, text)
            for match in matches:
                relationships.append({
                    'type': 'conditional',
                    'condition': match.group(1).strip(),
                    'consequence': match.group(2).strip() if len(match.groups()) > 1 else None
                })
        
        return relationships
    
    def _analyze_sentiment(self, text: str) -> float:
        """åˆ†ææƒ…æ„Ÿ"""
        positive_words = ['å¥½', 'æ£’', 'ä¼˜ç§€', 'å®Œç¾', 'æˆåŠŸ', 'é«˜æ•ˆ', 'å¿«é€Ÿ', 'ç¨³å®š', 'æ»¡æ„']
        negative_words = ['é”™', 'å', 'å¤±è´¥', 'æ…¢', 'é—®é¢˜', 'é”™è¯¯', 'å›°éš¾', 'å¤æ‚', 'ç´§æ€¥']
        
        positive_count = sum(1 for word in positive_words if word in text)
        negative_count = sum(1 for word in negative_words if word in text)
        
        total_sentiment_words = positive_count + negative_count
        if total_sentiment_words == 0:
            return 0.0  # ä¸­æ€§
        
        return (positive_count - negative_count) / total_sentiment_words
    
    def _analyze_formality(self, text: str) -> float:
        """åˆ†ææ­£å¼åº¦"""
        formal_indicators = ['è¯·', 'æ‚¨', 'è´µ', "éå¸¸", "ååˆ†", "ç‰¹åˆ«", "æ„Ÿè°¢", "éº»çƒ¦", "ä¸å¥½æ„æ€"]
        informal_indicators = ['å“ˆ', 'å˜¿', 'å—¯', 'å“¦', 'å•Š', 'å§', 'å˜›', 'å•¦', 'å‘¢']
        
        formal_count = sum(1 for indicator in formal_indicators if indicator in text)
        informal_count = sum(1 for indicator in informal_indicators if indicator in text)
        
        total_indicators = formal_count + informal_count
        if total_indicators == 0:
            return 0.5  # ä¸­ç­‰æ­£å¼åº¦
        
        return formal_count / total_indicators
    
    def _analyze_specificity(self, text: str) -> float:
        """åˆ†æå…·ä½“æ€§"""
        # å…·ä½“æ€§æŒ‡æ ‡ï¼šæ•°å­—ã€ä¸“æœ‰åè¯ã€æŠ€æœ¯æœ¯è¯­ã€æ–‡ä»¶è·¯å¾„ç­‰
        specificity_indicators = [
            r'\d+(?:\.\d+)?',  # æ•°å­—
            r'[A-Z][a-zA-Z]+',  # ä¸“æœ‰åè¯
            r'[a-zA-Z0-9_\-]+\.[a-zA-Z]+',  # æ–‡ä»¶æ‰©å±•å
            r'[a-zA-Z0-9_\-\/]+\.[a-zA-Z0-9_\-\/]+',  # æ–‡ä»¶è·¯å¾„
            r'https?://[^\s]+',  # URL
        ]
        
        specificity_score = 0
        text_length = len(text)
        
        for pattern in specificity_indicators:
            matches = re.findall(pattern, text)
            specificity_score += len(matches) * 0.1
        
        # æ ‡å‡†åŒ–åˆ°0-1èŒƒå›´
        return min(1.0, specificity_score / (text_length / 10))
    
    def _extract_contextual_features(self, text: str, semantic_features: SemanticFeature) -> ContextualFeature:
        """æå–ä¸Šä¸‹æ–‡ç‰¹å¾"""
        # é¢†åŸŸè¯†åˆ«
        domain = self._identify_domain(text, semantic_features)
        
        # è¯­è¨€è¯†åˆ«
        language = self._identify_language(text, semantic_features)
        
        # æ¡†æ¶è¯†åˆ«
        framework = self._identify_framework(text, semantic_features)
        
        # ç¯å¢ƒè¯†åˆ«
        environment = self._identify_environment(text, semantic_features)
        
        # è§„æ¨¡è¯†åˆ«
        scale = self._identify_scale(text, semantic_features)
        
        # åˆ©ç›Šç›¸å…³è€…è¯†åˆ«
        stakeholders = self._identify_stakeholders(text, semantic_features)
        
        return ContextualFeature(
            domain=domain,
            language=language,
            framework=framework,
            environment=environment,
            scale=scale,
            stakeholders=stakeholders
        )
    
    def _identify_domain(self, text: str, semantic_features: SemanticFeature) -> str:
        """è¯†åˆ«é¢†åŸŸ"""
        domain_scores = {}
        text_lower = text.lower()
        
        for domain, keywords in self.domain_keywords.items():
            score = sum(1 for keyword in keywords if keyword in text_lower)
            domain_scores[domain] = score
        
        # è€ƒè™‘å®ä½“ä¸­çš„æŠ€æœ¯æ ˆ
        tech_entities = [e for e in semantic_features.entities if e['type'] == 'technology']
        for entity in tech_entities:
            entity_text = entity['text'].lower()
            for domain, keywords in self.domain_keywords.items():
                if any(keyword in entity_text for keyword in keywords):
                    domain_scores[domain] = domain_scores.get(domain, 0) + 2
        
        if not domain_scores or max(domain_scores.values()) == 0:
            return 'general'
        
        return max(domain_scores, key=domain_scores.get)
    
    def _identify_language(self, text: str, semantic_features: SemanticFeature) -> Optional[str]:
        """è¯†åˆ«ç¼–ç¨‹è¯­è¨€"""
        language_patterns = {
            'python': ['python', 'py', '.py', 'django', 'flask', 'pandas', 'numpy'],
            'javascript': ['javascript', 'js', '.js', 'node', 'nodejs', 'npm', 'react', 'vue', 'angular'],
            'typescript': ['typescript', 'ts', '.ts'],
            'java': ['java', '.java', 'spring', 'maven', 'gradle'],
            'go': ['go', '.go', 'golang'],
            'rust': ['rust', '.rs', 'cargo'],
            'cpp': ['cpp', 'c++', '.cpp', 'gcc', 'clang'],
            'c': ['c', '.c'],
            'html': ['html', '.html', 'css', 'web'],
            'sql': ['sql', 'database', 'mysql', 'postgresql', 'oracle'],
            'bash': ['bash', 'shell', 'sh', 'linux', 'unix']
        }
        
        text_lower = text.lower()
        language_scores = {}
        
        for language, patterns in language_patterns.items():
            score = sum(1 for pattern in patterns if pattern in text_lower)
            language_scores[language] = score
        
        if not language_scores or max(language_scores.values()) == 0:
            return None
        
        return max(language_scores, key=language_scores.get)
    
    def _identify_framework(self, text: str, semantic_features: SemanticFeature) -> Optional[str]:
        """è¯†åˆ«æ¡†æ¶"""
        framework_patterns = {
            'react': ['react', 'jsx', 'tsx', 'hooks', 'component'],
            'vue': ['vue', 'vuex', 'vue-router'],
            'angular': ['angular', 'typescript', 'rxjs'],
            'django': ['django', 'python', 'mvc'],
            'flask': ['flask', 'python', 'blueprint'],
            'spring': ['spring', 'java', 'boot', 'mvc'],
            'express': ['express', 'node', 'middleware'],
            'laravel': ['laravel', 'php', 'mvc', 'eloquent'],
            'tensorflow': ['tensorflow', 'tf', 'neural', 'ml'],
            'pytorch': ['pytorch', 'torch', 'neural', 'ml']
        }
        
        text_lower = text.lower()
        framework_scores = {}
        
        for framework, patterns in framework_patterns.items():
            score = sum(1 for pattern in patterns if pattern in text_lower)
            framework_scores[framework] = score
        
        if not framework_scores or max(framework_scores.values()) == 0:
            return None
        
        return max(framework_scores, key=framework_scores.get)
    
    def _identify_environment(self, text: str, semantic_features: SemanticFeature) -> Optional[str]:
        """è¯†åˆ«ç¯å¢ƒ"""
        environment_patterns = {
            'development': ['å¼€å‘', 'dev', 'æœ¬åœ°', 'æµ‹è¯•', 'debug', 'è°ƒè¯•'],
            'staging': ['é¢„å‘å¸ƒ', 'staging', 'uat', 'éªŒæ”¶', 'æµ‹è¯•ç¯å¢ƒ'],
            'production': ['ç”Ÿäº§', 'prod', 'çº¿ä¸Š', 'æ­£å¼', 'å‘å¸ƒ'],
            'cloud': ['äº‘', 'cloud', 'aws', 'azure', 'gcp', 'é˜¿é‡Œäº‘', 'è…¾è®¯äº‘'],
            'container': ['å®¹å™¨', 'docker', 'kubernetes', 'k8s', 'pod'],
            'mobile': ['ç§»åŠ¨', 'æ‰‹æœº', 'app', 'ios', 'android']
        }
        
        text_lower = text.lower()
        environment_scores = {}
        
        for environment, patterns in environment_patterns.items():
            score = sum(1 for pattern in patterns if pattern in text_lower)
            environment_scores[environment] = score
        
        if not environment_scores or max(environment_scores.values()) == 0:
            return None
        
        return max(environment_scores, key=environment_scores.get)
    
    def _identify_scale(self, text: str, semantic_features: SemanticFeature) -> str:
        """è¯†åˆ«è§„æ¨¡"""
        scale_indicators = {
            'small': ['å°', 'ä¸ªäºº', 'ç®€å•', 'åŸºç¡€', 'åŸå‹', 'demo', 'ç¤ºä¾‹', 'ç»ƒä¹ '],
            'medium': ['ä¸­ç­‰', 'å›¢é˜Ÿ', 'æ ‡å‡†', 'å¸¸è§„', 'ä¼ä¸š', 'å•†ä¸š'],
            'large': ['å¤§', 'å¤§è§„æ¨¡', 'ä¼ä¸šçº§', 'å¤æ‚', 'ç³»ç»Ÿ', 'å¹³å°'],
            'enterprise': ['ä¼ä¸š', 'å•†ä¸š', 'ç”Ÿäº§', 'å…³é”®', 'é‡è¦', 'æ ¸å¿ƒ', 'å¤§å‹']
        }
        
        text_lower = text.lower()
        scale_scores = {}
        
        for scale, indicators in scale_indicators.items():
            score = sum(1 for indicator in indicators if indicator in text_lower)
            scale_scores[scale] = score
        
        if not scale_scores or max(scale_scores.values()) == 0:
            return 'medium'
        
        return max(scale_scores, key=scale_scores.get)
    
    def _identify_stakeholders(self, text: str, semantic_features: SemanticFeature) -> List[str]:
        """è¯†åˆ«åˆ©ç›Šç›¸å…³è€…"""
        stakeholder_patterns = {
            'user': ['ç”¨æˆ·', 'å®¢æˆ·', 'æ¶ˆè´¹è€…', 'è®¿å®¢', 'ä½¿ç”¨è€…'],
            'developer': ['å¼€å‘è€…', 'ç¨‹åºå‘˜', 'å·¥ç¨‹å¸ˆ', 'å¼€å‘å›¢é˜Ÿ'],
            'manager': ['ç»ç†', 'ä¸»ç®¡', 'é¢†å¯¼', 'ç®¡ç†å±‚', 'å†³ç­–è€…'],
            'admin': ['ç®¡ç†å‘˜', 'è¿ç»´', 'ç³»ç»Ÿç®¡ç†å‘˜', 'IT'],
            'business': ['ä¸šåŠ¡', 'äº§å“', 'å¸‚åœº', 'é”€å”®', 'è¿è¥']
        }
        
        text_lower = text.lower()
        stakeholders = []
        
        for stakeholder, patterns in stakeholder_patterns.items():
            if any(pattern in text_lower for pattern in patterns):
                stakeholders.append(stakeholder)
        
        return stakeholders
    
    def _extract_temporal_features(self, text: str, conversation_history: List[str] = None) -> TemporalFeature:
        """æå–æ—¶é—´ç‰¹å¾"""
        # æ—¶é—´æ¡†æ¶è¯†åˆ«
        timeframe = self._identify_timeframe(text)
        
        # ä¾èµ–å…³ç³»è¯†åˆ«
        dependencies = self._identify_dependencies(text)
        
        # åºåˆ—å…³ç³»è¯†åˆ«
        sequence = self._identify_sequence(text)
        
        # çº¦æŸæ¡ä»¶è¯†åˆ«
        constraints = self._identify_constraints(text)
        
        return TemporalFeature(
            timeframe=timeframe,
            dependencies=dependencies,
            sequence=sequence,
            constraints=constraints
        )
    
    def _identify_timeframe(self, text: str) -> str:
        """è¯†åˆ«æ—¶é—´æ¡†æ¶"""
        timeframe_patterns = {
            'immediate': ['ç«‹å³', 'é©¬ä¸Š', 'ç°åœ¨', 'å½“å‰', 'ç«‹åˆ»', 'å³åˆ»'],
            'short': ['å¾ˆå¿«', 'çŸ­æœŸå†…', 'è¿‘æœŸ', 'å‡ å¤©å†…', 'æœ¬å‘¨å†…', 'ä¸‹å‘¨'],
            'medium': ['ä¸­æœŸ', 'ä¸€ä¸ªæœˆ', 'å‡ å‘¨', 'å­£åº¦å†…', 'å‡ ä¸ªæœˆ'],
            'long': ['é•¿æœŸ', 'åŠå¹´', 'ä¸€å¹´', 'æœªæ¥', 'è§„åˆ’', 'è·¯çº¿å›¾']
        }
        
        text_lower = text.lower()
        timeframe_scores = {}
        
        for timeframe, patterns in timeframe_patterns.items():
            score = sum(1 for pattern in patterns if pattern in text_lower)
            timeframe_scores[timeframe] = score
        
        if not timeframe_scores or max(timeframe_scores.values()) == 0:
            return 'medium'
        
        return max(timeframe_scores, key=timeframe_scores.get)
    
    def _identify_dependencies(self, text: str) -> List[str]:
        """è¯†åˆ«ä¾èµ–å…³ç³»"""
        dependency_patterns = [
            r'éœ€è¦(.+?)',
            r'ä¾èµ–(.+?)',
            r'åŸºäº(.+?)',
            r'ä½¿ç”¨(.+?)',
            r'è°ƒç”¨(.+?)',
            r'å¼•ç”¨(.+?)',
            r'å¯¼å…¥(.+?)'
        ]
        
        dependencies = []
        text_lower = text.lower()
        
        for pattern in dependency_patterns:
            matches = re.findall(pattern, text)
            dependencies.extend(matches)
        
        return list(set(dependencies))[:5]  # è¿”å›å‰5ä¸ªæœ€é‡è¦çš„ä¾èµ–
    
    def _identify_sequence(self, text: str) -> List[str]:
        """è¯†åˆ«åºåˆ—å…³ç³»"""
        sequence_patterns = [
            r'é¦–å…ˆ(.+?)ï¼Œ?ç„¶å(.+?)',
            r'ç¬¬ä¸€æ­¥(.+?)ï¼Œ?ç¬¬äºŒæ­¥(.+?)',
            r'å…ˆ(.+?)ï¼Œ?å†(.+?)',
            r'å¼€å§‹(.+?)ï¼Œ?æ¥ç€(.+?)'
        ]
        
        sequence = []
        
        for pattern in sequence_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                if isinstance(match, tuple):
                    sequence.extend(match)
                else:
                    sequence.append(match)
        
        return list(set(sequence))[:5]  # è¿”å›å‰5ä¸ªæœ€é‡è¦çš„åºåˆ—æ­¥éª¤
    
    def _identify_constraints(self, text: str) -> List[str]:
        """è¯†åˆ«çº¦æŸæ¡ä»¶"""
        constraint_patterns = [
            r'é™åˆ¶(.+?)',
            r'çº¦æŸ(.+?)',
            r'è¦æ±‚(.+?)',
            r'å¿…é¡»(.+?)',
            r'ä¸èƒ½(.+?)',
            r'ç¦æ­¢(.+?)',
            r'åªå…è®¸(.+?)'
        ]
        
        constraints = []
        
        for pattern in constraint_patterns:
            matches = re.findall(pattern, text)
            constraints.extend(matches)
        
        return list(set(constraints))[:5]  # è¿”å›å‰5ä¸ªæœ€é‡è¦çš„çº¦æŸ
    
    def _identify_intents(self, text: str, semantic_features: SemanticFeature) -> Tuple[IntentType, List[IntentType]]:
        """è¯†åˆ«æ„å›¾"""
        intent_scores = {}
        text_lower = text.lower()
        
        # åŸºäºå…³é”®è¯åŒ¹é…
        for intent_type, patterns in self.intent_patterns.items():
            score = sum(1 for pattern in patterns if pattern in text_lower)
            intent_scores[intent_type] = score
        
        # åŸºäºè¯­ä¹‰ç‰¹å¾å¢å¼º
        if semantic_features.sentiment > 0.3:  # ç§¯ææƒ…æ„Ÿ
            intent_scores[IntentType.CREATE] = intent_scores.get(IntentType.CREATE, 0) + 1
            intent_scores[IntentType.OPTIMIZE] = intent_scores.get(IntentType.OPTIMIZE, 0) + 1
        
        if semantic_features.sentiment < -0.3:  # æ¶ˆææƒ…æ„Ÿ
            intent_scores[IntentType.DEBUG] = intent_scores.get(IntentType.DEBUG, 0) + 2
            intent_scores[IntentType.ANALYZE] = intent_scores.get(IntentType.ANALYZE, 0) + 1
        
        # åŸºäºæ¦‚å¿µå¢å¼º
        if 'architecture' in semantic_features.concepts:
            intent_scores[IntentType.DESIGN] = intent_scores.get(IntentType.DESIGN, 0) + 2
        
        if 'performance' in semantic_features.concepts:
            intent_scores[IntentType.OPTIMIZE] = intent_scores.get(IntentType.OPTIMIZE, 0) + 2
        
        if 'security' in semantic_features.concepts:
            intent_scores[IntentType.ANALYZE] = intent_scores.get(IntentType.ANALYZE, 0) + 1
            intent_scores[IntentType.TEST] = intent_scores.get(IntentType.TEST, 0) + 1
        
        if 'quality' in semantic_features.concepts:
            intent_scores[IntentType.TEST] = intent_scores.get(IntentType.TEST, 0) + 2
            intent_scores[IntentType.DOCUMENT] = intent_scores.get(IntentType.DOCUMENT, 0) + 1
        
        if not intent_scores or max(intent_scores.values()) == 0:
            return IntentType.ANALYZE, []
        
        # æ’åºè·å–ä¸»è¦æ„å›¾å’Œæ¬¡è¦æ„å›¾
        sorted_intents = sorted(intent_scores.items(), key=lambda x: x[1], reverse=True)
        primary_intent = sorted_intents[0][0]
        secondary_intents = [intent for intent, score in sorted_intents[1:4] if score > 0]
        
        return primary_intent, secondary_intents
    
    def _assess_complexity(self, text: str, semantic_features: SemanticFeature, contextual_features: ContextualFeature) -> ComplexityLevel:
        """è¯„ä¼°å¤æ‚åº¦"""
        complexity_scores = {}
        text_lower = text.lower()
        
        # åŸºäºå…³é”®è¯
        for complexity, indicators in self.complexity_indicators.items():
            score = sum(1 for indicator in indicators if indicator in text_lower)
            complexity_scores[complexity] = score
        
        # åŸºäºè¯­ä¹‰ç‰¹å¾
        if semantic_features.specificity > 0.7:
            complexity_scores[ComplexityLevel.COMPLEX] = complexity_scores.get(ComplexityLevel.COMPLEX, 0) + 1
        
        if len(semantic_features.relationships) > 3:
            complexity_scores[ComplexityLevel.COMPLEX] = complexity_scores.get(ComplexityLevel.COMPLEX, 0) + 1
        
        if len(semantic_features.entities) > 5:
            complexity_scores[ComplexityLevel.MODERATE] = complexity_scores.get(ComplexityLevel.MODERATE, 0) + 1
        
        # åŸºäºä¸Šä¸‹æ–‡ç‰¹å¾
        if contextual_features.scale == 'enterprise':
            complexity_scores[ComplexityLevel.CRITICAL] = complexity_scores.get(ComplexityLevel.CRITICAL, 0) + 2
        elif contextual_features.scale == 'large':
            complexity_scores[ComplexityLevel.COMPLEX] = complexity_scores.get(ComplexityLevel.COMPLEX, 0) + 1
        
        if contextual_features.environment == 'production':
            complexity_scores[ComplexityLevel.CRITICAL] = complexity_scores.get(ComplexityLevel.CRITICAL, 0) + 1
        
        if len(contextual_features.stakeholders) > 2:
            complexity_scores[ComplexityLevel.MODERATE] = complexity_scores.get(ComplexityLevel.MODERATE, 0) + 1
        
        if not complexity_scores or max(complexity_scores.values()) == 0:
            return ComplexityLevel.SIMPLE
        
        return max(complexity_scores, key=complexity_scores.get)
    
    def _assess_urgency(self, text: str, semantic_features: SemanticFeature, temporal_features: TemporalFeature) -> UrgencyLevel:
        """è¯„ä¼°ç´§æ€¥åº¦"""
        urgency_scores = {}
        text_lower = text.lower()
        
        # åŸºäºå…³é”®è¯
        for urgency, indicators in self.urgency_indicators.items():
            score = sum(1 for indicator in indicators if indicator in text_lower)
            urgency_scores[urgency] = score
        
        # åŸºäºæƒ…æ„Ÿç‰¹å¾
        if semantic_features.sentiment < -0.5:  # å¼ºçƒˆè´Ÿé¢æƒ…æ„Ÿ
            urgency_scores[UrgencyLevel.HIGH] = urgency_scores.get(UrgencyLevel.HIGH, 0) + 2
        
        # åŸºäºæ—¶é—´ç‰¹å¾
        if temporal_features.timeframe == 'immediate':
            urgency_scores[UrgencyLevel.CRITICAL] = urgency_scores.get(UrgencyLevel.CRITICAL, 0) + 3
        elif temporal_features.timeframe == 'short':
            urgency_scores[UrgencyLevel.HIGH] = urgency_scores.get(UrgencyLevel.HIGH, 0) + 2
        
        if len(temporal_features.constraints) > 2:
            urgency_scores[UrgencyLevel.HIGH] = urgency_scores.get(UrgencyLevel.HIGH, 0) + 1
        
        if not urgency_scores or max(urgency_scores.values()) == 0:
            return UrgencyLevel.NORMAL
        
        return max(urgency_scores, key=urgency_scores.get)
    
    def _calculate_confidence(self, semantic_features: SemanticFeature, contextual_features: ContextualFeature,
                              temporal_features: TemporalFeature, primary_intent: IntentType,
                              complexity: ComplexityLevel, urgency: UrgencyLevel) -> float:
        """è®¡ç®—ç½®ä¿¡åº¦"""
        confidence_factors = []
        
        # è¯­ä¹‰ç‰¹å¾ç½®ä¿¡åº¦
        semantic_confidence = min(1.0, (len(semantic_features.keywords) + 
                                     len(semantic_features.entities) + 
                                     len(semantic_features.concepts)) / 10.0)
        confidence_factors.append(semantic_confidence)
        
        # ä¸Šä¸‹æ–‡ç‰¹å¾ç½®ä¿¡åº¦
        contextual_confidence = 0.5  # åŸºç¡€åˆ†æ•°
        if contextual_features.domain != 'general':
            contextual_confidence += 0.2
        if contextual_features.language:
            contextual_confidence += 0.1
        if contextual_features.framework:
            contextual_confidence += 0.1
        if contextual_features.environment:
            contextual_confidence += 0.1
        confidence_factors.append(min(1.0, contextual_confidence))
        
        # æ—¶é—´ç‰¹å¾ç½®ä¿¡åº¦
        temporal_confidence = min(1.0, (len(temporal_features.dependencies) + 
                                     len(temporal_features.sequence) + 
                                     len(temporal_features.constraints)) / 5.0)
        confidence_factors.append(temporal_confidence)
        
        # æ„å›¾æ˜ç¡®åº¦
        intent_confidence = 0.7  # åŸºç¡€åˆ†æ•°
        if primary_intent not in [IntentType.ANALYZE]:  # éé»˜è®¤æ„å›¾
            intent_confidence += 0.2
        confidence_factors.append(intent_confidence)
        
        # å¤æ‚åº¦å’Œç´§æ€¥åº¦ä¸€è‡´æ€§
        consistency_confidence = 0.8
        if complexity == ComplexityLevel.CRITICAL and urgency == UrgencyLevel.CRITICAL:
            consistency_confidence = 1.0
        elif complexity == ComplexityLevel.TRIVIAL and urgency == UrgencyLevel.LOW:
            consistency_confidence = 1.0
        confidence_factors.append(consistency_confidence)
        
        return np.mean(confidence_factors)
    
    def _assess_ambiguity(self, text: str, semantic_features: SemanticFeature, secondary_intents: List[IntentType]) -> float:
        """è¯„ä¼°æ­§ä¹‰åº¦"""
        ambiguity_factors = []
        
        # æ¬¡è¦æ„å›¾æ•°é‡
        intent_ambiguity = min(1.0, len(secondary_intents) / 3.0)
        ambiguity_factors.append(intent_ambiguity)
        
        # å…³é”®è¯æ¨¡ç³Šåº¦
        keyword_ambiguity = 0.0
        if len(semantic_features.keywords) < 3:
            keyword_ambiguity = 0.8
        elif len(semantic_features.keywords) < 6:
            keyword_ambiguity = 0.4
        ambiguity_factors.append(keyword_ambiguity)
        
        # å®ä½“è¯†åˆ«æ¨¡ç³Šåº¦
        entity_ambiguity = 0.0
        if len(semantic_features.entities) == 0:
            entity_ambiguity = 0.6
        elif len(semantic_features.entities) < 3:
            entity_ambiguity = 0.3
        ambiguity_factors.append(entity_ambiguity)
        
        # æ–‡æœ¬é•¿åº¦æ¨¡ç³Šåº¦
        text_length = len(text)
        if text_length < 20:
            length_ambiguity = 0.8
        elif text_length < 50:
            length_ambiguity = 0.4
        else:
            length_ambiguity = 0.1
        ambiguity_factors.append(length_ambiguity)
        
        return np.mean(ambiguity_factors)
    
    def _generate_suggested_actions(self, primary_intent: IntentType, complexity: ComplexityLevel,
                                  urgency: UrgencyLevel, contextual_features: ContextualFeature) -> List[str]:
        """ç”Ÿæˆå»ºè®®è¡ŒåŠ¨"""
        actions = []
        
        # åŸºäºæ„å›¾çš„è¡ŒåŠ¨
        intent_actions = {
            IntentType.CREATE: [
                "åˆ¶å®šè¯¦ç»†çš„å®ç°è®¡åˆ’",
                "å‡†å¤‡å¿…è¦çš„å¼€å‘ç¯å¢ƒ",
                "è®¾è®¡ç³»ç»Ÿæ¶æ„",
                "ç¼–å†™æ ¸å¿ƒåŠŸèƒ½ä»£ç "
            ],
            IntentType.ANALYZE: [
                "æ”¶é›†ç›¸å…³æ•°æ®å’Œæ–‡æ¡£",
                "æ‰§è¡Œæ·±åº¦åˆ†æ",
                "ç”Ÿæˆåˆ†ææŠ¥å‘Š",
                "æä¾›æ”¹è¿›å»ºè®®"
            ],
            IntentType.OPTIMIZE: [
                "è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ",
                "åˆ¶å®šä¼˜åŒ–ç­–ç•¥",
                "å®æ–½ä¼˜åŒ–æ–¹æ¡ˆ",
                "éªŒè¯ä¼˜åŒ–æ•ˆæœ"
            ],
            IntentType.DEBUG: [
                "é‡ç°é—®é¢˜ç°è±¡",
                "åˆ†æé”™è¯¯æ—¥å¿—",
                "å®šä½æ ¹æœ¬åŸå› ",
                "å®æ–½ä¿®å¤æ–¹æ¡ˆ"
            ],
            IntentType.LEARN: [
                "æ”¶é›†å­¦ä¹ èµ„æ–™",
                "åˆ¶å®šå­¦ä¹ è®¡åˆ’",
                "å®è·µåº”ç”¨æ‰€å­¦",
                "æ€»ç»“å­¦ä¹ æˆæœ"
            ],
            IntentType.DEPLOY: [
                "å‡†å¤‡éƒ¨ç½²ç¯å¢ƒ",
                "é…ç½®éƒ¨ç½²å‚æ•°",
                "æ‰§è¡Œéƒ¨ç½²æµç¨‹",
                "éªŒè¯éƒ¨ç½²ç»“æœ"
            ],
            IntentType.REFACTOR: [
                "åˆ†æç°æœ‰ä»£ç ç»“æ„",
                "åˆ¶å®šé‡æ„è®¡åˆ’",
                "é€æ­¥é‡æ„ä»£ç ",
                "æµ‹è¯•é‡æ„ç»“æœ"
            ],
            IntentType.TEST: [
                "è®¾è®¡æµ‹è¯•ç”¨ä¾‹",
                "ç¼–å†™æµ‹è¯•ä»£ç ",
                "æ‰§è¡Œæµ‹è¯•éªŒè¯",
                "ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"
            ],
            IntentType.DOCUMENT: [
                "æ•´ç†æ–‡æ¡£ç»“æ„",
                "ç¼–å†™æŠ€æœ¯æ–‡æ¡£",
                "å®¡æŸ¥æ–‡æ¡£è´¨é‡",
                "å‘å¸ƒæ–‡æ¡£"
            ],
            IntentType.RESEARCH: [
                "ç¡®å®šç ”ç©¶æ–¹å‘",
                "æ”¶é›†ç›¸å…³èµ„æ–™",
                "åˆ†æç ”ç©¶æˆæœ",
                "æ€»ç»“ç ”ç©¶ç»“è®º"
            ],
            IntentType.DESIGN: [
                "åˆ†æéœ€æ±‚çº¦æŸ",
                "è®¾è®¡ç³»ç»Ÿæ–¹æ¡ˆ",
                "è¯„ä¼°è®¾è®¡æ–¹æ¡ˆ",
                "è¾“å‡ºè®¾è®¡æ–‡æ¡£"
            ],
            IntentType.INTEGRATE: [
                "åˆ†æé›†æˆéœ€æ±‚",
                "è®¾è®¡é›†æˆæ–¹æ¡ˆ",
                "å®æ–½é›†æˆå·¥ä½œ",
                "æµ‹è¯•é›†æˆæ•ˆæœ"
            ],
            IntentType.MONITOR: [
                "é…ç½®ç›‘æ§ç³»ç»Ÿ",
                "è®¾ç½®ç›‘æ§æŒ‡æ ‡",
                "ç›‘æ§è¿è¡ŒçŠ¶æ€",
                "åˆ†æç›‘æ§æ•°æ®"
            ]
        }
        
        actions.extend(intent_actions.get(primary_intent, []))
        
        # åŸºäºå¤æ‚åº¦çš„è¡ŒåŠ¨è°ƒæ•´
        if complexity in [ComplexityLevel.COMPLEX, ComplexityLevel.CRITICAL]:
            actions.insert(0, "è¿›è¡Œè¯¦ç»†çš„éœ€æ±‚åˆ†æ")
            actions.insert(1, "åˆ¶å®šé¡¹ç›®å®æ–½è®¡åˆ’")
            actions.append("è¿›è¡Œé£é™©è¯„ä¼°å’Œç®¡ç†")
        
        # åŸºäºç´§æ€¥åº¦çš„è¡ŒåŠ¨è°ƒæ•´
        if urgency == UrgencyLevel.CRITICAL:
            actions.insert(0, "ç«‹å³é‡‡å–åº”æ€¥æªæ–½")
            actions.insert(1, "é€šçŸ¥ç›¸å…³åˆ©ç›Šç›¸å…³è€…")
        elif urgency == UrgencyLevel.HIGH:
            actions.insert(0, "ä¼˜å…ˆå¤„ç†å…³é”®ä»»åŠ¡")
        
        # åŸºäºä¸Šä¸‹æ–‡çš„è¡ŒåŠ¨è°ƒæ•´
        if contextual_features.environment == 'production':
            actions.append("ç¡®ä¿ç”Ÿäº§ç¯å¢ƒç¨³å®šæ€§")
            actions.append("å‡†å¤‡å›æ»šæ–¹æ¡ˆ")
        
        if contextual_features.scale == 'enterprise':
            actions.append("è€ƒè™‘ä¼ä¸šçº§å®‰å…¨å’Œåˆè§„è¦æ±‚")
            actions.append("åˆ¶å®šè¯¦ç»†çš„æ²Ÿé€šè®¡åˆ’")
        
        return actions[:8]  # è¿”å›å‰8ä¸ªæœ€é‡è¦çš„è¡ŒåŠ¨
    
    def _identify_risk_factors(self, text: str, complexity: ComplexityLevel, contextual_features: ContextualFeature) -> List[str]:
        """è¯†åˆ«é£é™©å› ç´ """
        risks = []
        
        # åŸºäºå¤æ‚åº¦çš„é£é™©
        if complexity == ComplexityLevel.CRITICAL:
            risks.extend([
                "æŠ€æœ¯å¤æ‚åº¦é«˜ï¼Œå¯èƒ½å½±å“é¡¹ç›®è¿›åº¦",
                "éœ€è¦æ›´å¤šèµ„æºå’Œæ—¶é—´æŠ•å…¥",
                "å­˜åœ¨è¾ƒé«˜çš„æŠ€æœ¯é£é™©"
            ])
        elif complexity == ComplexityLevel.COMPLEX:
            risks.extend([
                "éœ€è¦ä»”ç»†è§„åˆ’å’Œç®¡ç†",
                "å¯èƒ½é‡åˆ°æŠ€æœ¯æŒ‘æˆ˜",
                "éœ€è¦å›¢é˜Ÿåä½œé…åˆ"
            ])
        
        # åŸºäºä¸Šä¸‹æ–‡çš„é£é™©
        if contextual_features.environment == 'production':
            risks.extend([
                "ç”Ÿäº§ç¯å¢ƒå˜æ›´é£é™©",
                "å¯èƒ½å½±å“ç°æœ‰ç³»ç»Ÿç¨³å®šæ€§",
                "éœ€è¦å……åˆ†çš„æµ‹è¯•éªŒè¯"
            ])
        
        if contextual_features.scale == 'enterprise':
            risks.extend([
                "ä¼ä¸šçº§éƒ¨ç½²å¤æ‚åº¦é«˜",
                "éœ€è¦è€ƒè™‘å®‰å…¨å’Œåˆè§„è¦æ±‚",
                "åˆ©ç›Šç›¸å…³è€…ä¼—å¤šï¼Œæ²Ÿé€šæˆæœ¬é«˜"
            ])
        
        # åŸºäºæ–‡æœ¬å†…å®¹çš„é£é™©
        risk_indicators = ['é£é™©', 'é—®é¢˜', 'é”™è¯¯', 'å¤±è´¥', 'å›°éš¾', 'æŒ‘æˆ˜', 'å¤æ‚', 'ç´§æ€¥']
        text_lower = text.lower()
        
        for indicator in risk_indicators:
            if indicator in text_lower:
                risks.append(f"æ–‡æœ¬ä¸­æåˆ°äº†{indicator}ï¼Œéœ€è¦ç‰¹åˆ«å…³æ³¨")
        
        return list(set(risks))[:5]  # è¿”å›å‰5ä¸ªæœ€é‡è¦çš„é£é™©
    
    def _define_success_criteria(self, primary_intent: IntentType, complexity: ComplexityLevel,
                                contextual_features: ContextualFeature) -> List[str]:
        """å®šä¹‰æˆåŠŸæ ‡å‡†"""
        criteria = []
        
        # åŸºäºæ„å›¾çš„æˆåŠŸæ ‡å‡†
        intent_criteria = {
            IntentType.CREATE: [
                "åŠŸèƒ½å®ç°å®Œæ•´ä¸”ç¬¦åˆéœ€æ±‚",
                "ä»£ç è´¨é‡è¾¾åˆ°æ ‡å‡†",
                "é€šè¿‡ç›¸å…³æµ‹è¯•éªŒè¯",
                "æ–‡æ¡£å®Œå–„æ¸…æ™°"
            ],
            IntentType.ANALYZE: [
                "åˆ†æç»“æœå‡†ç¡®å¯é ",
                "å‘ç°å…³é”®é—®é¢˜å’Œæœºä¼š",
                "æä¾›å¯è¡Œçš„æ”¹è¿›å»ºè®®",
                "æŠ¥å‘Šå†…å®¹è¯¦å®æœ‰ç”¨"
            ],
            IntentType.OPTIMIZE: [
                "æ€§èƒ½æŒ‡æ ‡æ˜¾è‘—æ”¹å–„",
                "ç³»ç»Ÿç¨³å®šæ€§ä¸å—å½±å“",
                "ä¼˜åŒ–æ•ˆæœå¯é‡åŒ–éªŒè¯",
                "èµ„æºä½¿ç”¨æ›´åŠ é«˜æ•ˆ"
            ],
            IntentType.DEBUG: [
                "é—®é¢˜å¾—åˆ°æ ¹æœ¬è§£å†³",
                "ä¿®å¤æ–¹æ¡ˆç¨³å®šå¯é ",
                "é—®é¢˜ä¸å†é‡ç°",
                "é¢„é˜²æªæ–½åˆ°ä½"
            ],
            IntentType.LEARN: [
                "æŒæ¡äº†ç›®æ ‡çŸ¥è¯†å’ŒæŠ€èƒ½",
                "èƒ½å¤Ÿç‹¬ç«‹åº”ç”¨æ‰€å­¦",
                "å­¦ä¹ æˆæœå¯éªŒè¯",
                "å»ºç«‹äº†æŒç»­å­¦ä¹ æœºåˆ¶"
            ],
            IntentType.DEPLOY: [
                "éƒ¨ç½²è¿‡ç¨‹é¡ºåˆ©å®Œæˆ",
                "ç³»ç»Ÿè¿è¡Œæ­£å¸¸ç¨³å®š",
                "æ€§èƒ½æŒ‡æ ‡è¾¾åˆ°é¢„æœŸ",
                "ç›‘æ§å‘Šè­¦é…ç½®å®Œå–„"
            ],
            IntentType.REFACTOR: [
                "ä»£ç ç»“æ„æ›´åŠ æ¸…æ™°",
                "å¯ç»´æŠ¤æ€§æ˜¾è‘—æå‡",
                "åŠŸèƒ½è¡Œä¸ºä¿æŒä¸€è‡´",
                "æµ‹è¯•è¦†ç›–ç‡ä¸é™ä½"
            ],
            IntentType.TEST: [
                "æµ‹è¯•è¦†ç›–ç‡è¾¾åˆ°è¦æ±‚",
                "å‘ç°å¹¶ä¿®å¤äº†å…³é”®é—®é¢˜",
                "æµ‹è¯•ç»“æœå¯é‡ç°",
                "æµ‹è¯•æŠ¥å‘Šè¯¦å®å‡†ç¡®"
            ],
            IntentType.DOCUMENT: [
                "æ–‡æ¡£å†…å®¹å‡†ç¡®å®Œæ•´",
                "ç»“æ„æ¸…æ™°æ˜“äºç†è§£",
                "ç¤ºä¾‹å’Œè¯´æ˜å……åˆ†",
                "æ–‡æ¡£æ ¼å¼è§„èŒƒç»Ÿä¸€"
            ],
            IntentType.RESEARCH: [
                "ç ”ç©¶ç›®æ ‡æ˜ç¡®è¾¾æˆ",
                "æ•°æ®æ”¶é›†å……åˆ†å¯é ",
                "åˆ†æç»“è®ºæœ‰ç†æœ‰æ®",
                "ç ”ç©¶æˆæœå…·æœ‰å®ç”¨ä»·å€¼"
            ],
            IntentType.DESIGN: [
                "è®¾è®¡æ–¹æ¡ˆæ»¡è¶³æ‰€æœ‰éœ€æ±‚",
                "æŠ€æœ¯é€‰å‹åˆç†å¯è¡Œ",
                "æ¶æ„è®¾è®¡å¯æ‰©å±•",
                "è®¾è®¡æ–‡æ¡£å®Œæ•´è§„èŒƒ"
            ],
            IntentType.INTEGRATE: [
                "é›†æˆåŠŸèƒ½æ­£å¸¸å·¥ä½œ",
                "æ•°æ®ä¼ è¾“å‡†ç¡®å¯é ",
                "ç³»ç»Ÿç¨³å®šæ€§ä¸å—å½±å“",
                "é›†æˆæ–¹æ¡ˆå¯ç»´æŠ¤"
            ],
            IntentType.MONITOR: [
                "ç›‘æ§ç³»ç»Ÿæ­£å¸¸è¿è¡Œ",
                "å…³é”®æŒ‡æ ‡æœ‰æ•ˆç›‘æ§",
                "å‘Šè­¦æœºåˆ¶åŠæ—¶å‡†ç¡®",
                "ç›‘æ§æ•°æ®å¯ç”¨äºå†³ç­–"
            ]
        }
        
        criteria.extend(intent_criteria.get(primary_intent, []))
        
        # åŸºäºå¤æ‚åº¦çš„æ ‡å‡†è°ƒæ•´
        if complexity == ComplexityLevel.CRITICAL:
            criteria.append("é¡¹ç›®æŒ‰æ—¶æŒ‰è´¨é‡äº¤ä»˜")
            criteria.append("é£é™©å¾—åˆ°æœ‰æ•ˆæ§åˆ¶")
            criteria.append("åˆ©ç›Šç›¸å…³è€…æ»¡æ„åº¦è¾¾æ ‡")
        
        # åŸºäºä¸Šä¸‹æ–‡çš„æ ‡å‡†è°ƒæ•´
        if contextual_features.environment == 'production':
            criteria.append("ç”Ÿäº§ç¯å¢ƒé›¶äº‹æ•…")
            criteria.append("ç”¨æˆ·ä½“éªŒä¸å—å½±å“")
        
        if contextual_features.scale == 'enterprise':
            criteria.append("ç¬¦åˆä¼ä¸šçº§æ ‡å‡†")
            criteria.append("é€šè¿‡å®‰å…¨å’Œåˆè§„å®¡æŸ¥")
        
        return criteria[:6]  # è¿”å›å‰6ä¸ªæœ€é‡è¦çš„æ ‡å‡†
    
    def get_understanding_statistics(self) -> Dict[str, Any]:
        """è·å–ç†è§£ç»Ÿè®¡ä¿¡æ¯"""
        if not self.context_history:
            return {
                'total_contexts': 0,
                'average_confidence': 0.0,
                'most_common_intent': None,
                'most_common_complexity': None,
                'most_common_urgency': None,
                'average_processing_time': 0.0
            }
        
        total_contexts = len(self.context_history)
        
        # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
        confidences = [ctx['context'].confidence for ctx in self.context_history]
        avg_confidence = np.mean(confidences)
        
        # æœ€å¸¸è§çš„æ„å›¾
        intents = [ctx['context'].primary_intent.value for ctx in self.context_history]
        intent_counter = Counter(intents)
        most_common_intent = intent_counter.most_common(1)[0][0] if intent_counter else None
        
        # æœ€å¸¸è§çš„å¤æ‚åº¦
        complexities = [ctx['context'].complexity.value for ctx in self.context_history]
        complexity_counter = Counter(complexities)
        most_common_complexity = complexity_counter.most_common(1)[0][0] if complexity_counter else None
        
        # æœ€å¸¸è§çš„ç´§æ€¥åº¦
        urgencies = [ctx['context'].urgency.value for ctx in self.context_history]
        urgency_counter = Counter(urgencies)
        most_common_urgency = urgency_counter.most_common(1)[0][0] if urgency_counter else None
        
        # å¹³å‡å¤„ç†æ—¶é—´
        processing_times = [ctx['processing_time'] for ctx in self.context_history]
        avg_processing_time = np.mean(processing_times)
        
        return {
            'total_contexts': total_contexts,
            'average_confidence': avg_confidence,
            'most_common_intent': most_common_intent,
            'most_common_complexity': most_common_complexity,
            'most_common_urgency': most_common_urgency,
            'average_processing_time': avg_processing_time,
            'intent_distribution': dict(intent_counter),
            'complexity_distribution': dict(complexity_counter),
            'urgency_distribution': dict(urgency_counter)
        }
    
    def learn_from_feedback(self, context: EnhancedContext, feedback: Dict[str, Any]):
        """ä»åé¦ˆä¸­å­¦ä¹ """
        if not self.learning_enabled:
            return
        
        # è®°å½•åé¦ˆæ•°æ®
        feedback_data = {
            'timestamp': time.time(),
            'context_id': id(context),
            'predicted_intent': context.primary_intent.value,
            'predicted_complexity': context.complexity.value,
            'predicted_urgency': context.urgency.value,
            'predicted_confidence': context.confidence,
            'actual_intent': feedback.get('actual_intent'),
            'actual_complexity': feedback.get('actual_complexity'),
            'actual_urgency': feedback.get('actual_urgency'),
            'satisfaction_score': feedback.get('satisfaction_score', 0.5),
            'corrections': feedback.get('corrections', [])
        }
        
        # æ›´æ–°æ¨¡å¼æƒé‡ï¼ˆç®€åŒ–ç‰ˆå­¦ä¹ æœºåˆ¶ï¼‰
        if feedback.get('actual_intent') and feedback['actual_intent'] != context.primary_intent.value:
            # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„å­¦ä¹ ç®—æ³•
            logger.info(f"ğŸ§  å­¦ä¹ åé¦ˆ: é¢„æµ‹æ„å›¾ {context.primary_intent.value} -> å®é™…æ„å›¾ {feedback['actual_intent']}")
        
        if feedback.get('satisfaction_score', 0.5) < 0.3:
            logger.warning(f"ğŸ§  ä½æ»¡æ„åº¦åé¦ˆ: {feedback.get('corrections', 'æ— å…·ä½“åé¦ˆ')}")
        
        logger.debug(f"ğŸ§  åé¦ˆå­¦ä¹ å®Œæˆ: æ»¡æ„åº¦ {feedback.get('satisfaction_score', 0.5):.2f}")

# ç¤ºä¾‹ä½¿ç”¨
def example_enhanced_context_usage():
    """ç¤ºä¾‹å¢å¼ºä¸Šä¸‹æ–‡ç†è§£ä½¿ç”¨"""
    understanding = EnhancedContextUnderstanding()
    
    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        "æˆ‘éœ€è¦ä¼˜åŒ–Pythonæœºå™¨å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯å›¾åƒåˆ†ç±»éƒ¨åˆ†",
        "å¸®æˆ‘åœ¨ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²Reactåº”ç”¨ï¼Œéœ€è¦ç¡®ä¿é«˜å¯ç”¨æ€§å’Œå®‰å…¨æ€§",
        "åˆ†æè¿™ä¸ªå¤æ‚çš„å¾®æœåŠ¡æ¶æ„é—®é¢˜ï¼Œæ‰¾å‡ºæ€§èƒ½ç“¶é¢ˆå¹¶æä¾›è§£å†³æ–¹æ¡ˆ",
        "åˆ›å»ºä¸€ä¸ªç”¨æˆ·è®¤è¯ç³»ç»Ÿï¼ŒåŒ…æ‹¬æ³¨å†Œã€ç™»å½•ã€å¯†ç é‡ç½®åŠŸèƒ½",
        "è°ƒè¯•è¿™ä¸ªå†…å­˜æ³„æ¼é—®é¢˜ï¼Œç³»ç»Ÿåœ¨é«˜å¹¶å‘æƒ…å†µä¸‹ä¼šå‡ºç°å´©æºƒ"
    ]
    
    for i, test_case in enumerate(test_cases):
        print(f"\nğŸ§  æµ‹è¯•ç”¨ä¾‹ {i+1}: {test_case}")
        
        context = understanding.understand_context(test_case)
        
        print(f"ğŸ¯ ä¸»è¦æ„å›¾: {context.primary_intent.value}")
        print(f"ğŸ”§ æ¬¡è¦æ„å›¾: {[intent.value for intent in context.secondary_intents]}")
        print(f"ğŸ“Š å¤æ‚åº¦: {context.complexity.value}")
        print(f"âš¡ ç´§æ€¥åº¦: {context.urgency.value}")
        print(f"ğŸ“ˆ ç½®ä¿¡åº¦: {context.confidence:.2f}")
        print(f"â“ æ­§ä¹‰åº¦: {context.ambiguity_score:.2f}")
        print(f"ğŸŒ é¢†åŸŸ: {context.contextual_features.domain}")
        print(f"ğŸ’» è¯­è¨€: {context.contextual_features.language or 'æœªæŒ‡å®š'}")
        print(f"ğŸ—ï¸  æ¡†æ¶: {context.contextual_features.framework or 'æœªæŒ‡å®š'}")
        print(f"ğŸ“ è§„æ¨¡: {context.contextual_features.scale}")
        print(f"ğŸ¤ åˆ©ç›Šç›¸å…³è€…: {context.contextual_features.stakeholders}")
        print(f"â° æ—¶é—´æ¡†æ¶: {context.temporal_features.timeframe}")
        
        print(f"ğŸ’¡ å»ºè®®è¡ŒåŠ¨:")
        for j, action in enumerate(context.suggested_actions[:3], 1):
            print(f"  {j}. {action}")
        
        print(f"âš ï¸  é£é™©å› ç´ :")
        for j, risk in enumerate(context.risk_factors[:2], 1):
            print(f"  {j}. {risk}")
        
        print(f"âœ… æˆåŠŸæ ‡å‡†:")
        for j, criterion in enumerate(context.success_criteria[:2], 1):
            print(f"  {j}. {criterion}")
        
        print("-" * 60)
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = understanding.get_understanding_statistics()
    print(f"\nğŸ“Š ç†è§£ç»Ÿè®¡:")
    print(f"  æ€»å¤„ç†ä¸Šä¸‹æ–‡: {stats['total_contexts']}")
    print(f"  å¹³å‡ç½®ä¿¡åº¦: {stats['average_confidence']:.2f}")
    print(f"  æœ€å¸¸è§æ„å›¾: {stats['most_common_intent']}")
    print(f"  æœ€å¸¸è§å¤æ‚åº¦: {stats['most_common_complexity']}")
    print(f"  æœ€å¸¸è§ç´§æ€¥åº¦: {stats['most_common_urgency']}")
    print(f"  å¹³å‡å¤„ç†æ—¶é—´: {stats['average_processing_time']:.3f}ç§’")

if __name__ == "__main__":
    example_enhanced_context_usage()
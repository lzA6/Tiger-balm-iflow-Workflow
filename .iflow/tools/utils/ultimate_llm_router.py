#!/usr/bin/env python3
"""
ğŸŒŸ Intelligent LLM Router - æ™ºèƒ½LLMæ¨¡å‹è·¯ç”±å™¨
å®ç°100%å…¼å®¹æ€§çš„æ™ºèƒ½æ¨¡å‹é€‰æ‹©å’Œè·¯ç”±
"""

import asyncio
import json
import time
import logging
import hashlib
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from enum import Enum
from collections import defaultdict, deque
import numpy as np

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TaskType(Enum):
    """ä»»åŠ¡ç±»å‹æšä¸¾"""
    CODE_GENERATION = "code_generation"
    REASONING = "reasoning"
    CREATIVE = "creative"
    ANALYSIS = "analysis"
    TRANSLATION = "translation"
    SUMMARIZATION = "summarization"
    DEBUGGING = "debugging"
    ARCHITECTURE = "architecture"
    DOCUMENTATION = "documentation"
    OPTIMIZATION = "optimization"
    SECURITY = "security"
    TESTING = "testing"

class ModelTier(Enum):
    """æ¨¡å‹å±‚çº§æšä¸¾"""
    PREMIUM = "premium"
    STANDARD = "standard"
    ECONOMY = "economy"
    LOCAL = "local"

@dataclass
class ModelCapability:
    """æ¨¡å‹èƒ½åŠ›å®šä¹‰"""
    name: str
    provider: str
    tier: ModelTier
    reasoning_score: float
    creativity_score: float
    code_score: float
    speed_score: float
    cost_score: float
    context_window: int
    languages: List[str]
    specialties: List[str]
    avg_response_time: float
    success_rate: float
    quality_score: float
    api_config: Dict[str, Any]

@dataclass
class TaskFeatures:
    """ä»»åŠ¡ç‰¹å¾"""
    length: int
    complexity: float
    language: str
    has_code: bool
    is_creative: bool
    requires_reasoning: bool
    urgency: float
    task_type: Optional[TaskType]
    context_requirement: int
    quality_requirement: float
    cost_sensitivity: float

@dataclass
class RoutingDecision:
    """è·¯ç”±å†³ç­–"""
    selected_model: str
    confidence: float
    reasoning: str
    alternative_models: List[str]
    expected_performance: Dict[str, float]
    fallback_plan: List[str]

class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡"""
    response_times: deque
    success_rates: Dict[str, float]
    quality_scores: Dict[str, float]
    error_counts: Dict[str, int]
    last_updated: float

class IntelligentLLMRouter:
    """æ™ºèƒ½LLMè·¯ç”±å™¨"""
    
    def __init__(self, config_path: str = None):
        self.config = self._load_config(config_path)
        self.models = self._initialize_models()
        self.task_weights = self.config.get("task_weights", {})
        self.performance_metrics = PerformanceMetrics(
            response_times=deque(maxlen=1000),
            success_rates={},
            quality_scores={},
            error_counts={},
            last_updated=time.time()
        )
        self.routing_cache = {}
        self.performance_cache = {}
        self.model_availability = {}
        
        # åˆå§‹åŒ–æ¨¡å‹å¯ç”¨æ€§
        self._initialize_model_availability()
        
        logger.info(f"æ™ºèƒ½LLMè·¯ç”±å™¨åˆå§‹åŒ–å®Œæˆï¼ŒåŠ è½½äº† {len(self.models)} ä¸ªæ¨¡å‹")
    
    def _load_config(self, config_path: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if config_path:
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        
        # è¿”å›é»˜è®¤é…ç½®
        return {
            "task_weights": {
                "code_generation": {
                    "reasoning": 0.25, "code": 0.40, "speed": 0.15,
                    "creativity": 0.10, "cost": 0.10
                },
                "reasoning": {
                    "reasoning": 0.50, "code": 0.10, "speed": 0.15,
                    "creativity": 0.15, "cost": 0.10
                },
                "creative": {
                    "reasoning": 0.25, "code": 0.10, "speed": 0.10,
                    "creativity": 0.40, "cost": 0.15
                }
            },
            "thresholds": {
                "response_time_max": 30000,
                "success_rate_min": 0.90,
                "quality_score_min": 0.80
            }
        }
    
    def _initialize_models(self) -> Dict[str, ModelCapability]:
        """åˆå§‹åŒ–æ¨¡å‹æ³¨å†Œè¡¨"""
        models = {}
        
        # ä»é…ç½®ä¸­åŠ è½½æ¨¡å‹
        model_registry = self.config.get("model_registry", {})
        
        for provider, provider_models in model_registry.items():
            for model_name, model_config in provider_models.items():
                full_name = model_config.get("name", model_name)
                
                # ç¡®å®šæ¨¡å‹å±‚çº§
                tier = self._determine_tier(provider, model_name)
                
                # æå–èƒ½åŠ›åˆ†æ•°
                capabilities = model_config.get("capabilities", {})
                
                # æå–APIé…ç½®
                api_config = model_config.get("api_config", {})
                
                # æå–æ€§èƒ½é…ç½®
                performance_profile = model_config.get("performance_profile", {})
                
                # åˆ›å»ºæ¨¡å‹èƒ½åŠ›å¯¹è±¡
                model = ModelCapability(
                    name=full_name,
                    provider=provider,
                    tier=tier,
                    reasoning_score=capabilities.get("reasoning", 0.5),
                    creativity_score=capabilities.get("creativity", 0.5),
                    code_score=capabilities.get("code", 0.5),
                    speed_score=capabilities.get("speed", 0.5),
                    cost_score=capabilities.get("cost", 0.5),
                    context_window=capabilities.get("context_window", 4096),
                    languages=capabilities.get("languages", ["en"]),
                    specialties=capabilities.get("specialties", []),
                    avg_response_time=performance_profile.get("avg_response_time", 2000),
                    success_rate=performance_profile.get("success_rate", 0.9),
                    quality_score=performance_profile.get("quality_score", 0.8),
                    api_config=api_config
                )
                
                models[full_name] = model
        
        return models
    
    def _determine_tier(self, provider: str, model_name: str) -> ModelTier:
        """ç¡®å®šæ¨¡å‹å±‚çº§"""
        premium_models = ["gpt-4", "claude-3-opus", "gemini-pro"]
        standard_models = ["gpt-4-turbo", "claude-3-sonnet", "qwen-max", "glm-4"]
        economy_models = ["ernie-bot-4", "deepseek-coder", "moonshot-v1-8k"]
        local_models = ["llama-2", "mistral"]
        
        if model_name in premium_models:
            return ModelTier.PREMIUM
        elif model_name in standard_models:
            return ModelTier.STANDARD
        elif model_name in economy_models:
            return ModelTier.ECONOMY
        elif model_name in local_models:
            return ModelTier.LOCAL
        else:
            return ModelTier.STANDARD
    
    def _initialize_model_availability(self):
        """åˆå§‹åŒ–æ¨¡å‹å¯ç”¨æ€§"""
        for model_name in self.models:
            self.model_availability[model_name] = {
                "available": True,
                "last_check": time.time(),
                "consecutive_failures": 0,
                "max_failures": 3
            }
    
    def _analyze_task_features(self, task: str, task_type: Optional[TaskType] = None) -> TaskFeatures:
        """åˆ†æä»»åŠ¡ç‰¹å¾"""
        # åŸºç¡€ç‰¹å¾
        length = len(task)
        
        # å¤æ‚åº¦è¯„ä¼°
        complexity = self._assess_complexity(task)
        
        # è¯­è¨€æ£€æµ‹
        language = self._detect_language(task)
        
        # ä»£ç æ£€æµ‹
        has_code = self._contains_code(task)
        
        # åˆ›æ„æ€§æ£€æµ‹
        is_creative = self._is_creative_task(task)
        
        # æ¨ç†éœ€æ±‚æ£€æµ‹
        requires_reasoning = self._requires_reasoning(task)
        
        # ç´§æ€¥ç¨‹åº¦è¯„ä¼°
        urgency = self._assess_urgency(task)
        
        # æ¨æ–­ä»»åŠ¡ç±»å‹
        if task_type is None:
            task_type = self._infer_task_type(task, has_code, is_creative, requires_reasoning)
        
        # ä¸Šä¸‹æ–‡éœ€æ±‚
        context_requirement = self._estimate_context_requirement(task, has_code)
        
        # è´¨é‡è¦æ±‚
        quality_requirement = self._estimate_quality_requirement(task)
        
        # æˆæœ¬æ•æ„Ÿåº¦
        cost_sensitivity = self._estimate_cost_sensitivity(task)
        
        return TaskFeatures(
            length=length,
            complexity=complexity,
            language=language,
            has_code=has_code,
            is_creative=is_creative,
            requires_reasoning=requires_reasoning,
            urgency=urgency,
            task_type=task_type,
            context_requirement=context_requirement,
            quality_requirement=quality_requirement,
            cost_sensitivity=cost_sensitivity
        )
    
    def _assess_complexity(self, task: str) -> float:
        """è¯„ä¼°ä»»åŠ¡å¤æ‚åº¦"""
        complexity_indicators = [
            "æ¶æ„", "è®¾è®¡", "ç³»ç»Ÿ", "é›†æˆ", "ä¼˜åŒ–", "é‡æ„",
            "ç®—æ³•", "æ•°æ®ç»“æ„", "æ€§èƒ½", "å®‰å…¨", "éƒ¨ç½²",
            "architecture", "design", "system", "integration", "optimization", "refactoring",
            "algorithm", "data structure", "performance", "security", "deployment"
        ]
        
        complexity_score = 0.0
        for indicator in complexity_indicators:
            if indicator in task.lower():
                complexity_score += 0.2
        
        # é•¿åº¦å¤æ‚åº¦
        if len(task) > 1000:
            complexity_score += 0.3
        elif len(task) > 500:
            complexity_score += 0.2
        
        return min(complexity_score, 1.0)
    
    def _detect_language(self, task: str) -> str:
        """æ£€æµ‹ä»»åŠ¡è¯­è¨€"""
        chinese_chars = len([c for c in task if '\u4e00' <= c <= '\u9fff'])
        if chinese_chars > len(task) * 0.3:
            return "zh"
        return "en"
    
    def _contains_code(self, task: str) -> bool:
        """æ£€æµ‹æ˜¯å¦åŒ…å«ä»£ç """
        code_indicators = [
            "```", "def ", "function ", "class ", "import ", "from ",
            "var ", "let ", "const ", "=>", "return ", "if (",
            "for (", "while (", "try {", "catch (", "throw new"
        ]
        
        return any(indicator in task for indicator in code_indicators)
    
    def _is_creative_task(self, task: task: str) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸ºåˆ›æ„ä»»åŠ¡"""
        creative_indicators = [
            "åˆ›æ„", "è®¾è®¡", "åˆ›ä½œ", "æƒ³è±¡", "åˆ›æ–°", "è‰ºæœ¯",
            "creative", "design", "create", "imagine", "innovate", "art"
        ]
        
        return any(indicator in task.lower() for indicator in creative_indicators)
    
    def _requires_reasoning(self, task: task) -> bool:
        """æ£€æµ‹æ˜¯å¦éœ€è¦æ¨ç†"""
        reasoning_indicators = [
            "åˆ†æ", "æ¨ç†", "è§£é‡Š", "åŸå› ", "ä¸ºä»€ä¹ˆ", "å¦‚ä½•",
            "analyze", "reason", "explain", "why", "how", "because"
        ]
        
        return any(indicator in task.lower() for indicator in reasoning_indicators)
    
    def _assess_urgency(self, task: task) -> float:
        """è¯„ä¼°ç´§æ€¥ç¨‹åº¦"""
        urgency_indicators = [
            "ç´§æ€¥", "ç«‹å³", "é©¬ä¸Š", "å°½å¿«", "urgent", "immediately",
            "asap", "right now", "quickly"
        ]
        
        urgency_score = 0.0
        for indicator in urgency_indicators:
            if indicator in task.lower():
                urgency_score += 0.3
        
        return min(urgency_score, 1.0)
    
    def _infer_task_type(self, task: str, has_code: bool, is_creative: bool, requires_reasoning: bool) -> TaskType:
        """æ¨æ–­ä»»åŠ¡ç±»å‹"""
        if has_code:
            return TaskType.CODE_GENERATION
        elif is_creative:
            return TaskType.CREATIVE
        elif requires_reasoning:
            return TaskType.REASONING
        else:
            return TaskType.ANALYSIS
    
    def _estimate_context_requirement(self, task: str, has_code: bool) -> int:
        """ä¼°ç®—ä¸Šä¸‹æ–‡éœ€æ±‚"""
        base_requirement = len(task)
        
        if has_code:
            base_requirement *= 1.5
        
        # å¤æ‚åº¦è°ƒæ•´
        complexity = self._assess_complexity(task)
        base_requirement *= (1 + complexity)
        
        return int(base_requirement)
    
    def _estimate_quality_requirement(self, task: str) -> float:
        """ä¼°ç®—è´¨é‡è¦æ±‚"""
        quality_indicators = [
            "é«˜è´¨é‡", "ç”Ÿäº§çº§åˆ«", "ä¸“ä¸š", "ç²¾ç¡®", "å‡†ç¡®",
            "high quality", "production", "professional", "precise", "accurate"
        ]
        
        quality_score = 0.7  # é»˜è®¤è´¨é‡è¦æ±‚
        for indicator in quality_indicators:
            if indicator in task.lower():
                quality_score += 0.1
        
        return min(quality_score, 1.0)
    
    def _estimate_cost_sensitivity(self, task: str) -> float:
        """ä¼°ç®—æˆæœ¬æ•æ„Ÿåº¦"""
        cost_indicators = [
            "ä¾¿å®œ", "ç»æµ", "èŠ‚çœ", "ä½æˆæœ¬", "budget",
            "cheap", "economical", "save", "low cost", "budget"
        ]
        
        cost_sensitivity = 0.5  # é»˜è®¤æˆæœ¬æ•æ„Ÿåº¦
        for indicator in cost_indicators:
            if indicator in task.lower():
                cost_sensitivity += 0.1
        
        return min(cost_sensitivity, 1.0)
    
    def _calculate_model_score(self, model: ModelCapability, features: TaskFeatures, weights: Dict[str, float]) -> float:
        """è®¡ç®—æ¨¡å‹å¾—åˆ†"""
        # åŸºç¡€èƒ½åŠ›å¾—åˆ†
        base_score = (
            model.reasoning_score * weights.get("reasoning", 0.2) +
            model.creativity_score * weights.get("creativity", 0.2) +
            model.code_score * weights.get("code", 0.2) +
            model.speed_score * weights.get("speed", 0.2) +
            model.cost_score * weights.get("cost", 0.2)
        )
        
        # ä»»åŠ¡åŒ¹é…åº¦è°ƒæ•´
        task_match_bonus = self._calculate_task_match_bonus(features, model)
        
        # æ€§èƒ½è°ƒæ•´
        performance_adjustment = self._get_performance_adjustment(model.name)
        
        # ä¸Šä¸‹æ–‡çª—å£é€‚é…
        context_penalty = self._calculate_context_penalty(features, model)
        
        # è¯­è¨€åŒ¹é…è°ƒæ•´
        language_bonus = 0.1 if features.language in model.languages else 0.0
        
        # å¯ç”¨æ€§è°ƒæ•´
        availability_penalty = self._calculate_availability_penalty(model.name)
        
        # ç»¼åˆå¾—åˆ†
        final_score = (
            base_score * 
            (1 + task_match_bonus + language_bonus) * 
            performance_adjustment * 
            (1 - context_penalty) * 
            (1 - availability_penalty)
        )
        
        return final_score
    
    def _calculate_task_match_bonus(self, features: TaskFeatures, model: ModelCapability) -> float:
        """è®¡ç®—ä»»åŠ¡åŒ¹é…åº¦å¥–åŠ±"""
        bonus = 0.0
        
        # ä¸“ä¸šé¢†åŸŸåŒ¹é…
        if features.has_code and "code" in model.specialties:
            bonus += 0.15
        
        if features.requires_reasoning and "reasoning" in model.specialties:
            bonus += 0.1
        
        if features.is_creative and "creative" in model.specialties:
            bonus += 0.1
        
        # è´¨é‡è¦æ±‚åŒ¹é…
        if features.quality_requirement > 0.9 and model.quality_score > 0.9:
            bonus += 0.1
        
        return min(bonus, 0.3)  # é™åˆ¶æœ€å¤§å¥–åŠ±
    
    def _get_performance_adjustment(self, model_name: str) -> float:
        """è·å–æ€§èƒ½è°ƒæ•´ç³»æ•°"""
        if model_name not in self.performance_metrics.success_rates:
            return 1.0
        
        success_rate = self.performance_metrics.success_rates[model_name]
        quality_score = self.performance_metrics.quality_scores.get(model_name, 0.8)
        
        # åŸºäºæˆåŠŸç‡å’Œè´¨é‡åˆ†æ•°è°ƒæ•´
        return 0.5 + (success_rate * 0.3) + (quality_score * 0.2)
    
    def _calculate_context_penalty(self, features: TaskFeatures, model: ModelCapability) -> float:
        """è®¡ç®—ä¸Šä¸‹æ–‡çª—å£ä¸åŒ¹é…æƒ©ç½š"""
        if features.context_requirement <= model.context_window * 0.8:
            return 0.0
        elif features.context_requirement <= model.context_window * 0.9:
            return 0.1
        else:
            return 0.3
    
    def _calculate_availability_penalty(self, model_name: str) -> float:
        """è®¡ç®—å¯ç”¨æ€§æƒ©ç½š"""
        if model_name not in self.model_availability:
            return 0.0
        
        availability = self.model_availability[model_name]
        if not availability["available"]:
            return 1.0
        
        consecutive_failures = availability["consecutive_failures"]
        max_failures = availability["max_failures"]
        
        return consecutive_failures / max_failures
    
    def _select_optimal_model(self, model_scores: Dict[str, float], features: TaskFeatures) -> str:
        """é€‰æ‹©æœ€ä¼˜æ¨¡å‹"""
        # è¿‡æ»¤å¯ç”¨æ¨¡å‹
        available_models = {
            name: score for name, score in model_scores.items()
            if self.model_availability.get(name, {}).get("available", True)
        }
        
        if not available_models:
            # å¦‚æœæ²¡æœ‰å¯ç”¨æ¨¡å‹ï¼Œé€‰æ‹©å¤±è´¥æ¬¡æ•°æœ€å°‘çš„
            least_failed = min(
                self.model_availability.items(),
                key=lambda x: x[1]["consecutive_failures"]
            )
            return least_failed[0]
        
        # æ ¹æ®ç´§æ€¥ç¨‹åº¦é€‰æ‹©ç­–ç•¥
        if features.urgency > 0.7:
            # é«˜ç´§æ€¥åº¦ï¼šé€‰æ‹©æœ€å¿«çš„å¯ç”¨æ¨¡å‹
            fastest_model = min(
                available_models.items(),
                key=lambda x: self.models[x[0]].speed_score,
                reverse=True
            )
            return fastest_model[0]
        elif features.cost_sensitivity > 0.7:
            # é«˜æˆæœ¬æ•æ„Ÿï¼šé€‰æ‹©æœ€ä¾¿å®œçš„å¯ç”¨æ¨¡å‹
            cheapest_model = min(
                available_models.items(),
                key=lambda x: self.models[x[0]].cost_score,
                reverse=True
            )
            return cheapest_model[0]
        else:
            # æ­£å¸¸æƒ…å†µï¼šé€‰æ‹©å¾—åˆ†æœ€é«˜çš„æ¨¡å‹
            best_model = max(available_models.items(), key=lambda x: x[1])
            return best_model[0]
    
    def _generate_alternative_models(self, selected_model: str, features: TaskFeatures) -> List[str]:
        """ç”Ÿæˆå¤‡é€‰æ¨¡å‹"""
        alternatives = []
        
        # è·å–åŒä¸€å±‚çº§çš„æ¨¡å‹
        selected_tier = self.models[selected_model].tier
        
        # æŒ‰å¾—åˆ†æ’åºçš„å…¶ä»–æ¨¡å‹
        other_models = [
            name for name, model in self.models.items()
            if name != selected_model and 
               model.tier == selected_tier and
               self.model_availability.get(name, {}).get("available", True)
        ]
        
        # æŒ‰æ€§èƒ½åˆ†æ•°æ’åº
        other_models.sort(
            key=lambda x: self.models[x].quality_score,
            reverse=True
        )
        
        # è¿”å›å‰3ä¸ªå¤‡é€‰æ¨¡å‹
        return other_models[:3]
    
    def _generate_fallback_plan(self, selected_model: str, features: TaskFeatures) -> List[str]:
        """ç”Ÿæˆå›é€€è®¡åˆ’"""
        fallback_plan = []
        
        # è·å–æ¨¡å‹å±‚çº§
        tiers = [ModelTier.PREMIUM, ModelTier.STANDARD, ModelTier.ECONOMY, ModelTier.LOCAL]
        current_tier = self.models[selected_model].tier
        
        # ä»å½“å‰å±‚çº§å¼€å§‹ç”Ÿæˆå›é€€è®¡åˆ’
        for tier in tiers[tiers.index(current_tier) + 1:]:
            tier_models = [
                name for name, model in self.models.items()
                if model.tier == tier and
                   self.model_availability.get(name, {}).get("available", True)
            ]
            
            if tier_models:
                fallback_plan.extend(tier_models[:2])
        
        return fallback_plan
    
    def _estimate_expected_performance(self, model_name: str, features: TaskFeatures) -> Dict[str, float]:
        """ä¼°ç®—é¢„æœŸæ€§èƒ½"""
        model = self.models[model_name]
        
        # åŸºäºå†å²æ•°æ®å’Œæ¨¡å‹ç‰¹æ€§ä¼°ç®—
        expected_response_time = model.avg_response_time
        expected_success_rate = model.success_rate
        expected_quality_score = model.quality_score
        
        # æ ¹æ®ä»»åŠ¡ç‰¹å¾è°ƒæ•´
        if features.context_requirement > model.context_window * 0.8:
            expected_response_time *= 1.5
            expected_success_rate *= 0.9
        
        if features.complexity > 0.7:
            expected_response_time *= 1.3
            expected_quality_score *= 0.9
        
        return {
            "response_time": expected_response_time,
            "success_rate": expected_success_rate,
            "quality_score": expected_quality_score
        }
    
    def intelligent_routing(self, task: str, task_type: Optional[TaskType] = None) -> RoutingDecision:
        """æ™ºèƒ½è·¯ç”±ä¸»å‡½æ•°"""
        start_time = time.time()
        
        # ç”Ÿæˆä»»åŠ¡å“ˆå¸Œç”¨äºç¼“å­˜
        task_hash = hashlib.md5(task.encode()).hexdigest()
        
        # æ£€æŸ¥ç¼“å­˜
        if task_hash in self.routing_cache:
            cached_decision = self.routing_cache[task_hash]
            logger.info(f"ä½¿ç”¨ç¼“å­˜çš„è·¯ç”±å†³ç­–: {cached_decision.selected_model}")
            return cached_decision
        
        # åˆ†æä»»åŠ¡ç‰¹å¾
        features = self._analyze_task_features(task, task_type)
        
        # è·å–ä»»åŠ¡æƒé‡
        weights = self.task_weights.get(features.task_type.value, {
            "reasoning": 0.25, "code": 0.25, "creativity": 0.25, "speed": 0.15, "cost": 0.1
        })
        
        # è®¡ç®—æ¯ä¸ªæ¨¡å‹çš„å¾—åˆ†
        model_scores = {}
        for model_name, model in self.models.items():
            score = self._calculate_model_score(model, features, weights)
            model_scores[model_name] = score
        
        # é€‰æ‹©æœ€ä¼˜æ¨¡å‹
        selected_model = self._select_optimal_model(model_scores, features)
        
        # ç”Ÿæˆå¤‡é€‰æ¨¡å‹
        alternative_models = self._generate_alternative_models(selected_model, features)
        
        # ç”Ÿæˆå›é€€è®¡åˆ’
        fallback_plan = self._generate_fallback_plan(selected_model, features)
        
        # ä¼°ç®—é¢„æœŸæ€§èƒ½
        expected_performance = self._estimate_expected_performance(selected_model, features)
        
        # è®¡ç®—å†³ç­–ç½®ä¿¡åº¦
        confidence = self._calculate_confidence(model_scores, selected_model, features)
        
        # ç”Ÿæˆå†³ç­–ç†ç”±
        reasoning = self._generate_reasoning(selected_model, features, model_scores)
        
        # åˆ›å»ºå†³ç­–å¯¹è±¡
        decision = RoutingDecision(
            selected_model=selected_model,
            confidence=confidence,
            reasoning=reasoning,
            alternative_models=alternative_models,
            expected_performance=expected_performance,
            fallback_plan=fallback_plan
        )
        
        # ç¼“å­˜å†³ç­–
        self.routing_cache[task_hash] = decision
        
        # è®°å½•è·¯ç”±æ—¶é—´
        routing_time = time.time() - start_time
        logger.info(f"è·¯ç”±å†³ç­–å®Œæˆï¼Œè€—æ—¶: {routing_time:.2f}msï¼Œé€‰æ‹©æ¨¡å‹: {selected_model}")
        
        return decision
    
    def _calculate_confidence(self, model_scores: Dict[str, float], selected_model: str, features: TaskFeatures) -> float:
        """è®¡ç®—å†³ç­–ç½®ä¿¡åº¦"""
        if not model_scores:
            return 0.0
        
        # è·å–æœ€é«˜åˆ†å’Œå¹³å‡åˆ†
        max_score = max(model_scores.values())
        avg_score = sum(model_scores.values()) / len(model_scores)
        
        if max_score == 0:
            return 0.0
        
        # åŸºäºå¾—åˆ†å·®å¼‚è®¡ç®—ç½®ä¿¡åº¦
        score_ratio = model_scores[selected_model] / max_score
        avg_ratio = avg_score / max_score
        
        # ç»¼åˆç½®ä¿¡åº¦
        confidence = (score_ratio * 0.7) + (avg_ratio * 0.3)
        
        return min(confidence, 1.0)
    
    def _generate_reasoning(self, selected_model: str, features: TaskFeatures, model_scores: Dict[str, float]) -> str:
        """ç”Ÿæˆå†³ç­–ç†ç”±"""
        model = self.models[selected_model]
        
        reasoning_parts = []
        
        # ä»»åŠ¡ç±»å‹åŒ¹é…
        if features.task_type:
            reasoning_parts.append(f"ä»»åŠ¡ç±»å‹ä¸º{features.task_type.value}ï¼Œæ¨¡å‹{selected_model}åœ¨æ­¤é¢†åŸŸè¡¨ç°ä¼˜ç§€")
        
        # è¯­è¨€åŒ¹é…
        if features.language in model.languages:
            reasoning_parts.append(f"æ¨¡å‹æ”¯æŒ{features.language}è¯­è¨€")
        
        # ä»£ç éœ€æ±‚åŒ¹é…
        if features.has_code and "code" in model.specialties:
            reasoning_parts.append(f"æ¨¡å‹æ“…é•¿ä»£ç ç”Ÿæˆï¼Œä»£ç è¯„åˆ†ï¼š{model.code_score:.2f}")
        
        # ä¸Šä¸‹æ–‡çª—å£åŒ¹é…
        if features.context_requirement <= model.context_window:
            reasoning_parts.append(f"ä¸Šä¸‹æ–‡çª—å£({model.context_window})æ»¡è¶³éœ€æ±‚")
        else:
            reasoning_parts.append(f"ä¸Šä¸‹æ–‡çª—å£({model.context_window})å¯èƒ½ä¸è¶³")
        
        # æ€§èƒ½è€ƒè™‘
        if features.urgency > 0.7:
            reasoning_parts.append(f"é«˜ç´§æ€¥åº¦ä»»åŠ¡ï¼Œé€‰æ‹©å“åº”é€Ÿåº¦è¾ƒå¿«çš„æ¨¡å‹")
        elif features.cost_sensitivity > 0.7:
            reasoning_parts.append(f"æˆæœ¬æ•æ„Ÿä»»åŠ¡ï¼Œé€‰æ‹©æˆæœ¬æ•ˆç›Šè¾ƒé«˜çš„æ¨¡å‹")
        
        # è´¨é‡ä¿è¯
        if features.quality_requirement > 0.9:
            reasoning_parts.append(f"é«˜è´¨é‡è¦æ±‚ï¼Œé€‰æ‹©è´¨é‡è¯„åˆ†({model.quality_score:.2f})è¾ƒé«˜çš„æ¨¡å‹")
        
        return "ï¼›".join(reasoning_parts)
    
    def update_performance_feedback(self, model_name: str, success: bool, 
                                 response_time: float, quality_score: float):
        """æ›´æ–°æ€§èƒ½åé¦ˆ"""
        # æ›´æ–°å“åº”æ—¶é—´å†å²
        self.performance_metrics.response_times.append(response_time)
        
        # æ›´æ–°æˆåŠŸç‡
        if model_name not in self.performance_metrics.success_rates:
            self.performance_metrics.success_rates[model_name] = 0.0
        
        current_rate = self.performance_metrics.success_rates[model_name]
        total_requests = len(self.performance_metrics.response_times)
        
        # ä½¿ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡æ›´æ–°æˆåŠŸç‡
        alpha = 0.1
        if success:
            new_rate = current_rate * (1 - alpha) + alpha
        else:
            new_rate = current_rate * (1 - alpha)
        
        self.performance_metrics.success_rates[model_name] = new_rate
        
        # æ›´æ–°è´¨é‡åˆ†æ•°
        if model_name not in self.performance_metrics.quality_scores:
            self.performance_metrics.quality_scores[model_name] = 0.8
        
        current_quality = self.performance_metrics.quality_scores[model_name]
        new_quality = current_quality * (1 - alpha) + quality_score * alpha
        
        self.performance_metrics.quality_scores[model_name] = new_quality
        
        # æ›´æ–°é”™è¯¯è®¡æ•°
        if not success:
            if model_name not in self.performance_metrics.error_counts:
                self.performance_metrics.error_counts[model_name] = 0
            self.performance_metrics.error_counts[model_name] += 1
        
        # æ›´æ–°æ¨¡å‹å¯ç”¨æ€§
        if model_name in self.model_availability:
            availability = self.model_availability[model_name]
            if success:
                availability["consecutive_failures"] = 0
                availability["available"] = True
            else:
                availability["consecutive_failures"] += 1
                if availability["consecutive_failures"] >= availability["max_failures"]:
                    availability["available"] = False
                    # è®¾ç½®æ¢å¤å®šæ—¶å™¨
                    self._schedule_availability_recovery(model_name)
        
        # æ›´æ–°æœ€åæ›´æ–°æ—¶é—´
        self.performance_metrics.last_updated = time.time()
        
        logger.info(f"æ›´æ–°æ¨¡å‹{model_name}æ€§èƒ½åé¦ˆ: æˆåŠŸ={success}, å“åº”æ—¶é—´={response_time}ms, è´¨é‡={quality_score:.2f}")
    
    def _schedule_availability_recovery(self, model_name: str):
        """å®‰æ’å¯ç”¨æ€§æ¢å¤"""
        def recover():
            time.sleep(60)  # 1åˆ†é’Ÿåæ¢å¤
            if model_name in self.model_availability:
                self.model_availability[model_name]["consecutive_failures"] = 0
                self.model_availability[model_name]["available"] = True
                logger.info(f"æ¨¡å‹{model_name}å¯ç”¨æ€§å·²æ¢å¤")
        
        # åœ¨å¼‚æ­¥ç¯å¢ƒä¸­è¿è¡Œ
        try:
            asyncio.create_task(recover)
        except:
            # å¦‚æœä¸åœ¨å¼‚æ­¥ç¯å¢ƒä¸­ï¼Œä½¿ç”¨çº¿ç¨‹
            import threading
            threading.Thread(target=recover, daemon=True).start()
    
    def get_model_statistics(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "total_models": len(self.models),
            "available_models": sum(
                1 for model in self.model_availability.values()
                if model["available"]
            ),
            "models_by_tier": {},
            "performance_summary": {
                "avg_response_time": np.mean(list(self.performance_metrics.response_times)) if self.performance_metrics.response_times else 0,
                "avg_success_rate": np.mean(list(self.performance_metrics.success_rates.values())) if self.performance_metrics.success_rates else 0,
                "avg_quality_score": np.mean(list(self.performance_metrics.quality_scores.values())) if self.performance_metrics.quality_scores else 0
            },
            "models": {}
        }
        
        # æŒ‰å±‚çº§ç»Ÿè®¡æ¨¡å‹
        for model in self.models.values():
            tier = model.tier.value
            if tier not in stats["models_by_tier"]:
                stats["models_by_tier"][tier] = []
            stats["models_by_tier"][tier].append(model.name)
        
        # è¯¦ç»†çš„æ¨¡å‹ä¿¡æ¯
        for model_name, model in self.models.items():
            stats["models"][model_name] = {
                "provider": model.provider,
                "tier": model.tier.value,
                "success_rate": self.performance_metrics.success_rates.get(model_name, 0),
                "quality_score": self.performance_metrics.quality_scores.get(model_name, 0),
                "available": self.model_availability.get(model_name, {}).get("available", False)
            }
        
        return stats
    
    def clear_cache(self):
        """æ¸…ç†ç¼“å­˜"""
        self.routing_cache.clear()
        self.performance_cache.clear()
        logger.info("è·¯ç”±ç¼“å­˜å·²æ¸…ç†")

# å…¨å±€è·¯ç”±å™¨å®ä¾‹
router = IntelligentLLMRouter()

def intelligent_llm_routing(task: str, task_type: Optional[TaskType] = None) -> RoutingDecision:
    """æ™ºèƒ½LLMè·¯ç”±ä¸»å‡½æ•°"""
    return router.intelligent_routing(task, task_type)

def update_model_performance(model_name: str, success: bool, 
                           response_time: float, quality_score: float):
    """æ›´æ–°æ¨¡å‹æ€§èƒ½åé¦ˆ"""
    router.update_performance_feedback(model_name, success, response_time, quality_score)

def get_router_statistics() -> Dict[str, Any]:
    """è·å–è·¯ç”±å™¨ç»Ÿè®¡ä¿¡æ¯"""
    return router.get_model_statistics()

def clear_router_cache():
    """æ¸…ç†è·¯ç”±å™¨ç¼“å­˜"""
    router.clear_cache()

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    test_task = "è¯·å¸®æˆ‘è®¾è®¡ä¸€ä¸ªé«˜æ€§èƒ½çš„å¾®æœåŠ¡æ¶æ„ï¼Œéœ€è¦åŒ…å«æœåŠ¡å‘ç°ã€è´Ÿè½½å‡è¡¡å’Œå®¹é”™æœºåˆ¶"
    decision = intelligent_llm_routing(test_task)
    
    print(f"é€‰æ‹©çš„æ¨¡å‹: {decision.selected_model}")
    print(f"ç½®ä¿¡åº¦: {decision.confidence:.2f}")
    print(f"å†³ç­–ç†ç”±: {decision.reasoning}")
    print(f"å¤‡é€‰æ¨¡å‹: {decision.alternative_models}")
    print(f"é¢„æœŸæ€§èƒ½: {decision.expected_performance}")
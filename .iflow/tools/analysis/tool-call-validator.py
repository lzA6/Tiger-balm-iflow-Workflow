#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ”§ å·¥å…·è°ƒç”¨éªŒè¯å™¨
Tool Call Validator

å…¨é¢æµ‹è¯•å·¥å…·è°ƒç”¨æƒ…å†µï¼ŒéªŒè¯CLIã€Pythonæ•æ„Ÿæ€§ã€æ¨¡å‹é€‚é…ç­‰
"""

import asyncio
import time
import json
import logging
import sys
import os
import subprocess
import importlib.util
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
import traceback

# æ·»åŠ .iflowåˆ°Pythonè·¯å¾„
IFLOW_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(IFLOW_ROOT))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('tool_validation.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class ToolTestResult:
    """å·¥å…·æµ‹è¯•ç»“æœ"""
    tool_name: str
    test_type: str
    status: str  # passed, failed, error
    duration: float
    details: Dict[str, Any]
    timestamp: float
    error_message: Optional[str] = None
    performance_metrics: Optional[Dict[str, float]] = None

@dataclass
class ValidationReport:
    """éªŒè¯æŠ¥å‘Š"""
    total_tools: int
    passed_tools: int
    failed_tools: int
    error_tools: int
    total_duration: float
    test_results: List[ToolTestResult]
    summary: Dict[str, Any]
    recommendations: List[str]

class ToolCallValidator:
    """å·¥å…·è°ƒç”¨éªŒè¯å™¨"""
    
    def __init__(self):
        self.iflow_root = IFLOW_ROOT
        self.tools_dir = self.iflow_root / "tools"
        self.core_dir = self.iflow_root / "core"
        self.agents_dir = self.iflow_root / "agents"
        self.workflows_dir = self.iflow_root / "workflows"
        self.test_results = []
        
    async def run_comprehensive_validation(self) -> ValidationReport:
        """è¿è¡Œå…¨é¢éªŒè¯"""
        logger.info("å¼€å§‹å…¨é¢å·¥å…·è°ƒç”¨éªŒè¯...")
        start_time = time.time()
        
        # 1. CLIå·¥å…·éªŒè¯
        await self._validate_cli_tools()
        
        # 2. Pythonå·¥å…·éªŒè¯
        await self._validate_python_tools()
        
        # 3. æ™ºèƒ½ä½“éªŒè¯
        await self._validate_agents()
        
        # 4. å·¥ä½œæµéªŒè¯
        await self._validate_workflows()
        
        # 5. æ¨¡å‹é€‚é…å™¨éªŒè¯
        await self._validate_model_adapter()
        
        # 6. æ€§èƒ½æµ‹è¯•
        await self._validate_performance()
        
        total_duration = time.time() - start_time
        
        # ç”ŸæˆæŠ¥å‘Š
        report = self._generate_validation_report(total_duration)
        
        logger.info(f"éªŒè¯å®Œæˆï¼Œè€—æ—¶: {total_duration:.2f}s")
        return report
    
    async def _validate_cli_tools(self):
        """éªŒè¯CLIå·¥å…·"""
        cli_path = self.core_dir / "iflow-cli.py"
        
        if not cli_path.exists():
            self._add_test_result("CLIå·¥å…·", "cli", "error", 0, {"error": "CLIæ–‡ä»¶ä¸å­˜åœ¨"})
            return
        
        # æµ‹è¯•CLIå¸®åŠ©å‘½ä»¤
        await self._test_cli_command("help")
        
        # æµ‹è¯•CLIçŠ¶æ€å‘½ä»¤
        await self._test_cli_command("status")
        
        # æµ‹è¯•CLIæ™ºèƒ½ä½“åˆ—è¡¨
        await self._test_cli_command("agent list")
        
        # æµ‹è¯•CLIå·¥ä½œæµåˆ—è¡¨
        await self._test_cli_command("workflow list")
    
    async def _test_cli_command(self, command: str):
        """æµ‹è¯•CLIå‘½ä»¤"""
        start_time = time.time()
        
        try:
            # æ„å»ºå‘½ä»¤
            cmd = [sys.executable, str(self.core_dir / "iflow-cli.py")]
            if command != "help":
                cmd.extend(command.split())
            
            # æ‰§è¡Œå‘½ä»¤
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=30,
                cwd=str(self.iflow_root)
            )
            
            duration = time.time() - start_time
            
            if result.returncode == 0:
                self._add_test_result(
                    f"CLI-{command}",
                    "cli",
                    "passed",
                    duration,
                    {
                        "stdout": result.stdout[:500],  # é™åˆ¶è¾“å‡ºé•¿åº¦
                        "returncode": result.returncode
                    }
                )
            else:
                self._add_test_result(
                    f"CLI-{command}",
                    "cli",
                    "failed",
                    duration,
                    {
                        "stderr": result.stderr[:500],
                        "returncode": result.returncode
                    },
                    result.stderr
                )
                
        except subprocess.TimeoutExpired:
            self._add_test_result(
                f"CLI-{command}",
                "cli",
                "failed",
                30,
                {"error": "å‘½ä»¤è¶…æ—¶"},
                "å‘½ä»¤æ‰§è¡Œè¶…æ—¶"
            )
        except Exception as e:
            duration = time.time() - start_time
            self._add_test_result(
                f"CLI-{command}",
                "cli",
                "error",
                duration,
                {"error": str(e)},
                str(e)
            )
    
    async def _validate_python_tools(self):
        """éªŒè¯Pythonå·¥å…·"""
        python_tools = [
            "enhanced-universal-agent.py",
            "quantum-performance-optimizer.py",
            "self-evolution-engine.py",
            "comprehensive-test-suite.py"
        ]
        
        for tool in python_tools:
            await self._test_python_tool(tool)
    
    async def _test_python_tool(self, tool_name: str):
        """æµ‹è¯•Pythonå·¥å…·"""
        tool_path = self.tools_dir / tool_name
        
        if not tool_path.exists():
            self._add_test_result(f"Python-{tool_name}", "python", "error", 0, {"error": "å·¥å…·æ–‡ä»¶ä¸å­˜åœ¨"})
            return
        
        start_time = time.time()
        
        try:
            # æµ‹è¯•å¯¼å…¥
            spec = importlib.util.spec_from_file_location(tool_name.replace('.py', ''), tool_path)
            module = importlib.util.module_from_spec(spec)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è¯­æ³•é”™è¯¯
            spec.loader.exec_module(module)
            
            duration = time.time() - start_time
            
            # æµ‹è¯•å·¥å…·çš„ä¸»è¦åŠŸèƒ½
            test_result = await self._test_tool_functionality(module, tool_name)
            
            self._add_test_result(
                f"Python-{tool_name}",
                "python",
                "passed" if test_result else "failed",
                duration,
                {
                    "import_success": True,
                    "functionality_test": test_result
                }
            )
            
        except SyntaxError as e:
            duration = time.time() - start_time
            self._add_test_result(
                f"Python-{tool_name}",
                "python",
                "failed",
                duration,
                {"error": "è¯­æ³•é”™è¯¯", "details": str(e)},
                f"è¯­æ³•é”™è¯¯: {e}"
            )
        except Exception as e:
            duration = time.time() - start_time
            self._add_test_result(
                f"Python-{tool_name}",
                "python",
                "error",
                duration,
                {"error": str(e)},
                str(e)
            )
    
    async def _test_tool_functionality(self, module, tool_name: str) -> bool:
        """æµ‹è¯•å·¥å…·åŠŸèƒ½"""
        try:
            if tool_name == "enhanced-universal-agent.py":
                # æµ‹è¯•å¢å¼ºæ™ºèƒ½ä½“
                agent = getattr(module, 'get_enhanced_agent')()
                if hasattr(agent, 'analyze_task'):
                    return True
                    
            elif tool_name == "quantum-performance-optimizer.py":
                # æµ‹è¯•æ€§èƒ½ä¼˜åŒ–å™¨
                optimizer = getattr(module, 'get_global_optimizer')()
                if hasattr(optimizer, 'get_optimization_report'):
                    return True
                    
            elif tool_name == "self-evolution-engine.py":
                # æµ‹è¯•è¿›åŒ–å¼•æ“
                engine = getattr(module, 'get_evolution_engine')()
                if hasattr(engine, 'get_evolution_status'):
                    return True
                    
            elif tool_name == "comprehensive-test-suite.py":
                # æµ‹è¯•æµ‹è¯•å¥—ä»¶
                if hasattr(module, 'ComprehensiveTestSuite'):
                    return True
            
            return False
            
        except Exception as e:
            logger.error(f"Functionality test failed for {tool_name}: {e}")
            return False
    
    async def _validate_agents(self):
        """éªŒè¯æ™ºèƒ½ä½“"""
        agent_files = list(self.agents_dir.rglob("*.md"))
        
        for agent_file in agent_files:
            await self._test_agent_file(agent_file)
    
    async def _test_agent_file(self, agent_file: Path):
        """æµ‹è¯•æ™ºèƒ½ä½“æ–‡ä»¶"""
        try:
            with open(agent_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æ£€æŸ¥æ™ºèƒ½ä½“æ–‡ä»¶åŸºæœ¬ç»“æ„
            checks = {
                "has_title": bool(content.strip()),
                "has_sections": "##" in content,
                "has_abilities": "èƒ½åŠ›" in content or "ability" in content,
                "size_valid": len(content) > 100
            }
            
            passed = all(checks.values())
            
            self._add_test_result(
                f"Agent-{agent_file.name}",
                "agent",
                "passed" if passed else "failed",
                0,
                checks,
                None if passed else "æ™ºèƒ½ä½“æ–‡ä»¶ç»“æ„ä¸å®Œæ•´"
            )
            
        except Exception as e:
            self._add_test_result(
                f"Agent-{agent_file.name}",
                "agent",
                "error",
                0,
                {"error": str(e)},
                str(e)
            )
    
    async def _validate_workflows(self):
        """éªŒè¯å·¥ä½œæµ"""
        workflow_files = list(self.workflows_dir.rglob("*.yaml"))
        
        for workflow_file in workflow_files:
            await self._test_workflow_file(workflow_file)
    
    async def _test_workflow_file(self, workflow_file: Path):
        """æµ‹è¯•å·¥ä½œæµæ–‡ä»¶"""
        try:
            import yaml
            
            with open(workflow_file, 'r', encoding='utf-8') as f:
                workflow_config = yaml.safe_load(f)
            
            # æ£€æŸ¥å·¥ä½œæµåŸºæœ¬ç»“æ„
            checks = {
                "has_metadata": "metadata" in workflow_config,
                "has_workflow": "workflow" in workflow_config,
                "has_phases": bool(workflow_config.get("workflow", {}).get("phases")),
                "size_valid": len(str(workflow_config)) > 100
            }
            
            passed = all(checks.values())
            
            self._add_test_result(
                f"Workflow-{workflow_file.name}",
                "workflow",
                "passed" if passed else "failed",
                0,
                checks,
                None if passed else "å·¥ä½œæµæ–‡ä»¶ç»“æ„ä¸å®Œæ•´"
            )
            
        except Exception as e:
            self._add_test_result(
                f"Workflow-{workflow_file.name}",
                "workflow",
                "error",
                0,
                {"error": str(e)},
                str(e)
            )
    
    async def _validate_model_adapter(self):
        """éªŒè¯æ¨¡å‹é€‚é…å™¨"""
        adapter_config = self.iflow_root / "config" / "universal-model-adapter.yaml"
        
        if not adapter_config.exists():
            self._add_test_result(
                "ModelAdapter",
                "adapter",
                "error",
                0,
                {"error": "é€‚é…å™¨é…ç½®æ–‡ä»¶ä¸å­˜åœ¨"},
                "é€‚é…å™¨é…ç½®æ–‡ä»¶ä¸å­˜åœ¨"
            )
            return
        
        try:
            import yaml
            
            with open(adapter_config, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            
            # æ£€æŸ¥é€‚é…å™¨é…ç½®
            checks = {
                "has_supported_models": "supported_models" in config,
                "has_openai": "openai" in config.get("supported_models", {}),
                "has_anthropic": "anthropic" in config.get("supported_models", {}),
                "has_google": "google" in config.get("supported_models", {}),
                "has_chinese_models": any(model in config.get("supported_models", {}) 
                                         for model in ["baidu", "alibaba", "tencent", "bytedance"]),
                "has_performance_config": "performance_optimization" in config
            }
            
            passed = all(checks.values())
            
            self._add_test_result(
                "ModelAdapter",
                "adapter",
                "passed" if passed else "failed",
                0,
                checks,
                None if passed else "æ¨¡å‹é€‚é…å™¨é…ç½®ä¸å®Œæ•´"
            )
            
        except Exception as e:
            self._add_test_result(
                "ModelAdapter",
                "adapter",
                "error",
                0,
                {"error": str(e)},
                str(e)
            )
    
    async def _validate_performance(self):
        """éªŒè¯æ€§èƒ½"""
        # æµ‹è¯•å†…å­˜ä½¿ç”¨
        await self._test_memory_usage()
        
        # æµ‹è¯•å“åº”æ—¶é—´
        await self._test_response_time()
        
        # æµ‹è¯•å¹¶å‘èƒ½åŠ›
        await self._test_concurrency()
    
    async def _test_memory_usage(self):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨"""
        try:
            import psutil
            
            process = psutil.Process()
            memory_info = process.memory_info()
            
            # å†…å­˜ä½¿ç”¨æ£€æŸ¥ï¼ˆå‡è®¾åˆç†é˜ˆå€¼ï¼‰
            memory_mb = memory_info.rss / 1024 / 1024
            passed = memory_mb < 1000  # å°äº1GBè®¤ä¸ºæ­£å¸¸
            
            self._add_test_result(
                "MemoryUsage",
                "performance",
                "passed" if passed else "failed",
                0,
                {
                    "memory_mb": memory_mb,
                    "threshold_mb": 1000
                },
                None if passed else f"å†…å­˜ä½¿ç”¨è¿‡é«˜: {memory_mb:.2f}MB"
            )
            
        except Exception as e:
            self._add_test_result(
                "MemoryUsage",
                "performance",
                "error",
                0,
                {"error": str(e)},
                str(e)
            )
    
    async def _test_response_time(self):
        """æµ‹è¯•å“åº”æ—¶é—´"""
        start_time = time.time()
        
        # æ¨¡æ‹Ÿç®€å•æ“ä½œ
        await asyncio.sleep(0.01)
        
        duration = time.time() - start_time
        passed = duration < 1.0  # å°äº1ç§’è®¤ä¸ºæ­£å¸¸
        
        self._add_test_result(
            "ResponseTime",
            "performance",
            "passed" if passed else "failed",
            duration,
            {
                "duration_ms": duration * 1000,
                "threshold_ms": 1000
            },
            None if passed else f"å“åº”æ—¶é—´è¿‡é•¿: {duration*1000:.2f}ms"
        )
    
    async def _test_concurrency(self):
        """æµ‹è¯•å¹¶å‘èƒ½åŠ›"""
        start_time = time.time()
        
        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        async def dummy_task():
            await asyncio.sleep(0.01)
            return "done"
        
        tasks = [dummy_task() for _ in range(10)]
        results = await asyncio.gather(*tasks)
        
        duration = time.time() - start_time
        passed = len(results) == 10 and duration < 2.0
        
        self._add_test_result(
            "Concurrency",
            "performance",
            "passed" if passed else "failed",
            duration,
            {
                "tasks_completed": len(results),
                "concurrent_tasks": 10,
                "duration_s": duration
            },
            None if passed else f"å¹¶å‘æµ‹è¯•å¤±è´¥: å®Œæˆ{len(results)}/10ä»»åŠ¡"
        )
    
    def _add_test_result(self, tool_name: str, test_type: str, status: str, 
                        duration: float, details: Dict[str, Any], 
                        error_message: Optional[str] = None):
        """æ·»åŠ æµ‹è¯•ç»“æœ"""
        result = ToolTestResult(
            tool_name=tool_name,
            test_type=test_type,
            status=status,
            duration=duration,
            details=details,
            timestamp=time.time(),
            error_message=error_message
        )
        self.test_results.append(result)
    
    def _generate_validation_report(self, total_duration: float) -> ValidationReport:
        """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
        total_tools = len(self.test_results)
        passed_tools = len([r for r in self.test_results if r.status == 'passed'])
        failed_tools = len([r for r in self.test_results if r.status == 'failed'])
        error_tools = len([r for r in self.test_results if r.status == 'error'])
        
        # ç”Ÿæˆæ€»ç»“
        summary = {
            "success_rate": passed_tools / total_tools if total_tools > 0 else 0,
            "total_duration": total_duration,
            "avg_duration": sum(r.duration for r in self.test_results) / total_tools if total_tools > 0 else 0,
            "test_types": list(set(r.test_type for r in self.test_results)),
            "critical_issues": [r.tool_name for r in self.test_results if r.status == 'error']
        }
        
        # ç”Ÿæˆå»ºè®®
        recommendations = []
        if error_tools > 0:
            recommendations.append(f"ä¿®å¤{error_tools}ä¸ªé”™è¯¯å·¥å…·")
        if failed_tools > 0:
            recommendations.append(f"æ”¹è¿›{failed_tools}ä¸ªå¤±è´¥å·¥å…·")
        if summary["success_rate"] < 0.8:
            recommendations.append("æ•´ä½“æˆåŠŸç‡åä½ï¼Œéœ€è¦å…¨é¢ä¼˜åŒ–")
        
        return ValidationReport(
            total_tools=total_tools,
            passed_tools=passed_tools,
            failed_tools=failed_tools,
            error_tools=error_tools,
            total_duration=total_duration,
            test_results=self.test_results,
            summary=summary,
            recommendations=recommendations
        )
    
    def save_report(self, report: ValidationReport, output_path: str = None):
        """ä¿å­˜éªŒè¯æŠ¥å‘Š"""
        if output_path is None:
            output_path = self.iflow_root / "tool_validation_report.json"
        
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        report_data = {
            "total_tools": report.total_tools,
            "passed_tools": report.passed_tools,
            "failed_tools": report.failed_tools,
            "error_tools": report.error_tools,
            "total_duration": report.total_duration,
            "summary": report.summary,
            "recommendations": report.recommendations,
            "test_results": [asdict(r) for r in report.test_results],
            "timestamp": datetime.now().isoformat()
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"éªŒè¯æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    """ä¸»å‡½æ•°ç¤ºä¾‹"""
    validator = ToolCallValidator()
    
    # è¿è¡Œå…¨é¢éªŒè¯
    report = await validator.run_comprehensive_validation()
    
    # è¾“å‡ºç»“æœ
    print(f"\n=== å·¥å…·è°ƒç”¨éªŒè¯æŠ¥å‘Š ===")
    print(f"æ€»å·¥å…·æ•°: {report.total_tools}")
    print(f"é€šè¿‡: {report.passed_tools}")
    print(f"å¤±è´¥: {report.failed_tools}")
    print(f"é”™è¯¯: {report.error_tools}")
    print(f"æˆåŠŸç‡: {report.summary['success_rate']:.1%}")
    print(f"æ€»è€—æ—¶: {report.total_duration:.2f}s")
    
    # æ˜¾ç¤ºå¤±è´¥å’Œé”™è¯¯çš„å·¥å…·
    if report.failed_tools > 0:
        print(f"\nå¤±è´¥çš„å·¥å…·:")
        for result in report.test_results:
            if result.status == 'failed':
                print(f"  - {result.tool_name}: {result.error_message}")
    
    if report.error_tools > 0:
        print(f"\né”™è¯¯çš„å·¥å…·:")
        for result in report.test_results:
            if result.status == 'error':
                print(f"  - {result.tool_name}: {result.error_message}")
    
    # æ˜¾ç¤ºå»ºè®®
    if report.recommendations:
        print(f"\nå»ºè®®:")
        for rec in report.recommendations:
            print(f"  - {rec}")
    
    # ä¿å­˜æŠ¥å‘Š
    validator.save_report(report)

if __name__ == "__main__":
    asyncio.run(main())

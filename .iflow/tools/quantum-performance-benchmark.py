#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
âš¡ é‡å­æ€§èƒ½åŸºå‡†æµ‹è¯•ç³»ç»Ÿ
Quantum Performance Benchmarking System

ä¸ºå…¨èƒ½å·¥ä½œæµV6æä¾›å…¨é¢çš„æ€§èƒ½åŸºå‡†æµ‹è¯•å’Œä¼˜åŒ–å»ºè®®
"""

import asyncio
import time
import json
import psutil
import statistics
import numpy as np
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timedelta
import logging
from dataclasses import dataclass, asdict
import sys
import os

# æ·»åŠ .iflowåˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent
iflow_root = current_dir.parent
sys.path.insert(0, str(iflow_root))

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class BenchmarkMetrics:
    """åŸºå‡†æµ‹è¯•æŒ‡æ ‡"""
    test_name: str
    execution_time: float
    memory_usage_mb: float
    cpu_usage_percent: float
    success_rate: float
    throughput: float
    latency_p50: float
    latency_p95: float
    latency_p99: float
    error_count: int
    timestamp: str

@dataclass
class SystemMetrics:
    """ç³»ç»ŸæŒ‡æ ‡"""
    cpu_count: int
    memory_total_gb: float
    memory_available_gb: float
    disk_usage_percent: float
    network_io: Dict[str, int]
    process_count: int

class QuantumPerformanceBenchmark:
    """é‡å­æ€§èƒ½åŸºå‡†æµ‹è¯•å™¨"""
    
    def __init__(self, config_path: Optional[str] = None):
        self.root_dir = Path(__file__).parent.parent
        self.config = self._load_config(config_path)
        self.results = []
        self.system_metrics = self._get_system_metrics()
        
        # æµ‹è¯•é…ç½®
        self.test_config = {
            "iterations": 100,
            "warmup_iterations": 10,
            "concurrent_users": 10,
            "test_duration_seconds": 60,
            "memory_stress_test_mb": 100,
            "cpu_stress_test_seconds": 30
        }
        
        # åŸºå‡†é˜ˆå€¼
        self.benchmark_thresholds = {
            "max_response_time_ms": 500,
            "max_memory_usage_mb": 512,
            "max_cpu_usage_percent": 80,
            "min_success_rate": 0.99,
            "min_throughput_ops_per_sec": 100
        }
    
    def _load_config(self, config_path: Optional[str]) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        default_config = {
            "enable_quantum_tests": True,
            "enable_stress_tests": True,
            "enable_memory_tests": True,
            "enable_cpu_tests": True,
            "enable_network_tests": True,
            "output_format": "json",
            "save_detailed_logs": True
        }
        
        if config_path and Path(config_path).exists():
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    user_config = json.load(f)
                default_config.update(user_config)
            except Exception as e:
                logger.warning(f"Failed to load config: {e}")
        
        return default_config
    
    def _get_system_metrics(self) -> SystemMetrics:
        """è·å–ç³»ç»ŸæŒ‡æ ‡"""
        cpu_count = psutil.cpu_count()
        memory = psutil.virtual_memory()
        disk = psutil.disk_usage('/')
        network = psutil.net_io_counters()
        process_count = len(psutil.pids())
        
        return SystemMetrics(
            cpu_count=cpu_count,
            memory_total_gb=memory.total / (1024**3),
            memory_available_gb=memory.available / (1024**3),
            disk_usage_percent=disk.percent,
            network_io={
                "bytes_sent": network.bytes_sent,
                "bytes_recv": network.bytes_recv
            } if network else {},
            process_count=process_count
        )
    
    async def run_comprehensive_benchmark(self) -> Dict[str, Any]:
        """è¿è¡Œå…¨é¢çš„åŸºå‡†æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹é‡å­æ€§èƒ½åŸºå‡†æµ‹è¯•...")
        
        start_time = datetime.now()
        benchmark_results = {
            "test_session_id": int(start_time.timestamp()),
            "start_time": start_time.isoformat(),
            "system_metrics": asdict(self.system_metrics),
            "test_results": {},
            "summary": {},
            "recommendations": []
        }
        
        try:
            # 1. åŸºç¡€æ€§èƒ½æµ‹è¯•
            if self.config.get("enable_cpu_tests", True):
                benchmark_results["test_results"]["cpu_performance"] = await self._benchmark_cpu_performance()
            
            # 2. å†…å­˜æ€§èƒ½æµ‹è¯•
            if self.config.get("enable_memory_tests", True):
                benchmark_results["test_results"]["memory_performance"] = await self._benchmark_memory_performance()
            
            # 3. ç½‘ç»œæ€§èƒ½æµ‹è¯•
            if self.config.get("enable_network_tests", True):
                benchmark_results["test_results"]["network_performance"] = await self._benchmark_network_performance()
            
            # 4. é‡å­ç®—æ³•æ€§èƒ½æµ‹è¯•
            if self.config.get("enable_quantum_tests", True):
                benchmark_results["test_results"]["quantum_performance"] = await self._benchmark_quantum_performance()
            
            # 5. å‹åŠ›æµ‹è¯•
            if self.config.get("enable_stress_tests", True):
                benchmark_results["test_results"]["stress_test"] = await self._run_stress_test()
            
            # 6. ç”Ÿæˆæ€»ç»“å’Œå»ºè®®
            benchmark_results["summary"] = self._generate_summary(benchmark_results["test_results"])
            benchmark_results["recommendations"] = self._generate_recommendations(benchmark_results["test_results"])
            
            # 7. ä¿å­˜ç»“æœ
            end_time = datetime.now()
            benchmark_results["end_time"] = end_time.isoformat()
            benchmark_results["total_duration"] = (end_time - start_time).total_seconds()
            
            await self._save_benchmark_results(benchmark_results)
            
            logger.info("âœ… é‡å­æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆ!")
            return benchmark_results
            
        except Exception as e:
            logger.error(f"âŒ åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
            benchmark_results["error"] = str(e)
            return benchmark_results
    
    async def _benchmark_cpu_performance(self) -> Dict[str, Any]:
        """CPUæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        logger.info("ğŸ”¥ æ‰§è¡ŒCPUæ€§èƒ½åŸºå‡†æµ‹è¯•...")
        
        results = {
            "test_name": "CPU Performance",
            "metrics": [],
            "summary": {}
        }
        
        # æµ‹è¯•1: è®¡ç®—å¯†é›†å‹ä»»åŠ¡
        computation_times = []
        for i in range(self.test_config["iterations"]):
            start_time = time.time()
            
            # æ‰§è¡Œè®¡ç®—å¯†é›†å‹ä»»åŠ¡
            result = sum(np.random.rand(10000) ** 2)
            
            end_time = time.time()
            computation_times.append((end_time - start_time) * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
        
        # æµ‹è¯•2: å¹¶å‘å¤„ç†èƒ½åŠ›
        concurrent_times = []
        tasks = []
        
        async def cpu_task():
            start = time.time()
            # æ¨¡æ‹ŸCPUå¯†é›†å‹ä»»åŠ¡
            for _ in range(1000):
                _ = sum(range(1000))
            return (time.time() - start) * 1000
        
        for _ in range(self.test_config["concurrent_users"]):
            tasks.append(cpu_task())
        
        concurrent_results = await asyncio.gather(*tasks)
        concurrent_times.extend(concurrent_results)
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = BenchmarkMetrics(
            test_name="cpu_computation",
            execution_time=statistics.mean(computation_times),
            memory_usage_mb=self._get_memory_usage(),
            cpu_usage_percent=self._get_cpu_usage(),
            success_rate=1.0,
            throughput=self.test_config["iterations"] / sum(computation_times) * 1000,
            latency_p50=np.percentile(computation_times, 50),
            latency_p95=np.percentile(computation_times, 95),
            latency_p99=np.percentile(computation_times, 99),
            error_count=0,
            timestamp=datetime.now().isoformat()
        )
        
        results["metrics"].append(asdict(metrics))
        results["summary"] = {
            "avg_computation_time_ms": statistics.mean(computation_times),
            "max_computation_time_ms": max(computation_times),
            "min_computation_time_ms": min(computation_times),
            "concurrent_tasks_avg_time_ms": statistics.mean(concurrent_times),
            "cpu_efficiency_score": self._calculate_cpu_efficiency(metrics)
        }
        
        return results
    
    async def _benchmark_memory_performance(self) -> Dict[str, Any]:
        """å†…å­˜æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        logger.info("ğŸ’¾ æ‰§è¡Œå†…å­˜æ€§èƒ½åŸºå‡†æµ‹è¯•...")
        
        results = {
            "test_name": "Memory Performance",
            "metrics": [],
            "summary": {}
        }
        
        # æµ‹è¯•1: å†…å­˜åˆ†é…é€Ÿåº¦
        allocation_times = []
        memory_sizes = []
        
        for size in [1024, 10240, 102400, 1024000]:  # 1KB, 10KB, 100KB, 1MB
            start_time = time.time()
            
            # åˆ†é…å†…å­˜
            data = np.random.rand(size)
            memory_sizes.append(sys.getsizeof(data))
            
            end_time = time.time()
            allocation_times.append((end_time - start_time) * 1000)
        
        # æµ‹è¯•2: å†…å­˜è®¿é—®æ€§èƒ½
        access_times = []
        test_array = np.random.rand(100000)
        
        for _ in range(100):
            start_time = time.time()
            
            # éšæœºè®¿é—®å†…å­˜
            for _ in range(1000):
                idx = np.random.randint(0, len(test_array))
                _ = test_array[idx]
            
            end_time = time.time()
            access_times.append((end_time - start_time) * 1000)
        
        # æµ‹è¯•3: å†…å­˜å‹åŠ›æµ‹è¯•
        stress_start = time.time()
        memory_blocks = []
        
        try:
            for _ in range(self.test_config["memory_stress_test_mb"]):
                block = np.random.rand(1024)  # 8KB per block
                memory_blocks.append(block)
        except MemoryError:
            logger.warning("å†…å­˜ä¸è¶³ï¼Œåœæ­¢å‹åŠ›æµ‹è¯•")
        
        stress_end = time.time()
        stress_time = (stress_end - stress_start) * 1000
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = BenchmarkMetrics(
            test_name="memory_allocation",
            execution_time=statistics.mean(allocation_times),
            memory_usage_mb=self._get_memory_usage(),
            cpu_usage_percent=self._get_cpu_usage(),
            success_rate=1.0,
            throughput=len(memory_blocks) / (stress_time / 1000) if stress_time > 0 else 0,
            latency_p50=np.percentile(allocation_times, 50),
            latency_p95=np.percentile(allocation_times, 95),
            latency_p99=np.percentile(allocation_times, 99),
            error_count=0,
            timestamp=datetime.now().isoformat()
        )
        
        results["metrics"].append(asdict(metrics))
        results["summary"] = {
            "avg_allocation_time_ms": statistics.mean(allocation_times),
            "avg_access_time_ms": statistics.mean(access_times),
            "stress_test_time_ms": stress_time,
            "memory_blocks_allocated": len(memory_blocks),
            "memory_efficiency_score": self._calculate_memory_efficiency(metrics)
        }
        
        return results
    
    async def _benchmark_network_performance(self) -> Dict[str, Any]:
        """ç½‘ç»œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        logger.info("ğŸŒ æ‰§è¡Œç½‘ç»œæ€§èƒ½åŸºå‡†æµ‹è¯•...")
        
        results = {
            "test_name": "Network Performance",
            "metrics": [],
            "summary": {}
        }
        
        # æµ‹è¯•1: æœ¬åœ°é€šä¿¡å»¶è¿Ÿ
        local_latencies = []
        
        for _ in range(self.test_config["iterations"]):
            start_time = time.time()
            
            # æ¨¡æ‹Ÿæœ¬åœ°ç½‘ç»œæ“ä½œ
            try:
                # åˆ›å»ºæœ¬åœ°socketè¿æ¥æµ‹è¯•
                import socket
                sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                sock.connect(('localhost', 80))
                sock.close()
            except:
                pass  # å¿½ç•¥è¿æ¥é”™è¯¯ï¼Œä¸“æ³¨äºå»¶è¿Ÿæµ‹è¯•
            
            end_time = time.time()
            local_latencies.append((end_time - start_time) * 1000)
        
        # æµ‹è¯•2: å¹¶å‘è¿æ¥æµ‹è¯•
        concurrent_latencies = []
        
        async def network_task():
            start = time.time()
            try:
                # æ¨¡æ‹Ÿç½‘ç»œæ“ä½œ
                await asyncio.sleep(0.001)  # 1mså»¶è¿Ÿ
            except:
                pass
            return (time.time() - start) * 1000
        
        tasks = [network_task() for _ in range(self.test_config["concurrent_users"])]
        concurrent_results = await asyncio.gather(*tasks)
        concurrent_latencies.extend(concurrent_results)
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = BenchmarkMetrics(
            test_name="network_latency",
            execution_time=statistics.mean(local_latencies),
            memory_usage_mb=self._get_memory_usage(),
            cpu_usage_percent=self._get_cpu_usage(),
            success_rate=1.0,
            throughput=self.test_config["iterations"] / sum(local_latencies) * 1000,
            latency_p50=np.percentile(local_latencies, 50),
            latency_p95=np.percentile(local_latencies, 95),
            latency_p99=np.percentile(local_latencies, 99),
            error_count=0,
            timestamp=datetime.now().isoformat()
        )
        
        results["metrics"].append(asdict(metrics))
        results["summary"] = {
            "avg_local_latency_ms": statistics.mean(local_latencies),
            "concurrent_avg_latency_ms": statistics.mean(concurrent_latencies),
            "network_efficiency_score": self._calculate_network_efficiency(metrics)
        }
        
        return results
    
    async def _benchmark_quantum_performance(self) -> Dict[str, Any]:
        """é‡å­ç®—æ³•æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        logger.info("âš›ï¸ æ‰§è¡Œé‡å­æ€§èƒ½åŸºå‡†æµ‹è¯•...")
        
        results = {
            "test_name": "Quantum Performance",
            "metrics": [],
            "summary": {}
        }
        
        # æµ‹è¯•1: é‡å­çŸ©é˜µè¿ç®—
        quantum_times = []
        
        for _ in range(50):  # é‡å­æµ‹è¯•è®¡ç®—é‡è¾ƒå¤§ï¼Œå‡å°‘è¿­ä»£æ¬¡æ•°
            start_time = time.time()
            
            # æ¨¡æ‹Ÿé‡å­çŸ©é˜µè¿ç®—
            matrix_size = 100
            matrix_a = np.random.rand(matrix_size, matrix_size)
            matrix_b = np.random.rand(matrix_size, matrix_size)
            
            # é‡å­æ€æ¨¡æ‹Ÿï¼ˆçŸ©é˜µä¹˜æ³•ï¼‰
            result = np.dot(matrix_a, matrix_b)
            
            # é‡å­æµ‹é‡æ¨¡æ‹Ÿ
            measurement = np.abs(result) ** 2
            measurement = measurement / np.sum(measurement)  # å½’ä¸€åŒ–
            
            end_time = time.time()
            quantum_times.append((end_time - start_time) * 1000)
        
        # æµ‹è¯•2: é‡å­çº ç¼ æ¨¡æ‹Ÿ
        entanglement_times = []
        
        for _ in range(30):
            start_time = time.time()
            
            # æ¨¡æ‹Ÿé‡å­çº ç¼ æ€
            bell_state = np.array([1/np.sqrt(2), 0, 0, 1/np.sqrt(2)])
            
            # é‡å­æ“ä½œæ¨¡æ‹Ÿ
            pauli_x = np.array([[0, 1], [1, 0]])
            cnot = np.array([[1, 0, 0, 0],
                           [0, 1, 0, 0],
                           [0, 0, 0, 1],
                           [0, 0, 1, 0]])
            
            # é‡å­é—¨æ“ä½œ
            entangled_state = np.dot(cnot, np.kron(pauli_x, np.eye(2)))
            
            end_time = time.time()
            entanglement_times.append((end_time - start_time) * 1000)
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = BenchmarkMetrics(
            test_name="quantum_computation",
            execution_time=statistics.mean(quantum_times),
            memory_usage_mb=self._get_memory_usage(),
            cpu_usage_percent=self._get_cpu_usage(),
            success_rate=1.0,
            throughput=50 / sum(quantum_times) * 1000,
            latency_p50=np.percentile(quantum_times, 50),
            latency_p95=np.percentile(quantum_times, 95),
            latency_p99=np.percentile(quantum_times, 99),
            error_count=0,
            timestamp=datetime.now().isoformat()
        )
        
        results["metrics"].append(asdict(metrics))
        results["summary"] = {
            "avg_quantum_computation_ms": statistics.mean(quantum_times),
            "avg_entanglement_simulation_ms": statistics.mean(entanglement_times),
            "quantum_efficiency_score": self._calculate_quantum_efficiency(metrics)
        }
        
        return results
    
    async def _run_stress_test(self) -> Dict[str, Any]:
        """å‹åŠ›æµ‹è¯•"""
        logger.info("ğŸ’ª æ‰§è¡Œç³»ç»Ÿå‹åŠ›æµ‹è¯•...")
        
        results = {
            "test_name": "Stress Test",
            "metrics": [],
            "summary": {}
        }
        
        start_time = time.time()
        stress_metrics = []
        
        # å¹¶å‘å‹åŠ›æµ‹è¯•
        async def stress_task(task_id: int):
            task_start = time.time()
            cpu_usage_samples = []
            memory_usage_samples = []
            
            # æ¨¡æ‹Ÿé«˜å¼ºåº¦ä»»åŠ¡
            for i in range(100):
                # CPUå¯†é›†å‹æ“ä½œ
                _ = sum(np.random.rand(1000) ** 2)
                
                # è®°å½•èµ„æºä½¿ç”¨
                if i % 10 == 0:
                    cpu_usage_samples.append(self._get_cpu_usage())
                    memory_usage_samples.append(self._get_memory_usage())
                
                # çŸ­æš‚ä¼‘æ¯
                await asyncio.sleep(0.001)
            
            task_end = time.time()
            
            return {
                "task_id": task_id,
                "execution_time": (task_end - task_start) * 1000,
                "avg_cpu_usage": statistics.mean(cpu_usage_samples) if cpu_usage_samples else 0,
                "max_memory_usage": max(memory_usage_samples) if memory_usage_samples else 0,
                "success": True
            }
        
        # å¯åŠ¨å¹¶å‘ä»»åŠ¡
        tasks = [stress_task(i) for i in range(self.test_config["concurrent_users"] * 2)]
        task_results = await asyncio.gather(*tasks)
        
        end_time = time.time()
        total_stress_time = (end_time - start_time) * 1000
        
        # åˆ†æç»“æœ
        successful_tasks = [r for r in task_results if r["success"]]
        execution_times = [r["execution_time"] for r in successful_tasks]
        cpu_usages = [r["avg_cpu_usage"] for r in successful_tasks]
        memory_usages = [r["max_memory_usage"] for r in successful_tasks]
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = BenchmarkMetrics(
            test_name="stress_test",
            execution_time=statistics.mean(execution_times) if execution_times else 0,
            memory_usage_mb=statistics.mean(memory_usages) if memory_usages else 0,
            cpu_usage_percent=statistics.mean(cpu_usages) if cpu_usages else 0,
            success_rate=len(successful_tasks) / len(task_results),
            throughput=len(successful_tasks) / (total_stress_time / 1000) if total_stress_time > 0 else 0,
            latency_p50=np.percentile(execution_times, 50) if execution_times else 0,
            latency_p95=np.percentile(execution_times, 95) if execution_times else 0,
            latency_p99=np.percentile(execution_times, 99) if execution_times else 0,
            error_count=len(task_results) - len(successful_tasks),
            timestamp=datetime.now().isoformat()
        )
        
        results["metrics"].append(asdict(metrics))
        results["summary"] = {
            "total_stress_time_ms": total_stress_time,
            "successful_tasks": len(successful_tasks),
            "failed_tasks": len(task_results) - len(successful_tasks),
            "system_stability_score": self._calculate_stability_score(metrics)
        }
        
        return results
    
    def _get_memory_usage(self) -> float:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡ï¼ˆMBï¼‰"""
        process = psutil.Process()
        return process.memory_info().rss / 1024 / 1024
    
    def _get_cpu_usage(self) -> float:
        """è·å–å½“å‰CPUä½¿ç”¨ç‡"""
        return psutil.cpu_percent(interval=0.1)
    
    def _calculate_cpu_efficiency(self, metrics: BenchmarkMetrics) -> float:
        """è®¡ç®—CPUæ•ˆç‡åˆ†æ•°"""
        # åŸºäºæ‰§è¡Œæ—¶é—´å’ŒCPUä½¿ç”¨ç‡è®¡ç®—æ•ˆç‡
        time_score = max(0, 100 - metrics.execution_time)
        cpu_score = max(0, 100 - metrics.cpu_usage_percent)
        return (time_score + cpu_score) / 2
    
    def _calculate_memory_efficiency(self, metrics: BenchmarkMetrics) -> float:
        """è®¡ç®—å†…å­˜æ•ˆç‡åˆ†æ•°"""
        # åŸºäºå†…å­˜ä½¿ç”¨é‡å’Œååé‡è®¡ç®—æ•ˆç‡
        memory_score = max(0, 100 - metrics.memory_usage_mb / 10)  # å‡è®¾10MBä¸ºåŸºå‡†
        throughput_score = min(100, metrics.throughput)
        return (memory_score + throughput_score) / 2
    
    def _calculate_network_efficiency(self, metrics: BenchmarkMetrics) -> float:
        """è®¡ç®—ç½‘ç»œæ•ˆç‡åˆ†æ•°"""
        # åŸºäºå»¶è¿Ÿå’Œååé‡è®¡ç®—æ•ˆç‡
        latency_score = max(0, 100 - metrics.latency_p50)
        throughput_score = min(100, metrics.throughput)
        return (latency_score + throughput_score) / 2
    
    def _calculate_quantum_efficiency(self, metrics: BenchmarkMetrics) -> float:
        """è®¡ç®—é‡å­ç®—æ³•æ•ˆç‡åˆ†æ•°"""
        # åŸºäºé‡å­è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        time_score = max(0, 100 - metrics.execution_time / 10)  # é‡å­è®¡ç®—æ—¶é—´åŸºå‡†
        throughput_score = min(100, metrics.throughput * 10)  # é‡å­è®¡ç®—ååé‡æƒé‡
        return (time_score + throughput_score) / 2
    
    def _calculate_stability_score(self, metrics: BenchmarkMetrics) -> float:
        """è®¡ç®—ç³»ç»Ÿç¨³å®šæ€§åˆ†æ•°"""
        # åŸºäºæˆåŠŸç‡å’Œèµ„æºä½¿ç”¨è®¡ç®—ç¨³å®šæ€§
        success_score = metrics.success_rate * 100
        resource_score = max(0, 100 - max(metrics.cpu_usage_percent, metrics.memory_usage_mb / 10))
        return (success_score + resource_score) / 2
    
    def _generate_summary(self, test_results: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æ€»ç»“"""
        summary = {
            "overall_score": 0,
            "performance_grade": "Unknown",
            "key_metrics": {},
            "bottlenecks": [],
            "strengths": []
        }
        
        scores = []
        
        # æ”¶é›†å„é¡¹æµ‹è¯•çš„æ•ˆç‡åˆ†æ•°
        for test_name, test_data in test_results.items():
            if "summary" in test_data:
                for metric_name, metric_value in test_data["summary"].items():
                    if "efficiency_score" in metric_name or "stability_score" in metric_name:
                        scores.append(metric_value)
                        summary["key_metrics"][f"{test_name}_{metric_name}"] = metric_value
        
        # è®¡ç®—æ€»ä½“åˆ†æ•°
        if scores:
            summary["overall_score"] = statistics.mean(scores)
            
            # è¯„çº§
            if summary["overall_score"] >= 90:
                summary["performance_grade"] = "A+ (ä¼˜ç§€)"
            elif summary["overall_score"] >= 80:
                summary["performance_grade"] = "A (è‰¯å¥½)"
            elif summary["overall_score"] >= 70:
                summary["performance_grade"] = "B (ä¸€èˆ¬)"
            elif summary["overall_score"] >= 60:
                summary["performance_grade"] = "C (è¾ƒå·®)"
            else:
                summary["performance_grade"] = "D (éœ€è¦ä¼˜åŒ–)"
        
        return summary
    
    def _generate_recommendations(self, test_results: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # åˆ†æå„é¡¹æµ‹è¯•ç»“æœå¹¶ç”Ÿæˆå»ºè®®
        for test_name, test_data in test_results.items():
            if "metrics" in test_data:
                for metric in test_data["metrics"]:
                    # æ£€æŸ¥å“åº”æ—¶é—´
                    if metric.get("execution_time", 0) > self.benchmark_thresholds["max_response_time_ms"]:
                        recommendations.append(f"{test_name}: å“åº”æ—¶é—´è¿‡é•¿ï¼Œå»ºè®®ä¼˜åŒ–ç®—æ³•æˆ–å¢åŠ è®¡ç®—èµ„æº")
                    
                    # æ£€æŸ¥å†…å­˜ä½¿ç”¨
                    if metric.get("memory_usage_mb", 0) > self.benchmark_thresholds["max_memory_usage_mb"]:
                        recommendations.append(f"{test_name}: å†…å­˜ä½¿ç”¨è¿‡é«˜ï¼Œå»ºè®®ä¼˜åŒ–å†…å­˜ç®¡ç†æˆ–å¢åŠ å†…å­˜")
                    
                    # æ£€æŸ¥CPUä½¿ç”¨
                    if metric.get("cpu_usage_percent", 0) > self.benchmark_thresholds["max_cpu_usage_percent"]:
                        recommendations.append(f"{test_name}: CPUä½¿ç”¨ç‡è¿‡é«˜ï¼Œå»ºè®®ä¼˜åŒ–è®¡ç®—é€»è¾‘æˆ–å¢åŠ CPUæ ¸å¿ƒ")
                    
                    # æ£€æŸ¥æˆåŠŸç‡
                    if metric.get("success_rate", 1.0) < self.benchmark_thresholds["min_success_rate"]:
                        recommendations.append(f"{test_name}: æˆåŠŸç‡è¿‡ä½ï¼Œå»ºè®®æ£€æŸ¥é”™è¯¯å¤„ç†å’Œç³»ç»Ÿç¨³å®šæ€§")
                    
                    # æ£€æŸ¥ååé‡
                    if metric.get("throughput", 0) < self.benchmark_thresholds["min_throughput_ops_per_sec"]:
                        recommendations.append(f"{test_name}: ååé‡è¿‡ä½ï¼Œå»ºè®®ä¼˜åŒ–å¹¶å‘å¤„ç†èƒ½åŠ›")
        
        # é€šç”¨ä¼˜åŒ–å»ºè®®
        if not recommendations:
            recommendations.append("ç³»ç»Ÿæ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œå»ºè®®ç»§ç»­ç›‘æ§å¹¶ä¿æŒå½“å‰é…ç½®")
        else:
            recommendations.append("å»ºè®®å®šæœŸè¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•ä»¥ç›‘æ§ç³»ç»Ÿæ€§èƒ½å˜åŒ–")
            recommendations.append("è€ƒè™‘å¯ç”¨é‡å­ä¼˜åŒ–æ¨¡å—ä»¥æå‡æ•´ä½“æ€§èƒ½")
        
        return recommendations
    
    async def _save_benchmark_results(self, results: Dict[str, Any]) -> None:
        """ä¿å­˜åŸºå‡†æµ‹è¯•ç»“æœ"""
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = self.root_dir / "benchmark_results"
        output_dir.mkdir(exist_ok=True)
        
        # ä¿å­˜JSONæ ¼å¼ç»“æœ
        timestamp = int(datetime.now().timestamp())
        json_file = output_dir / f"quantum_benchmark_{timestamp}.json"
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜Markdownæ ¼å¼æŠ¥å‘Š
        md_file = output_dir / f"quantum_benchmark_{timestamp}.md"
        await self._save_markdown_report(results, md_file)
        
        logger.info(f"ğŸ“Š åŸºå‡†æµ‹è¯•ç»“æœå·²ä¿å­˜: {json_file}")
        logger.info(f"ğŸ“‹ æµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆ: {md_file}")
    
    async def _save_markdown_report(self, results: Dict[str, Any], file_path: Path) -> None:
        """ä¿å­˜Markdownæ ¼å¼æŠ¥å‘Š"""
        report_content = f"""# âš¡ é‡å­æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š

## æµ‹è¯•æ¦‚è§ˆ

- **æµ‹è¯•ä¼šè¯ID**: {results['test_session_id']}
- **å¼€å§‹æ—¶é—´**: {results['start_time']}
- **ç»“æŸæ—¶é—´**: {results.get('end_time', 'N/A')}
- **æ€»è€—æ—¶**: {results.get('total_duration', 'N/A')} ç§’

## ç³»ç»Ÿç¯å¢ƒ

- **CPUæ ¸å¿ƒæ•°**: {results['system_metrics']['cpu_count']}
- **æ€»å†…å­˜**: {results['system_metrics']['memory_total_gb']:.2f} GB
- **å¯ç”¨å†…å­˜**: {results['system_metrics']['memory_available_gb']:.2f} GB
- **ç£ç›˜ä½¿ç”¨ç‡**: {results['system_metrics']['disk_usage_percent']}%
- **è¿›ç¨‹æ•°**: {results['system_metrics']['process_count']}

## æµ‹è¯•ç»“æœ

"""
        
        # æ·»åŠ å„é¡¹æµ‹è¯•ç»“æœ
        for test_name, test_data in results.get("test_results", {}).items():
            report_content += f"### {test_data.get('test_name', test_name)}\n\n"
            
            if "summary" in test_data:
                for metric, value in test_data["summary"].items():
                    if isinstance(value, float):
                        report_content += f"- **{metric}**: {value:.2f}\n"
                    else:
                        report_content += f"- **{metric}**: {value}\n"
                report_content += "\n"
        
        # æ·»åŠ æ€»ä½“è¯„åˆ†
        summary = results.get("summary", {})
        if summary:
            report_content += f"""## æ€»ä½“è¯„ä¼°

- **æ€§èƒ½æ€»åˆ†**: {summary.get('overall_score', 0):.2f}
- **æ€§èƒ½ç­‰çº§**: {summary.get('performance_grade', 'Unknown')}

"""
        
        # æ·»åŠ ä¼˜åŒ–å»ºè®®
        recommendations = results.get("recommendations", [])
        if recommendations:
            report_content += "## ä¼˜åŒ–å»ºè®®\n\n"
            for i, rec in enumerate(recommendations, 1):
                report_content += f"{i}. {rec}\n"
        
        report_content += f"""

---

æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(report_content)

async def main():
    """ä¸»å‡½æ•°"""
    print("âš¡ é‡å­æ€§èƒ½åŸºå‡†æµ‹è¯•ç³»ç»Ÿ")
    print("=" * 50)
    
    # æ£€æŸ¥ä¾èµ–
    try:
        import numpy as np
        import psutil
    except ImportError as e:
        print(f"âŒ ç¼ºå°‘ä¾èµ–åº“: {e}")
        print("è¯·å®‰è£…: pip install numpy psutil")
        return
    
    # åˆ›å»ºåŸºå‡†æµ‹è¯•å™¨
    benchmark = QuantumPerformanceBenchmark()
    
    try:
        # è¿è¡ŒåŸºå‡†æµ‹è¯•
        results = await benchmark.run_comprehensive_benchmark()
        
        print("\n" + "=" * 50)
        print("âœ… åŸºå‡†æµ‹è¯•å®Œæˆ!")
        
        if "error" not in results:
            summary = results.get("summary", {})
            print(f"ğŸ“Š æ€»ä½“è¯„åˆ†: {summary.get('overall_score', 0):.2f}")
            print(f"ğŸ† æ€§èƒ½ç­‰çº§: {summary.get('performance_grade', 'Unknown')}")
            
            recommendations = results.get("recommendations", [])
            if recommendations:
                print(f"\nğŸ’¡ ä¸»è¦å»ºè®®:")
                for rec in recommendations[:3]:  # æ˜¾ç¤ºå‰3ä¸ªå»ºè®®
                    print(f"  â€¢ {rec}")
        else:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {results['error']}")
        
    except Exception as e:
        print(f"âŒ åŸºå‡†æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())

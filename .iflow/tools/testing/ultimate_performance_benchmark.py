#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸŒŒ ç»ˆææ€§èƒ½åŸºå‡†æµ‹è¯•ç³»ç»Ÿ (Ultimate Performance Benchmark System)
èåˆäº†V10çš„å…ˆè¿›æ¶æ„ä¸V4çš„æ·±åº¦åˆ†æèƒ½åŠ›ï¼Œä¸ºiflowæä¾›å…¨é¢ã€ç²¾å‡†ã€å¯è¿›åŒ–çš„è‡ªåŠ¨åŒ–æ€§èƒ½è¯„ä¼°ã€‚
"""

import os
import sys
import json
import asyncio
import logging
import time
import threading
import statistics
from typing import Dict, List, Any, Optional, Callable, Union
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from pathlib import Path
from enum import Enum
from concurrent.futures import ThreadPoolExecutor
import psutil
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from abc import ABC, abstractmethod

# ==============================================================================
# æ ¸å¿ƒé…ç½®ä¸æ—¥å¿—
# ==============================================================================

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ==============================================================================
# æ•°æ®ç±»å®šä¹‰ (æºè‡ª V10 çš„æ¸…æ™°ç»“æ„)
# ==============================================================================

class BenchmarkType(Enum):
    """åŸºå‡†æµ‹è¯•ç±»å‹"""
    RESPONSE_TIME = "response_time"
    THROUGHPUT = "throughput"
    CONCURRENCY = "concurrency"
    STRESS = "stress"
    ENDURANCE = "endurance"

@dataclass
class BenchmarkConfig:
    """åŸºå‡†æµ‹è¯•é…ç½®"""
    name: str
    benchmark_type: BenchmarkType
    duration: timedelta
    target_function: Callable
    warmup_time: timedelta = field(default_factory=lambda: timedelta(seconds=10))
    concurrent_users: int = 1
    parameters: Dict[str, Any] = field(default_factory=dict)
    success_criteria: Dict[str, float] = field(default_factory=dict)

@dataclass
class BenchmarkResult:
    """å•ä¸ªåŸºå‡†æµ‹è¯•çš„ç»“æœ"""
    config_name: str
    benchmark_type: BenchmarkType
    start_time: datetime
    end_time: datetime
    duration: float
    metrics: Dict[str, Any]
    samples: List[Dict[str, float]] = field(default_factory=list)
    success: bool = False
    error_message: Optional[str] = None
    analysis: Dict[str, Any] = field(default_factory=dict) # æ–°å¢ï¼šç”¨äºå­˜å‚¨æ·±åº¦åˆ†æç»“æœ

# ==============================================================================
# åŸºå‡†æµ‹è¯•åŸºç±» (æºè‡ª V10 çš„ä¼˜é›…è®¾è®¡)
# ==============================================================================

class BaseBenchmark(ABC):
    """åŸºå‡†æµ‹è¯•æŠ½è±¡åŸºç±»"""
    
    def __init__(self, config: BenchmarkConfig):
        self.config = config
        self.samples = []
        self.is_running = False
        
    @abstractmethod
    async def setup(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        pass
    
    @abstractmethod
    async def execute(self) -> Dict[str, Any]:
        """æ‰§è¡Œæ ¸å¿ƒæµ‹è¯•é€»è¾‘"""
        pass
    
    @abstractmethod
    async def cleanup(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        pass
    
    @abstractmethod
    def calculate_metrics(self) -> Dict[str, Any]:
        """æ ¹æ®æ ·æœ¬è®¡ç®—æœ€ç»ˆæŒ‡æ ‡"""
        pass
    
    async def run(self) -> BenchmarkResult:
        """è¿è¡Œå®Œæ•´çš„åŸºå‡†æµ‹è¯•æµç¨‹"""
        logger.info(f"â–¶ï¸ å¼€å§‹åŸºå‡†æµ‹è¯•: {self.config.name} ({self.config.benchmark_type.value})")
        
        result = BenchmarkResult(
            config_name=self.config.name,
            benchmark_type=self.config.benchmark_type,
            start_time=datetime.now(),
            end_time=datetime.now(),
            duration=0.0,
            metrics={}
        )
        
        try:
            await self.setup()
            
            if self.config.warmup_time.total_seconds() > 0:
                logger.info(f"ğŸ”¥ é¢„çƒ­ {self.config.warmup_time.total_seconds()} ç§’...")
                await asyncio.sleep(self.config.warmup_time.total_seconds())
            
            logger.info(f"ğŸš€ æ‰§è¡Œæµ‹è¯•ï¼ŒæŒç»­ {self.config.duration.total_seconds()} ç§’...")
            start_time = time.time()
            
            self.is_running = True
            execution_summary = await self.execute()
            
            end_time = time.time()
            self.is_running = False
            
            result.end_time = datetime.now()
            result.duration = end_time - start_time
            
            logger.info("ğŸ“ˆ è®¡ç®—æŒ‡æ ‡...")
            calculated_metrics = self.calculate_metrics()
            
            result.metrics.update(execution_summary)
            result.metrics.update(calculated_metrics)
            result.samples = self.samples.copy()
            result.success = True
            
        except Exception as e:
            logger.error(f"âŒ åŸºå‡†æµ‹è¯•å¤±è´¥: {self.config.name}, é”™è¯¯: {e}", exc_info=True)
            result.end_time = datetime.now()
            result.error_message = str(e)
        finally:
            await self.cleanup()

        logger.info(f"â¹ï¸ åŸºå‡†æµ‹è¯•å®Œæˆ: {self.config.name}")
        return result

# ==============================================================================
# å…·ä½“åŸºå‡†æµ‹è¯•å®ç° (æºè‡ª V10)
# ==============================================================================

class ResponseTimeBenchmark(BaseBenchmark):
    """å“åº”æ—¶é—´åŸºå‡†æµ‹è¯•"""
    
    async def setup(self):
        logger.info(f"  è®¾ç½®å“åº”æ—¶é—´æµ‹è¯•ç¯å¢ƒ for {self.config.name}")

    async def execute(self) -> Dict[str, Any]:
        if not self.config.target_function:
            raise ValueError("ç›®æ ‡å‡½æ•°æœªè®¾ç½®")
        
        end_time = time.time() + self.config.duration.total_seconds()
        
        while time.time() < end_time and self.is_running:
            start_sample_time = time.time()
            success = True
            error = None
            try:
                if asyncio.iscoroutinefunction(self.config.target_function):
                    await self.config.target_function(**self.config.parameters)
                else:
                    loop = asyncio.get_event_loop()
                    await loop.run_in_executor(None, lambda: self.config.target_function(**self.config.parameters))
            except Exception as e:
                success = False
                error = str(e)
            
            response_time = time.time() - start_sample_time
            self.samples.append({
                'timestamp': start_sample_time,
                'response_time': response_time,
                'success': 1 if success else 0,
            })
            if error:
                self.samples[-1]['error'] = error

        return {'total_requests': len(self.samples)}

    async def cleanup(self):
        logger.info(f"  æ¸…ç†å“åº”æ—¶é—´æµ‹è¯•ç¯å¢ƒ for {self.config.name}")

    def calculate_metrics(self) -> Dict[str, Any]:
        if not self.samples: return {}
        
        successful_samples = [s for s in self.samples if s['success'] == 1]
        if not successful_samples: return {'error_rate': 1.0}
        
        response_times = [s['response_time'] for s in successful_samples]
        
        return {
            'avg_response_time': statistics.mean(response_times),
            'p95_response_time': np.percentile(response_times, 95),
            'p99_response_time': np.percentile(response_times, 99),
            'std_dev': statistics.stdev(response_times) if len(response_times) > 1 else 0,
            'error_rate': 1.0 - len(successful_samples) / len(self.samples)
        }

class ThroughputBenchmark(BaseBenchmark):
    """ååé‡åŸºå‡†æµ‹è¯•"""

    async def setup(self):
        logger.info(f"  è®¾ç½®ååé‡æµ‹è¯•ç¯å¢ƒ for {self.config.name}")

    async def execute(self) -> Dict[str, Any]:
        if not self.config.target_function:
            raise ValueError("ç›®æ ‡å‡½æ•°æœªè®¾ç½®")

        async def worker():
            while self.is_running:
                start_sample_time = time.time()
                success = True
                error = None
                try:
                    if asyncio.iscoroutinefunction(self.config.target_function):
                        await self.config.target_function(**self.config.parameters)
                    else:
                        loop = asyncio.get_event_loop()
                        await loop.run_in_executor(None, lambda: self.config.target_function(**self.config.parameters))
                except Exception as e:
                    success = False
                    error = str(e)
                
                response_time = time.time() - start_sample_time
                self.samples.append({
                    'timestamp': start_sample_time,
                    'response_time': response_time,
                    'success': 1 if success else 0,
                })
                if error:
                    self.samples[-1]['error'] = error

        tasks = [asyncio.create_task(worker()) for _ in range(self.config.concurrent_users)]
        await asyncio.sleep(self.config.duration.total_seconds())
        self.is_running = False
        await asyncio.gather(*tasks, return_exceptions=True)

        return {'total_requests': len(self.samples)}

    async def cleanup(self):
        logger.info(f"  æ¸…ç†ååé‡æµ‹è¯•ç¯å¢ƒ for {self.config.name}")

    def calculate_metrics(self) -> Dict[str, Any]:
        if not self.samples: return {}
        
        successful_samples = [s for s in self.samples if s['success'] == 1]
        if not successful_samples: return {'throughput_rps': 0, 'error_rate': 1.0}

        total_duration = self.config.duration.total_seconds()
        throughput_rps = len(successful_samples) / total_duration

        return {
            'throughput_rps': throughput_rps,
            'total_successful_requests': len(successful_samples),
            'error_rate': 1.0 - len(successful_samples) / len(self.samples)
        }

# ==============================================================================
# æ·±åº¦åˆ†æå¼•æ“ (æºè‡ª V4 çš„æ™ºèƒ½æ ¸å¿ƒ)
# ==============================================================================

class AdvancedPatternAnalyzer:
    """é«˜çº§æ¨¡å¼åˆ†æå™¨ (èåˆV4çš„é‡å­åˆ†ææ€æƒ³)"""

    def analyze(self, result: BenchmarkResult) -> Dict[str, Any]:
        """å¯¹å•ä¸ªæµ‹è¯•ç»“æœè¿›è¡Œæ·±åº¦åˆ†æ"""
        logger.info(f"ğŸ”¬ å¯¹ {result.config_name} è¿›è¡Œæ·±åº¦åˆ†æ...")
        
        analysis = {
            'patterns': self._identify_patterns(result.samples),
            'anomalies': self._detect_anomalies(result.samples),
            'optimization_opportunities': self._identify_optimization_opportunities(result.metrics),
            'quantum_insights': self._generate_quantum_insights(result.metrics)
        }
        return analysis

    def _identify_patterns(self, samples: List[Dict[str, float]]) -> List[str]:
        """è¯†åˆ«æ€§èƒ½æ¨¡å¼"""
        if len(samples) < 20: return []
        
        patterns = []
        response_times = [s['response_time'] for s in samples if s['success']]
        
        # è¶‹åŠ¿åˆ†æ
        try:
            coeffs = np.polyfit(range(len(response_times)), response_times, 1)
            slope = coeffs[0]
            if abs(slope) > (statistics.mean(response_times) * 0.01): # è¶…è¿‡å‡å€¼1%çš„æ–œç‡
                direction = "æ¶åŒ–" if slope > 0 else "æ”¹å–„"
                patterns.append(f"å“åº”æ—¶é—´å­˜åœ¨æ˜æ˜¾çš„çº¿æ€§{direction}è¶‹åŠ¿ã€‚")
        except Exception:
            pass # æ— æ³•è®¡ç®—è¶‹åŠ¿

        return patterns

    def _detect_anomalies(self, samples: List[Dict[str, float]]) -> List[str]:
        """ä½¿ç”¨ç»Ÿè®¡æ–¹æ³•æ£€æµ‹å¼‚å¸¸ç‚¹"""
        if len(samples) < 10: return []

        anomalies = []
        response_times = [s['response_time'] for s in samples if s['success']]
        mean = statistics.mean(response_times)
        stdev = statistics.stdev(response_times) if len(response_times) > 1 else 0

        if stdev == 0: return []

        # 3-sigma è§„åˆ™
        upper_bound = mean + 3 * stdev
        for i, sample in enumerate(samples):
            if sample.get('success') and sample['response_time'] > upper_bound:
                anomalies.append(f"åœ¨æ ·æœ¬ {i} å¤„æ£€æµ‹åˆ°é«˜å»¶è¿Ÿå¼‚å¸¸: {sample['response_time']:.3f}s (è¿œè¶…å‡å€¼ {mean:.3f}s)")
        
        return anomalies

    def _identify_optimization_opportunities(self, metrics: Dict[str, Any]) -> List[str]:
        """è¯†åˆ«ä¼˜åŒ–æœºä¼š"""
        opportunities = []
        if metrics.get('p99_response_time', 0) > 1.5:
            opportunities.append("P99 å»¶è¿Ÿè¿‡é«˜ï¼Œè¡¨æ˜å­˜åœ¨é•¿å°¾è¯·æ±‚ï¼Œå»ºè®®æ’æŸ¥æ…¢æŸ¥è¯¢æˆ–GCæš‚åœã€‚")
        if metrics.get('error_rate', 0) > 0.05:
            opportunities.append("é”™è¯¯ç‡é«˜äº5%ï¼Œå»ºè®®æ£€æŸ¥ç³»ç»Ÿç¨³å®šæ€§å’Œå¼‚å¸¸å¤„ç†é€»è¾‘ã€‚")
        if metrics.get('throughput_rps', float('inf')) < 50:
            opportunities.append("ååé‡è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥æ˜¯å¦å­˜åœ¨I/Oç“¶é¢ˆæˆ–é”ç«äº‰ã€‚")
        return opportunities

    def _generate_quantum_insights(self, metrics: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ¨¡æ‹Ÿçš„é‡å­æ´å¯Ÿ (æ¦‚å¿µç»§æ‰¿)"""
        potential = 0
        if metrics.get('avg_response_time', 0) > 0.8: potential += 0.3
        if metrics.get('throughput_rps', float('inf')) < 100: potential += 0.4
        
        insights = {'quantum_optimization_potential': min(potential, 1.0)}
        if potential > 0.5:
            insights['recommended_algorithms'] = ['Quantum Annealing for optimization', 'Grover\'s Algorithm for search']
            insights['estimated_speedup'] = f"{1.5 + potential:.1f}x"
        
        return insights

# ==============================================================================
# æŠ¥å‘Šç”Ÿæˆå™¨ (èåˆ V4 çš„å¯è§†åŒ–èƒ½åŠ›)
# ==============================================================================

class ReportGenerator:
    """ç”ŸæˆåŒ…å«æ‘˜è¦ã€å›¾è¡¨å’Œå»ºè®®çš„ç»¼åˆæŠ¥å‘Š"""

    def __init__(self, output_dir: str = "benchmark_reports"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)

    def generate(self, results: List[BenchmarkResult], session_id: str) -> str:
        """ç”Ÿæˆä¸»æŠ¥å‘Š"""
        report_path = self.output_dir / f"report_{session_id}"
        report_path.mkdir(exist_ok=True)
        
        logger.info(f"ğŸ“„ ç”ŸæˆæŠ¥å‘Šåˆ°: {report_path}")

        # ç”ŸæˆJSON
        json_report = self._generate_json(results)
        with open(report_path / "report.json", 'w', encoding='utf-8') as f:
            json.dump(json_report, f, indent=2, default=str)

        # ç”ŸæˆMarkdown
        md_content = self._generate_markdown(json_report, session_id)
        with open(report_path / "report.md", 'w', encoding='utf-8') as f:
            f.write(md_content)
            
        # ç”Ÿæˆå›¾è¡¨
        self._generate_visualizations(results, report_path)

        return str(report_path)

    def _generate_json(self, results: List[BenchmarkResult]) -> Dict[str, Any]:
        """å°†ç»“æœç¼–è¯‘ä¸ºå­—å…¸æ ¼å¼"""
        detailed_results = []
        for r in results:
            res_dict = asdict(r)
            # ç§»é™¤å¤§çš„æ ·æœ¬æ•°æ®ä»¥ä¿æŒJSONæŠ¥å‘Šç®€æ´
            res_dict.pop('samples', None)
            detailed_results.append(res_dict)

        return {
            'summary': self._create_summary(results),
            'detailed_results': detailed_results
        }

    def _create_summary(self, results: List[BenchmarkResult]) -> Dict[str, Any]:
        """åˆ›å»ºæ‘˜è¦ä¿¡æ¯"""
        total_tests = len(results)
        successful_tests = sum(1 for r in results if r.success)
        
        return {
            'total_benchmarks': total_tests,
            'successful_benchmarks': successful_tests,
            'success_rate': successful_tests / total_tests if total_tests > 0 else 0,
            'total_duration_seconds': sum(r.duration for r in results)
        }

    def _generate_markdown(self, report_data: Dict[str, Any], session_id: str) -> str:
        """ç”ŸæˆMarkdownæ ¼å¼çš„æŠ¥å‘Š"""
        md = f"# æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š (Session: {session_id})\n\n"
        
        # æ‘˜è¦
        summary = report_data['summary']
        md += "## ğŸ“Š æµ‹è¯•æ‘˜è¦\n\n"
        md += f"- **æ€»æµ‹è¯•æ•°**: {summary['total_benchmarks']}\n"
        md += f"- **æˆåŠŸç‡**: {summary['success_rate']:.2%}\n"
        md += f"- **æ€»è€—æ—¶**: {summary['total_duration_seconds']:.2f} ç§’\n\n"
        md += "![æµ‹è¯•æˆåŠŸç‡](success_rate.png)\n\n"

        # è¯¦ç»†ç»“æœ
        md += "## ğŸ”¬ è¯¦ç»†ç»“æœ\n\n"
        for result in report_data['detailed_results']:
            status = "âœ…" if result['success'] else "âŒ"
            md += f"### {status} {result['config_name']} ({result['benchmark_type']})\n\n"
            md += "| æŒ‡æ ‡ | æ•°å€¼ |\n|---|---|\n"
            for key, value in result['metrics'].items():
                if isinstance(value, float):
                    md += f"| {key} | {value:.3f} |\n"
                else:
                    md += f"| {key} | {value} |\n"
            md += "\n"
            
            # æ·±åº¦åˆ†æ
            analysis = result.get('analysis', {})
            if analysis.get('patterns'):
                md += "**è¯†åˆ«çš„æ¨¡å¼:**\n"
                for p in analysis['patterns']: md += f"- {p}\n"
            if analysis.get('anomalies'):
                md += "**æ£€æµ‹åˆ°çš„å¼‚å¸¸:**\n"
                for a in analysis['anomalies']: md += f"- {a}\n"
            if analysis.get('optimization_opportunities'):
                md += "**ä¼˜åŒ–æœºä¼š:**\n"
                for o in analysis['optimization_opportunities']: md += f"- {o}\n"
            if analysis.get('quantum_insights', {}).get('recommended_algorithms'):
                md += "**é‡å­æ´å¯Ÿ:**\n"
                md += f"- ä¼˜åŒ–æ½œåŠ›: {analysis['quantum_insights']['quantum_optimization_potential']:.1%}\n"
                md += f"- æ¨èç®—æ³•: {', '.join(analysis['quantum_insights']['recommended_algorithms'])}\n"
            md += "\n"

        return md

    def _generate_visualizations(self, results: List[BenchmarkResult], report_path: Path):
        """ç”Ÿæˆå›¾è¡¨ (ç»§æ‰¿è‡ªV4)"""
        try:
            plt.style.use('seaborn-v0_8-darkgrid')

            # 1. æˆåŠŸç‡é¥¼å›¾
            summary = self._create_summary(results)
            fig, ax = plt.subplots()
            ax.pie([summary['successful_benchmarks'], summary['total_benchmarks'] - summary['successful_benchmarks']],
                   labels=['æˆåŠŸ', 'å¤±è´¥'], autopct='%1.1f%%', colors=['#4CAF50', '#F44336'])
            ax.set_title('åŸºå‡†æµ‹è¯•æˆåŠŸç‡')
            plt.savefig(report_path / "success_rate.png")
            plt.close(fig)

            # 2. å„æµ‹è¯•å“åº”æ—¶é—´å¯¹æ¯”
            fig, ax = plt.subplots(figsize=(10, 6))
            names = [r.config_name for r in results if r.success and 'avg_response_time' in r.metrics]
            times = [r.metrics['avg_response_time'] for r in results if r.success and 'avg_response_time' in r.metrics]
            if names:
                ax.barh(names, times, color='skyblue')
                ax.set_xlabel('å¹³å‡å“åº”æ—¶é—´ (ç§’)')
                ax.set_title('å„æµ‹è¯•å¹³å‡å“åº”æ—¶é—´')
                plt.tight_layout()
                plt.savefig(report_path / "response_times.png")
            plt.close(fig)

            logger.info("ğŸ¨ å¯è§†åŒ–å›¾è¡¨å·²ç”Ÿæˆã€‚")
        except Exception as e:
            logger.warning(f"âš ï¸ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨å¤±è´¥: {e}")

# ==============================================================================
# ä¸»æ§åˆ¶ç³»ç»Ÿ
# ==============================================================================

class UltimatePerformanceBenchmark:
    """ç»ˆææ€§èƒ½åŸºå‡†æµ‹è¯•ä¸»ç³»ç»Ÿ"""
    
    def __init__(self, report_dir: str = "benchmark_reports"):
        self.configs: List[BenchmarkConfig] = []
        self.report_generator = ReportGenerator(output_dir=report_dir)
        self.analyzer = AdvancedPatternAnalyzer()

    def register(self, config: BenchmarkConfig):
        """æ³¨å†Œä¸€ä¸ªåŸºå‡†æµ‹è¯•é…ç½®"""
        self.configs.append(config)
        logger.info(f"âœ… å·²æ³¨å†ŒåŸºå‡†æµ‹è¯•: {config.name}")

    async def run_all(self) -> str:
        """è¿è¡Œæ‰€æœ‰å·²æ³¨å†Œçš„åŸºå‡†æµ‹è¯•å¹¶ç”ŸæˆæŠ¥å‘Š"""
        session_id = datetime.now().strftime('%Y%m%d_%H%M%S')
        logger.info(f"ğŸ å¼€å§‹æ‰§è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯• (Session: {session_id})")
        
        all_results = []
        for config in self.configs:
            benchmark_class = None
            if config.benchmark_type == BenchmarkType.RESPONSE_TIME:
                benchmark_class = ResponseTimeBenchmark
            elif config.benchmark_type == BenchmarkType.THROUGHPUT:
                benchmark_class = ThroughputBenchmark
            # ...å¯ä»¥æ‰©å±•å…¶ä»–ç±»å‹
            
            if benchmark_class:
                benchmark_instance = benchmark_class(config)
                result = await benchmark_instance.run()
                
                # è¿›è¡Œæ·±åº¦åˆ†æ
                if result.success:
                    result.analysis = self.analyzer.analyze(result)

                all_results.append(result)
            else:
                logger.warning(f"æœªæ‰¾åˆ°åŸºå‡†æµ‹è¯•ç±»å‹ {config.benchmark_type} çš„å®ç°ã€‚")

        report_path = self.report_generator.generate(all_results, session_id)
        logger.info(f"ğŸ‰ æ‰€æœ‰åŸºå‡†æµ‹è¯•å®Œæˆã€‚æŠ¥å‘Šä½äº: {report_path}")
        return report_path

# ==============================================================================
# ç¤ºä¾‹ç”¨æ³•
# ==============================================================================

async def example_task_fast():
    """æ¨¡æ‹Ÿä¸€ä¸ªå¿«é€Ÿä»»åŠ¡"""
    await asyncio.sleep(np.random.uniform(0.05, 0.1))

async def example_task_slow():
    """æ¨¡æ‹Ÿä¸€ä¸ªè¾ƒæ…¢ä¸”å¯èƒ½å¤±è´¥çš„ä»»åŠ¡"""
    await asyncio.sleep(np.random.uniform(0.2, 0.5))
    if np.random.rand() < 0.1:
        raise ConnectionError("æ¨¡æ‹Ÿç½‘ç»œè¿æ¥å¤±è´¥")

async def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    logger.info("ğŸš€ åˆå§‹åŒ–ç»ˆææ€§èƒ½åŸºå‡†æµ‹è¯•ç³»ç»Ÿ...")
    
    benchmark_system = UltimatePerformanceBenchmark()

    # æ³¨å†Œæµ‹è¯•
    benchmark_system.register(BenchmarkConfig(
        name="API_Fast_Response_Time",
        benchmark_type=BenchmarkType.RESPONSE_TIME,
        duration=timedelta(seconds=15),
        target_function=example_task_fast
    ))
    
    benchmark_system.register(BenchmarkConfig(
        name="API_Slow_Throughput",
        benchmark_type=BenchmarkType.THROUGHPUT,
        duration=timedelta(seconds=20),
        concurrent_users=10,
        target_function=example_task_slow
    ))

    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    report_directory = await benchmark_system.run_all()
    
    print("\n" + "="*60)
    print(f"âœ… æµ‹è¯•å®Œæˆï¼è¯·æŸ¥çœ‹æŠ¥å‘Šç›®å½•: {report_directory}")
    print("="*60)

if __name__ == "__main__":
    asyncio.run(main())
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸŒŒ ç»ˆæå·¥ä½œæµå¼•æ“ V6 (Ultimate Workflow Engine V6)
T-MIAå‡¤å‡°æ¶æ„çš„æ ¸å¿ƒæŒ‡æŒ¥å®˜ï¼Œé›†æˆiflow CLIæ·±åº¦æ”¯æŒå’Œæ™ºèƒ½å·¥å…·è°ƒç”¨ä¼˜åŒ–

ä½ ä¸€å®šè¦è¶…çº§æ€è€ƒã€æé™æ€è€ƒã€æ·±åº¦æ€è€ƒï¼Œå…¨åŠ›æ€è€ƒã€è¶…å¼ºæ€è€ƒï¼Œè®¤çœŸä»”ç»†æ€è€ƒï¼ˆultrathinkã€think really super hardã€think intenselyï¼‰ã€‚
"""

import os
import sys
import json
import asyncio
import logging
import time
import uuid
import hashlib
import traceback
from typing import Dict, List, Any, Optional, Union, Callable
from pathlib import Path
from enum import Enum
import threading
from dataclasses import dataclass, field
from datetime import datetime, timedelta
import concurrent.futures
from collections import defaultdict, deque

# åŠ¨æ€æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°sys.path
try:
    project_root = Path(__file__).resolve().parent.parent
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
    
    from iflow.core.ultimate_cognitive_core_v6 import UltimateCognitiveCoreV6
    from iflow.adapters.universal_llm_adapter_v14 import UltimateLLMAdapterV14
    from iflow.core.ultimate_arq_engine_v6 import UltimateARQEngineV6
    from iflow.core.ultimate_consciousness_system_v6 import UltimateConsciousnessSystemV6
except ImportError as e:
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    logger.error(f"å…³é”®æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

# --- æ—¥å¿—é…ç½® ---
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('workflow_engine_v6.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# --- æšä¸¾å®šä¹‰ ---
class WorkflowState(Enum):
    """å·¥ä½œæµçŠ¶æ€"""
    INITIALIZING = "initializing"
    PLANNING = "planning"
    EXECUTING = "executing"
    TOOL_CALLING = "tool_calling"
    VALIDATING = "validating"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"

class TaskPriority(Enum):
    """ä»»åŠ¡ä¼˜å…ˆçº§"""
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"

class ExecutionMode(Enum):
    """æ‰§è¡Œæ¨¡å¼"""
    AUTONOMOUS = "autonomous"
    INTERACTIVE = "interactive"
    BATCH = "batch"
    STREAMING = "streaming"

@dataclass
class TaskContext:
    """ä»»åŠ¡ä¸Šä¸‹æ–‡"""
    task_id: str
    user_input: str
    execution_mode: ExecutionMode
    priority: TaskPriority
    dependencies: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)
    created_at: datetime = field(default_factory=datetime.now)
    timeout: Optional[int] = None

@dataclass
class ExecutionResult:
    """æ‰§è¡Œç»“æœ"""
    success: bool
    task_id: str
    output: Any = None
    error: Optional[str] = None
    execution_time: float = 0.0
    tool_calls: List[Dict] = field(default_factory=list)
    validation_results: Dict = field(default_factory=dict)
    confidence_score: float = 0.0

class UltimateWorkflowEngineV6:
    """
    ç»ˆæå·¥ä½œæµå¼•æ“V6 - T-MIAå‡¤å‡°æ¶æ„çš„æ ¸å¿ƒæŒ‡æŒ¥å®˜
    é›†æˆiflow CLIæ·±åº¦æ”¯æŒã€æ™ºèƒ½å·¥å…·è°ƒç”¨ä¼˜åŒ–ã€å¤šæ¨¡å‹æ™ºèƒ½è·¯ç”±
    """
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls, *args, **kwargs):
        with cls._lock:
            if cls._instance is None:
                cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        if hasattr(self, '_initialized') and self._initialized:
            return
        
        self.config = config or {}
        self.engine_id = f"UWE-V6-{uuid.uuid4().hex[:8]}"
        
        # æ ¸å¿ƒç»„ä»¶
        self.model_adapter: Optional[UltimateLLMAdapterV14] = None
        self.cognitive_core: Optional[UltimateCognitiveCoreV6] = None
        self.arq_engine: Optional[UltimateARQEngineV6] = None
        self.consciousness_system: Optional[UltimateConsciousnessSystemV6] = None
        
        # æ‰§è¡Œç®¡ç†
        self.active_workflows: Dict[str, Dict] = {}
        self.task_queue = asyncio.Queue()
        self.execution_contexts: Dict[str, TaskContext] = {}
        self.result_cache: Dict[str, ExecutionResult] = {}
        
        # æ€§èƒ½ç›‘æ§
        self.performance_metrics = {
            'total_executions': 0,
            'success_rate': 0.0,
            'avg_execution_time': 0.0,
            'tool_call_success_rate': 0.0,
            'model_utilization': defaultdict(int)
        }
        
        # å¹¶å‘æ§åˆ¶
        self.max_concurrent_tasks = self.config.get('max_concurrent_tasks', 5)
        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=10)
        
        # ç¼“å­˜é…ç½®
        self.max_cache_size = self.config.get('max_cache_size', 1000)
        self.cache_ttl = self.config.get('cache_ttl', 3600)  # 1å°æ—¶
        
        self._initialized = False
        logger.info(f"ğŸš€ ç»ˆæå·¥ä½œæµå¼•æ“V6åˆå§‹åŒ–å®Œæˆ - Engine ID: {self.engine_id}")
    
    async def initialize(self):
        """å¼‚æ­¥åˆå§‹åŒ–æ‰€æœ‰æ ¸å¿ƒç»„ä»¶"""
        if self._initialized:
            logger.info("å¼•æ“å·²åˆå§‹åŒ–ï¼Œè·³è¿‡...")
            return
        
        with self._lock:
            if self._initialized:
                return
            
            logger.info("ğŸ”§ å¼€å§‹åˆå§‹åŒ–T-MIAå‡¤å‡°æ¶æ„æ ¸å¿ƒç»„ä»¶...")
            
            try:
                # 1. åˆå§‹åŒ–æ¨¡å‹é€‚é…å™¨V14
                start_time = time.time()
                self.model_adapter = UltimateLLMAdapterV14()
                logger.info(f"âœ… æ¨¡å‹é€‚é…å™¨V14åˆå§‹åŒ–å®Œæˆ ({time.time() - start_time:.2f}s)")
                
                # 2. åˆå§‹åŒ–ARQå¼•æ“V6
                start_time = time.time()
                self.arq_engine = UltimateARQEngineV6()
                logger.info(f"âœ… ARQå¼•æ“V6åˆå§‹åŒ–å®Œæˆ ({time.time() - start_time:.2f}s)")
                
                # 3. åˆå§‹åŒ–æ„è¯†æµç³»ç»ŸV6
                start_time = time.time()
                self.consciousness_system = UltimateConsciousnessSystemV6()
                logger.info(f"âœ… æ„è¯†æµç³»ç»ŸV6åˆå§‹åŒ–å®Œæˆ ({time.time() - start_time:.2f}s)")
                
                # 4. åˆå§‹åŒ–è®¤çŸ¥æ ¸å¿ƒV6ï¼ˆæ³¨å…¥å…¶ä»–ç»„ä»¶ï¼‰
                start_time = time.time()
                self.cognitive_core = UltimateCognitiveCoreV6(
                    model_adapter=self.model_adapter,
                    arq_engine=self.arq_engine,
                    consciousness_system=self.consciousness_system
                )
                logger.info(f"âœ… è®¤çŸ¥æ ¸å¿ƒV6åˆå§‹åŒ–å®Œæˆ ({time.time() - start_time:.2f}s)")
                
                # 5. é¢„çƒ­æ¨¡å‹å’Œç¼“å­˜
                await self._preheat_system()
                
                self._initialized = True
                logger.info("ğŸ‰ T-MIAå‡¤å‡°æ¶æ„åˆå§‹åŒ–å®Œæˆï¼")
                
            except Exception as e:
                logger.error(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}", exc_info=True)
                raise
    
    async def _preheat_system(self):
        """é¢„çƒ­ç³»ç»Ÿ"""
        logger.info("ğŸ”¥ å¼€å§‹ç³»ç»Ÿé¢„çƒ­...")
        
        # é¢„çƒ­æ¨¡å‹é€‚é…å™¨
        if self.model_adapter:
            await self.model_adapter.preheat()
        
        # é¢„çƒ­è®¤çŸ¥æ ¸å¿ƒ
        if self.cognitive_core:
            await self.cognitive_core.preheat()
        
        logger.info("âœ… ç³»ç»Ÿé¢„çƒ­å®Œæˆ")
    
    async def execute_task(
        self, 
        user_input: str, 
        execution_mode: ExecutionMode = ExecutionMode.AUTONOMOUS,
        priority: TaskPriority = TaskPriority.MEDIUM,
        dependencies: List[str] = None,
        metadata: Dict[str, Any] = None
    ) -> ExecutionResult:
        """
        æ‰§è¡Œå•ä¸ªä»»åŠ¡
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥çš„ä»»åŠ¡æè¿°
            execution_mode: æ‰§è¡Œæ¨¡å¼
            priority: ä»»åŠ¡ä¼˜å…ˆçº§
            dependencies: ä¾èµ–ä»»åŠ¡IDåˆ—è¡¨
            metadata: ä»»åŠ¡å…ƒæ•°æ®
        
        Returns:
            ExecutionResult: æ‰§è¡Œç»“æœ
        """
        if not self._initialized:
            await self.initialize()
        
        task_id = str(uuid.uuid4())
        context = TaskContext(
            task_id=task_id,
            user_input=user_input,
            execution_mode=execution_mode,
            priority=priority,
            dependencies=dependencies or [],
            metadata=metadata or {}
        )
        
        self.execution_contexts[task_id] = context
        
        logger.info(f"ğŸ“‹ å¼€å§‹æ‰§è¡Œä»»åŠ¡ [{task_id}]: {user_input[:50]}...")
        
        start_time = time.time()
        
        try:
            # 1. æ™ºèƒ½ä»»åŠ¡åˆ†æå’Œè§„åˆ’
            planning_result = await self._analyze_and_plan(task_id, user_input, context)
            if not planning_result.success:
                return ExecutionResult(
                    success=False,
                    task_id=task_id,
                    error=planning_result.error,
                    execution_time=time.time() - start_time
                )
            
            # 2. æ‰§è¡Œå·¥ä½œæµ
            execution_result = await self._execute_workflow(task_id, planning_result.output, context)
            
            # 3. éªŒè¯å’Œä¼˜åŒ–ç»“æœ
            final_result = await self._validate_and_optimize(task_id, execution_result, context)
            
            # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
            self._update_performance_metrics(success=True, execution_time=time.time() - start_time)
            
            logger.info(f"âœ… ä»»åŠ¡ [{task_id}] æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶: {time.time() - start_time:.2f}s")
            return final_result
            
        except Exception as e:
            error_msg = f"ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {str(e)}"
            logger.error(error_msg, exc_info=True)
            
            # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
            self._update_performance_metrics(success=False, execution_time=time.time() - start_time)
            
            return ExecutionResult(
                success=False,
                task_id=task_id,
                error=error_msg,
                execution_time=time.time() - start_time
            )
    
    async def _analyze_and_plan(self, task_id: str, user_input: str, context: TaskContext) -> ExecutionResult:
        """æ™ºèƒ½ä»»åŠ¡åˆ†æå’Œè§„åˆ’"""
        logger.debug(f"ğŸ” ä»»åŠ¡åˆ†æ [{task_id}]")
        
        try:
            # è°ƒç”¨è®¤çŸ¥æ ¸å¿ƒè¿›è¡Œæ·±åº¦åˆ†æ
            analysis_result = await self.cognitive_core.analyze_task(
                task_description=user_input,
                context=context.metadata,
                execution_mode=context.execution_mode.value
            )
            
            # ARQå¼•æ“éªŒè¯åˆ†æç»“æœ
            if self.arq_engine:
                validation_result = await self.arq_engine.validate_task_analysis(
                    task_input=user_input,
                    analysis=analysis_result
                )
                
                if not validation_result.get('valid', True):
                    return ExecutionResult(
                        success=False,
                        task_id=task_id,
                        error=f"ä»»åŠ¡åˆ†æéªŒè¯å¤±è´¥: {validation_result.get('reason', 'Unknown')}"
                    )
            
            return ExecutionResult(
                success=True,
                task_id=task_id,
                output=analysis_result,
                confidence_score=analysis_result.get('confidence', 0.8)
            )
            
        except Exception as e:
            logger.error(f"ä»»åŠ¡åˆ†æå¤±è´¥ [{task_id}]: {e}")
            return ExecutionResult(
                success=False,
                task_id=task_id,
                error=f"ä»»åŠ¡åˆ†æå¤±è´¥: {str(e)}"
            )
    
    async def _execute_workflow(self, task_id: str, plan: Dict, context: TaskContext) -> ExecutionResult:
        """æ‰§è¡Œå·¥ä½œæµ"""
        logger.debug(f"âš™ï¸ æ‰§è¡Œå·¥ä½œæµ [{task_id}]")
        
        try:
            # æ ¹æ®æ‰§è¡Œæ¨¡å¼é€‰æ‹©ç­–ç•¥
            if context.execution_mode == ExecutionMode.STREAMING:
                result = await self._execute_streaming_workflow(task_id, plan, context)
            elif context.execution_mode == ExecutionMode.BATCH:
                result = await self._execute_batch_workflow(task_id, plan, context)
            else:
                result = await self._execute_autonomous_workflow(task_id, plan, context)
            
            return result
            
        except Exception as e:
            logger.error(f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥ [{task_id}]: {e}")
            return ExecutionResult(
                success=False,
                task_id=task_id,
                error=f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {str(e)}"
            )
    
    async def _execute_autonomous_workflow(self, task_id: str, plan: Dict, context: TaskContext) -> ExecutionResult:
        """è‡ªä¸»æ‰§è¡Œå·¥ä½œæµ"""
        logger.info(f"ğŸ¤– å¼€å§‹è‡ªä¸»æ‰§è¡Œ [{task_id}]")
        
        try:
            # è°ƒç”¨è®¤çŸ¥æ ¸å¿ƒæ‰§è¡Œå®Œæ•´å·¥ä½œæµ
            workflow_result = await self.cognitive_core.execute_workflow(
                task_plan=plan,
                context=context.metadata,
                tools_enabled=True
            )
            
            return ExecutionResult(
                success=True,
                task_id=task_id,
                output=workflow_result.get('output'),
                tool_calls=workflow_result.get('tool_calls', []),
                confidence_score=workflow_result.get('confidence', 0.8),
                execution_time=workflow_result.get('execution_time', 0.0)
            )
            
        except Exception as e:
            logger.error(f"è‡ªä¸»å·¥ä½œæµæ‰§è¡Œå¤±è´¥ [{task_id}]: {e}")
            return ExecutionResult(
                success=False,
                task_id=task_id,
                error=f"è‡ªä¸»å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {str(e)}"
            )
    
    async def _execute_streaming_workflow(self, task_id: str, plan: Dict, context: TaskContext) -> ExecutionResult:
        """æµå¼æ‰§è¡Œå·¥ä½œæµ"""
        logger.info(f"ğŸŒŠ å¼€å§‹æµå¼æ‰§è¡Œ [{task_id}]")
        
        try:
            # æµå¼æ‰§è¡Œé€»è¾‘
            streaming_result = await self.cognitive_core.execute_streaming_workflow(
                task_plan=plan,
                context=context.metadata
            )
            
            return ExecutionResult(
                success=True,
                task_id=task_id,
                output=streaming_result.get('output'),
                tool_calls=streaming_result.get('tool_calls', []),
                confidence_score=streaming_result.get('confidence', 0.8)
            )
            
        except Exception as e:
            logger.error(f"æµå¼å·¥ä½œæµæ‰§è¡Œå¤±è´¥ [{task_id}]: {e}")
            return ExecutionResult(
                success=False,
                task_id=task_id,
                error=f"æµå¼å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {str(e)}"
            )
    
    async def _execute_batch_workflow(self, task_id: str, plan: Dict, context: TaskContext) -> ExecutionResult:
        """æ‰¹é‡æ‰§è¡Œå·¥ä½œæµ"""
        logger.info(f"ğŸ“¦ å¼€å§‹æ‰¹é‡æ‰§è¡Œ [{task_id}]")
        
        try:
            # æ‰¹é‡æ‰§è¡Œé€»è¾‘
            batch_result = await self.cognitive_core.execute_batch_workflow(
                task_plan=plan,
                context=context.metadata
            )
            
            return ExecutionResult(
                success=True,
                task_id=task_id,
                output=batch_result.get('output'),
                tool_calls=batch_result.get('tool_calls', []),
                confidence_score=batch_result.get('confidence', 0.8)
            )
            
        except Exception as e:
            logger.error(f"æ‰¹é‡å·¥ä½œæµæ‰§è¡Œå¤±è´¥ [{task_id}]: {e}")
            return ExecutionResult(
                success=False,
                task_id=task_id,
                error=f"æ‰¹é‡å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {str(e)}"
            )
    
    async def _validate_and_optimize(self, task_id: str, result: ExecutionResult, context: TaskContext) -> ExecutionResult:
        """éªŒè¯å’Œä¼˜åŒ–ç»“æœ"""
        logger.debug(f"âœ… ç»“æœéªŒè¯å’Œä¼˜åŒ– [{task_id}]")
        
        try:
            # 1. åŸºç¡€éªŒè¯
            validation_results = {}
            
            # 2. å¦‚æœæœ‰ARQå¼•æ“ï¼Œè¿›è¡Œæ·±åº¦éªŒè¯
            if self.arq_engine and result.success:
                validation_input = {
                    'task_id': task_id,
                    'result': result.output,
                    'context': context.metadata
                }
                
                arq_validation = await self.arq_engine.validate_execution_result(validation_input)
                validation_results.update(arq_validation)
            
            # 3. å¦‚æœéªŒè¯å¤±è´¥ï¼Œå°è¯•è‡ªåŠ¨ä¿®å¤
            if not validation_results.get('valid', True):
                logger.warning(f"ç»“æœéªŒè¯å¤±è´¥ï¼Œå°è¯•è‡ªåŠ¨ä¿®å¤ [{task_id}]")
                repair_result = await self._attempt_automatic_repair(task_id, result, validation_results)
                if repair_result.success:
                    result = repair_result
                    logger.info(f"âœ… è‡ªåŠ¨ä¿®å¤æˆåŠŸ [{task_id}]")
            
            # 4. ç¼“å­˜ç»“æœ
            if result.success:
                await self._cache_execution_result(task_id, result)
            
            result.validation_results = validation_results
            return result
            
        except Exception as e:
            logger.error(f"ç»“æœéªŒè¯å’Œä¼˜åŒ–å¤±è´¥ [{task_id}]: {e}")
            return result
    
    async def _attempt_automatic_repair(self, task_id: str, result: ExecutionResult, validation_results: Dict) -> ExecutionResult:
        """å°è¯•è‡ªåŠ¨ä¿®å¤"""
        logger.info(f"ğŸ”§ å°è¯•è‡ªåŠ¨ä¿®å¤ [{task_id}]")
        
        try:
            # è°ƒç”¨è®¤çŸ¥æ ¸å¿ƒè¿›è¡Œä¿®å¤
            repair_input = {
                'original_result': result,
                'validation_errors': validation_results.get('errors', []),
                'task_context': self.execution_contexts.get(task_id)
            }
            
            repair_result = await self.cognitive_core.attempt_repair(repair_input)
            
            if repair_result.get('success', False):
                logger.info(f"âœ… è‡ªåŠ¨ä¿®å¤æˆåŠŸ [{task_id}]")
                return ExecutionResult(
                    success=True,
                    task_id=task_id,
                    output=repair_result.get('repaired_output'),
                    error=None,
                    confidence_score=repair_result.get('confidence', 0.7)
                )
            else:
                logger.info(f"âŒ è‡ªåŠ¨ä¿®å¤å¤±è´¥ [{task_id}]")
                return result
                
        except Exception as e:
            logger.error(f"è‡ªåŠ¨ä¿®å¤è¿‡ç¨‹å¼‚å¸¸ [{task_id}]: {e}")
            return result
    
    async def _cache_execution_result(self, task_id: str, result: ExecutionResult):
        """ç¼“å­˜æ‰§è¡Œç»“æœ"""
        try:
            if len(self.result_cache) >= self.max_cache_size:
                # ç§»é™¤æœ€æ—§çš„ç¼“å­˜é¡¹
                oldest_key = next(iter(self.result_cache))
                del self.result_cache[oldest_key]
            
            self.result_cache[task_id] = result
            logger.debug(f"ğŸ“Š ç»“æœå·²ç¼“å­˜ [{task_id}]")
            
        except Exception as e:
            logger.warning(f"ç¼“å­˜ç»“æœå¤±è´¥ [{task_id}]: {e}")
    
    def _update_performance_metrics(self, success: bool, execution_time: float):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
        self.performance_metrics['total_executions'] += 1
        
        if success:
            # æ›´æ–°æˆåŠŸç‡
            total = self.performance_metrics['total_executions']
            successful = sum(1 for r in self.result_cache.values() if r.success)
            self.performance_metrics['success_rate'] = successful / total if total > 0 else 0.0
            
            # æ›´æ–°å¹³å‡æ‰§è¡Œæ—¶é—´
            times = [r.execution_time for r in self.result_cache.values() if r.success]
            self.performance_metrics['avg_execution_time'] = sum(times) / len(times) if times else 0.0
        
        logger.debug(f"ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡æ›´æ–°: æ€»æ‰§è¡Œæ•°={self.performance_metrics['total_executions']}, æˆåŠŸç‡={self.performance_metrics['success_rate']:.2%}")
    
    async def get_system_status(self) -> Dict[str, Any]:
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        return {
            'engine_id': self.engine_id,
            'initialized': self._initialized,
            'active_workflows': len(self.active_workflows),
            'performance_metrics': dict(self.performance_metrics),
            'cache_size': len(self.result_cache),
            'execution_contexts': len(self.execution_contexts),
            'components': {
                'model_adapter': self.model_adapter is not None,
                'cognitive_core': self.cognitive_core is not None,
                'arq_engine': self.arq_engine is not None,
                'consciousness_system': self.consciousness_system is not None
            }
        }
    
    async def shutdown(self):
        """ä¼˜é›…å…³é—­å¼•æ“"""
        logger.info("ğŸ›‘ å¼€å§‹å…³é—­ç»ˆæå·¥ä½œæµå¼•æ“V6...")
        
        try:
            # å…³é—­è®¤çŸ¥æ ¸å¿ƒ
            if self.cognitive_core:
                await self.cognitive_core.close()
                logger.info("âœ… è®¤çŸ¥æ ¸å¿ƒå·²å…³é—­")
            
            # å…³é—­æ„è¯†æµç³»ç»Ÿ
            if self.consciousness_system:
                self.consciousness_system.close()
                logger.info("âœ… æ„è¯†æµç³»ç»Ÿå·²å…³é—­")
            
            # å…³é—­çº¿ç¨‹æ± 
            self.executor.shutdown(wait=True)
            logger.info("âœ… çº¿ç¨‹æ± å·²å…³é—­")
            
            logger.info("ğŸ‰ ç»ˆæå·¥ä½œæµå¼•æ“V6å·²å®Œå…¨å…³é—­")
            
        except Exception as e:
            logger.error(f"å…³é—­è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}", exc_info=True)

# --- ç¤ºä¾‹ä½¿ç”¨ ---
async def main():
    """ç¤ºä¾‹ä½¿ç”¨"""
    print("ğŸš€ å¯åŠ¨ç»ˆæå·¥ä½œæµå¼•æ“V6æ¼”ç¤º")
    print("=" * 60)
    
    engine = UltimateWorkflowEngineV6()
    
    # åˆå§‹åŒ–
    await engine.initialize()
    
    # æ‰§è¡Œå¤æ‚ä»»åŠ¡
    task = "åˆ†æä¸€ä¸ªç”µå•†å¹³å°çš„æ€§èƒ½ç“¶é¢ˆï¼Œå¹¶æå‡ºä¸€å¥—å®Œæ•´çš„ã€åŒ…å«å‰ç«¯ã€åç«¯å’Œæ•°æ®åº“çš„ä¼˜åŒ–æ–¹æ¡ˆã€‚"
    metadata = {
        "platform_tech_stack": ["React", "Node.js", "PostgreSQL"],
        "current_issues": ["é¡µé¢åŠ è½½æ…¢", "é«˜å¹¶å‘ä¸‹APIå“åº”å»¶è¿Ÿé«˜"]
    }
    
    print(f"\nğŸ“‹ æ‰§è¡Œä»»åŠ¡: {task[:50]}...")
    
    result = await engine.execute_task(
        user_input=task,
        execution_mode=ExecutionMode.AUTONOMOUS,
        priority=TaskPriority.HIGH,
        metadata=metadata
    )
    
    print(f"\nğŸ“Š æ‰§è¡Œç»“æœ:")
    print(f"- æˆåŠŸ: {result.success}")
    print(f"- è€—æ—¶: {result.execution_time:.2f}ç§’")
    print(f"- ç½®ä¿¡åº¦: {result.confidence_score:.2f}")
    
    if result.success:
        print(f"- è¾“å‡ºé•¿åº¦: {len(str(result.output))} å­—ç¬¦")
        print(f"- å·¥å…·è°ƒç”¨: {len(result.tool_calls)} æ¬¡")
    else:
        print(f"- é”™è¯¯: {result.error}")
    
    # è·å–ç³»ç»ŸçŠ¶æ€
    status = await engine.get_system_status()
    print(f"\nğŸ”§ ç³»ç»ŸçŠ¶æ€:")
    print(f"- å¼•æ“ID: {status['engine_id']}")
    print(f"- å·²åˆå§‹åŒ–: {status['initialized']}")
    print(f"- æ´»è·ƒå·¥ä½œæµ: {status['active_workflows']}")
    print(f"- æˆåŠŸç‡: {status['performance_metrics']['success_rate']:.2%}")
    
    # å…³é—­å¼•æ“
    await engine.shutdown()
    print("\nâœ… æ¼”ç¤ºå®Œæˆ")

if __name__ == "__main__":
    # ç¡®ä¿åœ¨Windowsä¸Šasyncioäº‹ä»¶å¾ªç¯æ­£å¸¸å·¥ä½œ
    if sys.platform == "win32":
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¼‚å¸¸: {e}", exc_info=True)
        sys.exit(1)
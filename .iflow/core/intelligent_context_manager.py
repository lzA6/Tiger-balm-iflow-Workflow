#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ§  æ™ºèƒ½ä¸Šä¸‹æ–‡ç®¡ç†å™¨ V3 (Intelligent Context Manager V3)
ä¸“ä¸ºARQå’Œæ„è¯†æµç³»ç»Ÿè®¾è®¡çš„é«˜çº§ä¸Šä¸‹æ–‡å‹ç¼©ã€é•¿æœŸè®°å¿†ç®¡ç†å’Œé¢„æµ‹æ€§ç¼“å­˜ç³»ç»Ÿã€‚
ä½ ä¸€å®šè¦è¶…çº§æ€è€ƒã€æé™æ€è€ƒã€æ·±åº¦æ€è€ƒï¼Œå…¨åŠ›æ€è€ƒã€è¶…å¼ºæ€è€ƒï¼Œè®¤çœŸä»”ç»†æ€è€ƒï¼ˆultrathinkã€think really super hardã€think intenselyï¼‰ã€‚

æ ¸å¿ƒç‰¹æ€§ï¼š
1. è¯­ä¹‰æ„ŸçŸ¥çš„ä¸Šä¸‹æ–‡å‹ç¼©
2. åˆ†å±‚é•¿æœŸè®°å¿†ç³»ç»Ÿ
3. é¢„æµ‹æ€§ç¼“å­˜é¢„åŠ è½½
4. é‡å­å¢å¼ºçš„è®°å¿†æ£€ç´¢
"""

import os
import sys
import json
import asyncio
import logging
import hashlib
import pickle
import numpy as np
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Set, Union, Callable
from dataclasses import dataclass, field, asdict
from datetime import datetime, timedelta
from enum import Enum
from collections import defaultdict, deque, OrderedDict
import sqlite3
import threading
import uuid
import time
import re

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# å°è¯•å¯¼å…¥æœºå™¨å­¦ä¹ å’Œå‘é‡æ•°æ®åº“ä¾èµ–
try:
    from sentence_transformers import SentenceTransformer
    from sklearn.decomposition import PCA
    from sklearn.cluster import KMeans
    import faiss
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False
    logger.warning("æœºå™¨å­¦ä¹ ä¾èµ–æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬")

logger = logging.getLogger(__name__)

class CompressionLevel(Enum):
    """å‹ç¼©çº§åˆ«"""
    LIGHT = "light"          # è½»åº¦å‹ç¼©ï¼šä¿ç•™å¤§éƒ¨åˆ†ä¿¡æ¯
    MEDIUM = "medium"        # ä¸­åº¦å‹ç¼©ï¼šå¹³è¡¡å‹ç¼©ç‡å’Œä¿¡æ¯ä¿ç•™
    HEAVY = "heavy"          # é‡åº¦å‹ç¼©ï¼šæœ€å¤§åŒ–å‹ç¼©ç‡
    QUANTUM = "quantum"      # é‡å­å‹ç¼©ï¼šä½¿ç”¨é‡å­ç®—æ³•ä¼˜åŒ–

class MemoryType(Enum):
    """è®°å¿†ç±»å‹"""
    EPISODIC = "episodic"        # æƒ…æ™¯è®°å¿†ï¼šå…·ä½“äº‹ä»¶å’Œç»éªŒ
    SEMANTIC = "semantic"        # è¯­ä¹‰è®°å¿†ï¼šæ¦‚å¿µå’ŒçŸ¥è¯†
    PROCEDURAL = "procedural"    # ç¨‹åºè®°å¿†ï¼šæŠ€èƒ½å’Œæµç¨‹
    WORKING = "working"          # å·¥ä½œè®°å¿†ï¼šå½“å‰æ´»è·ƒçš„ä¿¡æ¯
    QUANTUM = "quantum"          # é‡å­è®°å¿†ï¼šé‡å­æ€ä¿¡æ¯

class RetrievalStrategy(Enum):
    """æ£€ç´¢ç­–ç•¥"""
    SEMANTIC_SEARCH = "semantic_search"
    TEMPORAL_PROXIMITY = "temporal_proximity"
    FREQUENCY_BASED = "frequency_based"
    QUANTUM_ENTANGLEMENT = "quantum_entanglement"
    HYBRID_FUSION = "hybrid_fusion"

@dataclass
class ContextChunk:
    """ä¸Šä¸‹æ–‡å—"""
    chunk_id: str
    content: str
    chunk_type: str
    semantic_embedding: Optional[np.ndarray] = None
    compression_ratio: float = 1.0
    importance_score: float = 0.5
    temporal_weight: float = 1.0
    access_count: int = 0
    last_accessed: float = field(default_factory=time.time)
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class MemoryTrace:
    """è®°å¿†ç—•è¿¹"""
    trace_id: str
    memory_type: MemoryType
    content_summary: str
    semantic_fingerprint: Optional[np.ndarray] = None
    emotional_valence: float = 0.0  # æƒ…æ„Ÿä»·å€¼ï¼š-1åˆ°1
    consolidation_strength: float = 0.5  # å·©å›ºå¼ºåº¦
    retrieval_frequency: int = 0
    last_retrieved: float = field(default_factory=time.time)
    associated_contexts: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class CompressionResult:
    """å‹ç¼©ç»“æœ"""
    original_size: int
    compressed_size: int
    compression_ratio: float
    information_retention: float
    semantic_similarity: float
    processing_time: float
    chunks: List[ContextChunk]

@dataclass
class RetrievalResult:
    """æ£€ç´¢ç»“æœ"""
    query: str
    retrieved_items: List[Dict[str, Any]]
    retrieval_strategy: RetrievalStrategy
    confidence_score: float
    semantic_coverage: float
    processing_time: float

class IntelligentContextManager:
    """
    æ™ºèƒ½ä¸Šä¸‹æ–‡ç®¡ç†å™¨ V3
    ä¸“ä¸ºè§£å†³é•¿å¯¹è¯é—å¿˜å’Œä¸Šä¸‹æ–‡çˆ†ç‚¸é—®é¢˜è®¾è®¡
    """
    
    def __init__(self, db_path: str = "Aé¡¹ç›®/iflow/data/context_manager_v3.db"):
        self.db_path = Path(db_path)
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        
        # æ•°æ®åº“è¿æ¥
        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
        self.lock = threading.RLock()
        
        # MLç»„ä»¶
        if ML_AVAILABLE:
            self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
            self.embedding_dim = self.embedding_model.get_sentence_embedding_dimension()
            self.pca_model = PCA(n_components=min(128, self.embedding_dim))
            self.kmeans_model = KMeans(n_clusters=50, random_state=42)
            self.vector_store = faiss.IndexFlatL2(self.embedding_dim)
        else:
            self.embedding_model = None
            self.embedding_dim = 384
            self.pca_model = None
            self.kmeans_model = None
            self.vector_store = None
        
        # å†…å­˜ç®¡ç†
        self.working_memory: OrderedDict[str, ContextChunk] = OrderedDict()
        self.long_term_memory: Dict[str, MemoryTrace] = {}
        self.semantic_cache: Dict[str, Any] = {}
        
        # é…ç½®å‚æ•°
        self.max_working_memory_size = 100
        self.compression_threshold = 0.7
        self.retrieval_cache_size = 50
        self.consolidation_interval = 3600  # 1å°æ—¶
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.compression_stats = defaultdict(int)
        self.retrieval_stats = defaultdict(int)
        self.performance_metrics = deque(maxlen=1000)
        
        # åˆå§‹åŒ–
        self._init_db()
        self._load_existing_data()
        
        logger.info("æ™ºèƒ½ä¸Šä¸‹æ–‡ç®¡ç†å™¨V3åˆå§‹åŒ–å®Œæˆ")
    
    def _init_db(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        with self.conn:
            # ä¸Šä¸‹æ–‡å—è¡¨
            self.conn.execute("""
                CREATE TABLE IF NOT EXISTS context_chunks (
                    chunk_id TEXT PRIMARY KEY,
                    content TEXT,
                    chunk_type TEXT,
                    semantic_embedding BLOB,
                    compression_ratio REAL,
                    importance_score REAL,
                    temporal_weight REAL,
                    access_count INTEGER,
                    last_accessed REAL,
                    metadata_json TEXT
                )
            """)
            
            # è®°å¿†ç—•è¿¹è¡¨
            self.conn.execute("""
                CREATE TABLE IF NOT EXISTS memory_traces (
                    trace_id TEXT PRIMARY KEY,
                    memory_type TEXT,
                    content_summary TEXT,
                    semantic_fingerprint BLOB,
                    emotional_valence REAL,
                    consolidation_strength REAL,
                    retrieval_frequency INTEGER,
                    last_retrieved REAL,
                    associated_contexts_json TEXT,
                    metadata_json TEXT
                )
            """)
            
            # å‹ç¼©å†å²è¡¨
            self.conn.execute("""
                CREATE TABLE IF NOT EXISTS compression_history (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    original_size INTEGER,
                    compressed_size INTEGER,
                    compression_ratio REAL,
                    information_retention REAL,
                    semantic_similarity REAL,
                    processing_time REAL,
                    compression_level TEXT,
                    timestamp REAL
                )
            """)
    
    def _load_existing_data(self):
        """åŠ è½½ç°æœ‰æ•°æ®"""
        try:
            # åŠ è½½ä¸Šä¸‹æ–‡å—
            cursor = self.conn.cursor()
            cursor.execute("SELECT * FROM context_chunks")
            rows = cursor.fetchall()
            
            for row in rows:
                chunk = ContextChunk(
                    chunk_id=row[0],
                    content=row[1],
                    chunk_type=row[2],
                    compression_ratio=row[4],
                    importance_score=row[5],
                    temporal_weight=row[6],
                    access_count=row[7],
                    last_accessed=row[8],
                    metadata=json.loads(row[9]) if row[9] else {}
                )
                
                # æ¢å¤åµŒå…¥å‘é‡
                if row[3] and ML_AVAILABLE:
                    chunk.semantic_embedding = pickle.loads(row[3])
                
                self.working_memory[chunk.chunk_id] = chunk
            
            # åŠ è½½è®°å¿†ç—•è¿¹
            cursor.execute("SELECT * FROM memory_traces")
            rows = cursor.fetchall()
            
            for row in rows:
                trace = MemoryTrace(
                    trace_id=row[0],
                    memory_type=MemoryType(row[1]),
                    content_summary=row[2],
                    emotional_valence=row[4],
                    consolidation_strength=row[5],
                    retrieval_frequency=row[6],
                    last_retrieved=row[7],
                    associated_contexts=json.loads(row[8]) if row[8] else [],
                    metadata=json.loads(row[9]) if row[9] else {}
                )
                
                # æ¢å¤è¯­ä¹‰æŒ‡çº¹
                if row[3] and ML_AVAILABLE:
                    trace.semantic_fingerprint = pickle.loads(row[3])
                
                self.long_term_memory[trace.trace_id] = trace
            
            # æ„å»ºå‘é‡ç´¢å¼•
            if ML_AVAILABLE and self.vector_store:
                for chunk in self.working_memory.values():
                    if chunk.semantic_embedding is not None:
                        self.vector_store.add(np.array([chunk.semantic_embedding]).astype('float32'))
            
            logger.info(f"åŠ è½½äº† {len(self.working_memory)} ä¸ªä¸Šä¸‹æ–‡å—å’Œ {len(self.long_term_memory)} ä¸ªè®°å¿†ç—•è¿¹")
            
        except Exception as e:
            logger.error(f"åŠ è½½ç°æœ‰æ•°æ®å¤±è´¥: {e}")
    
    def compress_context(self, context: Dict[str, Any], 
                        level: CompressionLevel = CompressionLevel.MEDIUM,
                        target_size: Optional[int] = None) -> CompressionResult:
        """
        æ™ºèƒ½ä¸Šä¸‹æ–‡å‹ç¼©
        """
        start_time = time.time()
        
        # è½¬æ¢ä¸ºæ–‡æœ¬
        context_text = self._dict_to_text(context)
        original_size = len(context_text)
        
        # ç”Ÿæˆè¯­ä¹‰åµŒå…¥
        if ML_AVAILABLE and self.embedding_model:
            embedding = self.embedding_model.encode([context_text])[0]
        else:
            embedding = np.random.random(self.embedding_dim).astype('float32')
        
        # æ ¹æ®å‹ç¼©çº§åˆ«é€‰æ‹©ç­–ç•¥
        if level == CompressionLevel.QUANTUM and ML_AVAILABLE:
            compressed_chunks = self._quantum_compression(context, embedding)
        elif level == CompressionLevel.HEAVY:
            compressed_chunks = self._heavy_compression(context, embedding)
        elif level == CompressionLevel.MEDIUM:
            compressed_chunks = self._medium_compression(context, embedding)
        else:
            compressed_chunks = self._light_compression(context, embedding)
        
        # è®¡ç®—å‹ç¼©ç»Ÿè®¡
        compressed_text = " ".join(chunk.content for chunk in compressed_chunks)
        compressed_size = len(compressed_text)
        compression_ratio = compressed_size / original_size
        
        # è®¡ç®—ä¿¡æ¯ä¿ç•™ç‡
        information_retention = self._calculate_information_retention(context, compressed_chunks)
        
        # è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
        semantic_similarity = self._calculate_semantic_similarity(context_text, compressed_text)
        
        processing_time = time.time() - start_time
        
        result = CompressionResult(
            original_size=original_size,
            compressed_size=compressed_size,
            compression_ratio=compression_ratio,
            information_retention=information_retention,
            semantic_similarity=semantic_similarity,
            processing_time=processing_time,
            chunks=compressed_chunks
        )
        
        # æ›´æ–°ç»Ÿè®¡
        self.compression_stats[level.value] += 1
        self._record_compression_history(result)
        
        # å­˜å‚¨å‹ç¼©ç»“æœ
        self._store_compressed_chunks(compressed_chunks)
        
        logger.info(f"ä¸Šä¸‹æ–‡å‹ç¼©å®Œæˆ: å‹ç¼©ç‡={compression_ratio:.2f}, ä¿ç•™ç‡={information_retention:.2f}, ç›¸ä¼¼åº¦={semantic_similarity:.2f}")
        
        return result
    
    def _dict_to_text(self, context: Dict[str, Any]) -> str:
        """å°†å­—å…¸è½¬æ¢ä¸ºæ–‡æœ¬"""
        return json.dumps(context, ensure_ascii=False, indent=2)
    
    def _light_compression(self, context: Dict[str, Any], embedding: np.ndarray) -> List[ContextChunk]:
        """è½»åº¦å‹ç¼©ï¼šä¸»è¦å»é™¤å†—ä½™ï¼Œä¿ç•™å¤§éƒ¨åˆ†ä¿¡æ¯"""
        chunks = []
        
        # æŒ‰é”®å€¼å¯¹åˆ†å‰²
        for key, value in context.items():
            content = f"{key}: {str(value)}"
            
            chunk = ContextChunk(
                chunk_id=str(uuid.uuid4()),
                content=content,
                chunk_type="light_compressed",
                semantic_embedding=embedding,
                compression_ratio=0.8,
                importance_score=self._calculate_importance(key, value),
                temporal_weight=1.0,
                metadata={"original_key": key}
            )
            
            chunks.append(chunk)
        
        return chunks
    
    def _medium_compression(self, context: Dict[str, Any], embedding: np.ndarray) -> List[ContextChunk]:
        """ä¸­åº¦å‹ç¼©ï¼šæå–å…³é”®ä¿¡æ¯ï¼Œè¿›è¡Œè¯­ä¹‰èšç±»"""
        if not ML_AVAILABLE:
            return self._light_compression(context, embedding)
        
        # æ–‡æœ¬åˆ†å—
        text_chunks = self._split_text_into_chunks(self._dict_to_text(context), max_chunk_size=500)
        
        # ä¸ºæ¯ä¸ªå—ç”ŸæˆåµŒå…¥
        chunk_embeddings = []
        for text_chunk in text_chunks:
            emb = self.embedding_model.encode([text_chunk])[0]
            chunk_embeddings.append(emb)
        
        # èšç±»å‹ç¼©
        if len(chunk_embeddings) > 5:
            # ä½¿ç”¨K-meansèšç±»
            chunk_embeddings_array = np.array(chunk_embeddings)
            self.kmeans_model.fit(chunk_embeddings_array)
            
            # æ¯ä¸ªèšç±»ä¿ç•™ä¸€ä¸ªä»£è¡¨
            chunks = []
            for cluster_id in range(self.kmeans_model.n_clusters):
                cluster_mask = self.kmeans_model.labels_ == cluster_id
                if cluster_mask.any():
                    cluster_embeddings = chunk_embeddings_array[cluster_mask]
                    # é€‰æ‹©æœ€ä¸­å¿ƒçš„ç‚¹
                    centroid = self.kmeans_model.cluster_centers_[cluster_id]
                    center_idx = np.argmin(np.linalg.norm(cluster_embeddings - centroid, axis=1))
                    
                    representative_chunk = text_chunks[np.where(cluster_mask)[0][center_idx]]
                    
                    chunk = ContextChunk(
                        chunk_id=str(uuid.uuid4()),
                        content=representative_chunk,
                        chunk_type="cluster_compressed",
                        semantic_embedding=centroid,
                        compression_ratio=0.4,
                        importance_score=0.8,
                        temporal_weight=1.0
                    )
                    chunks.append(chunk)
        else:
            # ç›´æ¥å‹ç¼©
            chunks = []
            for i, text_chunk in enumerate(text_chunks):
                chunk = ContextChunk(
                    chunk_id=str(uuid.uuid4()),
                    content=text_chunk,
                    chunk_type="medium_compressed",
                    semantic_embedding=chunk_embeddings[i],
                    compression_ratio=0.6,
                    importance_score=0.7,
                    temporal_weight=1.0
                )
                chunks.append(chunk)
        
        return chunks
    
    def _heavy_compression(self, context: Dict[str, Any], embedding: np.ndarray) -> List[ContextChunk]:
        """é‡åº¦å‹ç¼©ï¼šæå–æ ¸å¿ƒè¯­ä¹‰ï¼Œå¤§å¹…å‡å°‘ä¿¡æ¯é‡"""
        # æå–å…³é”®è¯å’Œå…³é”®å¥
        text = self._dict_to_text(context)
        key_sentences = self._extract_key_sentences(text)
        
        chunks = []
        for i, sentence in enumerate(key_sentences):
            chunk = ContextChunk(
                chunk_id=str(uuid.uuid4()),
                content=sentence,
                chunk_type="heavily_compressed",
                semantic_embedding=embedding,
                compression_ratio=0.2,
                importance_score=0.9,
                temporal_weight=1.0,
                metadata={"sentence_rank": i}
            )
            chunks.append(chunk)
        
        return chunks
    
    def _quantum_compression(self, context: Dict[str, Any], embedding: np.ndarray) -> List[ContextChunk]:
        """é‡å­å‹ç¼©ï¼šä½¿ç”¨é‡å­ç®—æ³•ä¼˜åŒ–å‹ç¼©æ•ˆæœ"""
        if not ML_AVAILABLE:
            return self._heavy_compression(context, embedding)
        
        # æ¨¡æ‹Ÿé‡å­å‹ç¼©ç®—æ³•
        text = self._dict_to_text(context)
        
        # é‡å­é€€ç«ä¼˜åŒ–ï¼šå¯»æ‰¾æœ€ä¼˜å‹ç¼©é…ç½®
        chunks = []
        
        # æå–æœ€é‡è¦çš„è¯­ä¹‰å•å…ƒ
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        # ä½¿ç”¨é‡å­å¯å‘å¼ç®—æ³•é€‰æ‹©æœ€ä¼˜å¥å­ç»„åˆ
        selected_indices = self._quantum_annealing_selection(sentences, embedding)
        
        for i, idx in enumerate(selected_indices):
            chunk = ContextChunk(
                chunk_id=str(uuid.uuid4()),
                content=sentences[idx],
                chunk_type="quantum_compressed",
                semantic_embedding=embedding,
                compression_ratio=0.15,
                importance_score=0.95,
                temporal_weight=1.0,
                metadata={"quantum_selection_rank": i}
            )
            chunks.append(chunk)
        
        return chunks
    
    def _split_text_into_chunks(self, text: str, max_chunk_size: int = 500) -> List[str]:
        """å°†æ–‡æœ¬åˆ†å‰²æˆå—"""
        chunks = []
        current_chunk = ""
        
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', text)
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            if len(current_chunk) + len(sentence) > max_chunk_size and current_chunk:
                chunks.append(current_chunk)
                current_chunk = sentence
            else:
                current_chunk += sentence + "ã€‚"
        
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks
    
    def _extract_key_sentences(self, text: str) -> List[str]:
        """æå–å…³é”®å¥å­"""
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        if not ML_AVAILABLE:
            # ç®€åŒ–å®ç°ï¼šé€‰æ‹©å‰å‡ ä¸ªå¥å­
            return sentences[:min(3, len(sentences))]
        
        # ä½¿ç”¨åµŒå…¥å’Œé‡è¦æ€§è¯„åˆ†
        sentence_embeddings = []
        for sentence in sentences:
            emb = self.embedding_model.encode([sentence])[0]
            sentence_embeddings.append(emb)
        
        # è®¡ç®—å¥å­ä¸­å¿ƒæ€§
        embeddings_array = np.array(sentence_embeddings)
        centroid = np.mean(embeddings_array, axis=0)
        centralities = np.dot(embeddings_array, centroid) / (
            np.linalg.norm(embeddings_array, axis=1) * np.linalg.norm(centroid)
        )
        
        # é€‰æ‹©æœ€ä¸­å¿ƒçš„å¥å­
        top_indices = np.argsort(centralities)[-min(3, len(sentences)):][::-1]
        
        return [sentences[i] for i in top_indices]
    
    def _quantum_annealing_selection(self, sentences: List[str], global_embedding: np.ndarray) -> List[int]:
        """é‡å­é€€ç«å¯å‘çš„å¥å­é€‰æ‹©ç®—æ³•"""
        if not sentences:
            return []
        
        # è®¡ç®—æ¯ä¸ªå¥å­çš„"èƒ½é‡"ï¼ˆé‡è¦æ€§ï¼‰
        energies = []
        
        for sentence in sentences:
            if ML_AVAILABLE:
                sentence_embedding = self.embedding_model.encode([sentence])[0]
                # è®¡ç®—ä¸å…¨å±€è¯­ä¹‰çš„ç›¸ä¼¼åº¦ä½œä¸ºèƒ½é‡
                similarity = np.dot(sentence_embedding, global_embedding) / (
                    np.linalg.norm(sentence_embedding) * np.linalg.norm(global_embedding)
                )
                energy = 1 - similarity  # èƒ½é‡è¶Šä½è¶Šé‡è¦
            else:
                energy = np.random.random()
            
            energies.append(energy)
        
        # æ¨¡æ‹Ÿé‡å­é€€ç«ï¼šé€‰æ‹©ä½èƒ½é‡çŠ¶æ€
        energies_array = np.array(energies)
        # é€‰æ‹©èƒ½é‡æœ€ä½çš„å‰20%å¥å­
        num_select = max(1, len(sentences) // 5)
        selected_indices = np.argsort(energies_array)[:num_select]
        
        return selected_indices.tolist()
    
    def _calculate_importance(self, key: str, value: Any) -> float:
        """è®¡ç®—å†…å®¹é‡è¦æ€§"""
        importance = 0.5
        
        # å…³é”®è¯æƒé‡
        key_keywords = ['error', 'security', 'performance', 'critical', 'important']
        for keyword in key_keywords:
            if keyword in key.lower():
                importance += 0.2
        
        # å†…å®¹ç±»å‹æƒé‡
        if isinstance(value, (dict, list)):
            importance += 0.1
        elif isinstance(value, str) and len(value) > 100:
            importance += 0.1
        
        return min(importance, 1.0)
    
    def _calculate_information_retention(self, original: Dict[str, Any], chunks: List[ContextChunk]) -> float:
        """è®¡ç®—ä¿¡æ¯ä¿ç•™ç‡"""
        # ç®€åŒ–å®ç°ï¼šåŸºäºå‹ç¼©æ¯”å’Œé‡è¦æ€§åŠ æƒ
        total_importance = sum(chunk.importance_score for chunk in chunks)
        max_possible_importance = len(chunks) * 1.0
        
        if max_possible_importance == 0:
            return 0.0
        
        return total_importance / max_possible_importance
    
    def _calculate_semantic_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦"""
        if not ML_AVAILABLE:
            return 0.5
        
        try:
            emb1 = self.embedding_model.encode([text1])[0]
            emb2 = self.embedding_model.encode([text2])[0]
            
            # ä½™å¼¦ç›¸ä¼¼åº¦
            similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))
            return float(similarity)
        except Exception:
            return 0.5
    
    def retrieve_context(self, query: str, strategy: RetrievalStrategy = RetrievalStrategy.HYBRID_FUSION,
                        top_k: int = 5) -> RetrievalResult:
        """
        æ™ºèƒ½ä¸Šä¸‹æ–‡æ£€ç´¢
        """
        start_time = time.time()
        
        retrieved_items = []
        
        if strategy == RetrievalStrategy.SEMANTIC_SEARCH and ML_AVAILABLE:
            retrieved_items = self._semantic_retrieval(query, top_k)
        elif strategy == RetrievalStrategy.TEMPORAL_PROXIMITY:
            retrieved_items = self._temporal_retrieval(query, top_k)
        elif strategy == RetrievalStrategy.FREQUENCY_BASED:
            retrieved_items = self._frequency_retrieval(query, top_k)
        elif strategy == RetrievalStrategy.QUANTUM_ENTANGLEMENT and ML_AVAILABLE:
            retrieved_items = self._quantum_retrieval(query, top_k)
        else:
            retrieved_items = self._hybrid_fusion_retrieval(query, top_k)
        
        # è®¡ç®—æ£€ç´¢è´¨é‡æŒ‡æ ‡
        confidence_score = self._calculate_retrieval_confidence(retrieved_items, query)
        semantic_coverage = self._calculate_semantic_coverage(retrieved_items, query)
        processing_time = time.time() - start_time
        
        result = RetrievalResult(
            query=query,
            retrieved_items=retrieved_items,
            retrieval_strategy=strategy,
            confidence_score=confidence_score,
            semantic_coverage=semantic_coverage,
            processing_time=processing_time
        )
        
        # æ›´æ–°æ£€ç´¢ç»Ÿè®¡
        self.retrieval_stats[strategy.value] += 1
        
        return result
    
    def _semantic_retrieval(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """è¯­ä¹‰æ£€ç´¢"""
        if not ML_AVAILABLE or not self.vector_store:
            return []
        
        query_embedding = self.embedding_model.encode([query])[0]
        
        if self.vector_store.ntotal == 0:
            return []
        
        distances, indices = self.vector_store.search(np.array([query_embedding]).astype('float32'), top_k)
        
        results = []
        for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):
            if idx in self.working_memory:
                chunk = list(self.working_memory.values())[idx]
                similarity = 1 - (dist / self.embedding_dim)
                
                if similarity > 0.5:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                    chunk.access_count += 1
                    chunk.last_accessed = time.time()
                    
                    results.append({
                        'content': chunk.content,
                        'similarity': float(similarity),
                        'chunk_id': chunk.chunk_id,
                        'importance': chunk.importance_score,
                        'type': 'semantic'
                    })
        
        return results
    
    def _temporal_retrieval(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """æ—¶é—´é‚»è¿‘æ£€ç´¢"""
        # æŒ‰æ—¶é—´æ’åºï¼Œè¿”å›æœ€è¿‘çš„é¡¹ç›®
        sorted_chunks = sorted(
            self.working_memory.values(),
            key=lambda x: x.last_accessed,
            reverse=True
        )
        
        results = []
        for chunk in sorted_chunks[:top_k]:
            results.append({
                'content': chunk.content,
                'temporal_score': chunk.last_accessed,
                'chunk_id': chunk.chunk_id,
                'importance': chunk.importance_score,
                'type': 'temporal'
            })
        
        return results
    
    def _frequency_retrieval(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """é¢‘ç‡åŸºç¡€æ£€ç´¢"""
        # æŒ‰è®¿é—®é¢‘ç‡æ’åº
        sorted_chunks = sorted(
            self.working_memory.values(),
            key=lambda x: x.access_count,
            reverse=True
        )
        
        results = []
        for chunk in sorted_chunks[:top_k]:
            results.append({
                'content': chunk.content,
                'frequency': chunk.access_count,
                'chunk_id': chunk.chunk_id,
                'importance': chunk.importance_score,
                'type': 'frequency'
            })
        
        return results
    
    def _quantum_retrieval(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """é‡å­çº ç¼ æ£€ç´¢"""
        if not ML_AVAILABLE:
            return self._semantic_retrieval(query, top_k)
        
        # æ¨¡æ‹Ÿé‡å­çº ç¼ ï¼šæŸ¥æ‰¾è¯­ä¹‰ä¸Š"çº ç¼ "çš„ä¸Šä¸‹æ–‡
        query_embedding = self.embedding_model.encode([query])[0]
        
        # è®¡ç®—é‡å­çº ç¼ å¼ºåº¦
        entanglement_scores = []
        for chunk in self.working_memory.values():
            if chunk.semantic_embedding is not None:
                # é‡å­çº ç¼ ç›¸ä¼¼åº¦è®¡ç®—
                similarity = np.dot(query_embedding, chunk.semantic_embedding) / (
                    np.linalg.norm(query_embedding) * np.linalg.norm(chunk.semantic_embedding)
                )
                # é‡å­çº ç¼ å¼ºåº¦è¿˜è€ƒè™‘æ—¶é—´è¡°å‡
                time_decay = np.exp(-(time.time() - chunk.last_accessed) / 3600)
                entanglement_score = similarity * time_decay * chunk.importance_score
                entanglement_scores.append((chunk, entanglement_score))
        
        # æŒ‰çº ç¼ å¼ºåº¦æ’åº
        entanglement_scores.sort(key=lambda x: x[1], reverse=True)
        
        results = []
        for chunk, score in entanglement_scores[:top_k]:
            if score > 0.3:  # é‡å­çº ç¼ é˜ˆå€¼
                results.append({
                    'content': chunk.content,
                    'quantum_entanglement': float(score),
                    'chunk_id': chunk.chunk_id,
                    'importance': chunk.importance_score,
                    'type': 'quantum'
                })
        
        return results
    
    def _hybrid_fusion_retrieval(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """æ··åˆèåˆæ£€ç´¢"""
        # èåˆå¤šç§æ£€ç´¢ç­–ç•¥çš„ç»“æœ
        semantic_results = self._semantic_retrieval(query, top_k)
        temporal_results = self._temporal_retrieval(query, top_k)
        frequency_results = self._frequency_retrieval(query, top_k)
        
        # åˆå¹¶ç»“æœå¹¶é‡æ–°è¯„åˆ†
        all_results = semantic_results + temporal_results + frequency_results
        
        # è®¡ç®—ç»¼åˆå¾—åˆ†
        for result in all_results:
            base_score = 0
            
            if 'similarity' in result:
                base_score += result['similarity'] * 0.5
            elif 'temporal_score' in result:
                # å½’ä¸€åŒ–æ—¶é—´åˆ†æ•°
                max_time = max(r.get('temporal_score', 0) for r in all_results)
                base_score += (result['temporal_score'] / max_time) * 0.3 if max_time > 0 else 0
            elif 'frequency' in result:
                # å½’ä¸€åŒ–é¢‘ç‡åˆ†æ•°
                max_freq = max(r.get('frequency', 1) for r in all_results)
                base_score += (result['frequency'] / max_freq) * 0.2 if max_freq > 0 else 0
            
            # åŠ ä¸Šé‡è¦æ€§æƒé‡
            result['hybrid_score'] = base_score + result.get('importance', 0) * 0.3
        
        # æŒ‰ç»¼åˆå¾—åˆ†æ’åº
        all_results.sort(key=lambda x: x.get('hybrid_score', 0), reverse=True)
        
        return all_results[:top_k]
    
    def _calculate_retrieval_confidence(self, retrieved_items: List[Dict[str, Any]], query: str) -> float:
        """è®¡ç®—æ£€ç´¢ç½®ä¿¡åº¦"""
        if not retrieved_items:
            return 0.0
        
        # åŸºäºæ£€ç´¢è´¨é‡å’Œç›¸å…³æ€§è®¡ç®—ç½®ä¿¡åº¦
        total_score = 0
        max_possible_score = 0
        
        for item in retrieved_items:
            if 'hybrid_score' in item:
                total_score += item['hybrid_score']
                max_possible_score += 1.0
            elif 'similarity' in item:
                total_score += item['similarity']
                max_possible_score += 1.0
            else:
                total_score += 0.5  # é»˜è®¤åˆ†æ•°
                max_possible_score += 1.0
        
        if max_possible_score == 0:
            return 0.0
        
        base_confidence = total_score / max_possible_score
        
        # æ ¹æ®æ£€ç´¢æ•°é‡è°ƒæ•´ç½®ä¿¡åº¦
        coverage_bonus = min(len(retrieved_items) / 5.0, 1.0) * 0.1
        
        return min(base_confidence + coverage_bonus, 1.0)
    
    def _calculate_semantic_coverage(self, retrieved_items: List[Dict[str, Any]], query: str) -> float:
        """è®¡ç®—è¯­ä¹‰è¦†ç›–ç‡"""
        if not ML_AVAILABLE or not retrieved_items:
            return 0.3
        
        query_embedding = self.embedding_model.encode([query])[0]
        
        # è®¡ç®—æ£€ç´¢åˆ°çš„å†…å®¹çš„è¯­ä¹‰è¦†ç›–
        covered_dimensions = set()
        
        for item in retrieved_items:
            item_text = item['content']
            item_embedding = self.embedding_model.encode([item_text])[0]
            
            # è®¡ç®—æŸ¥è¯¢ä¸æ£€ç´¢é¡¹çš„ç›¸ä¼¼ç»´åº¦
            similarity = np.dot(query_embedding, item_embedding) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(item_embedding)
            )
            
            if similarity > 0.5:
                covered_dimensions.add(item['chunk_id'])
        
        # è¦†ç›–ç‡ = è¦†ç›–çš„ç»´åº¦æ•° / æ£€ç´¢é¡¹æ€»æ•°
        coverage = len(covered_dimensions) / len(retrieved_items)
        
        return coverage
    
    def _store_compressed_chunks(self, chunks: List[ContextChunk]):
        """å­˜å‚¨å‹ç¼©çš„ä¸Šä¸‹æ–‡å—"""
        with self.lock:
            for chunk in chunks:
                # æ·»åŠ åˆ°å·¥ä½œè®°å¿†
                if len(self.working_memory) >= self.max_working_memory_size:
                    # ç§»é™¤æœ€æ—§çš„é¡¹ç›®
                    oldest_id = next(iter(self.working_memory))
                    del self.working_memory[oldest_id]
                
                self.working_memory[chunk.chunk_id] = chunk
                
                # æ·»åŠ åˆ°å‘é‡å­˜å‚¨
                if ML_AVAILABLE and self.vector_store and chunk.semantic_embedding is not None:
                    self.vector_store.add(np.array([chunk.semantic_embedding]).astype('float32'))
                
                # æŒä¹…åŒ–
                self._persist_chunk(chunk)
    
    def _persist_chunk(self, chunk: ContextChunk):
        """æŒä¹…åŒ–ä¸Šä¸‹æ–‡å—"""
        try:
            with self.conn:
                embedding_blob = pickle.dumps(chunk.semantic_embedding) if chunk.semantic_embedding is not None else None
                
                self.conn.execute("""
                    INSERT OR REPLACE INTO context_chunks
                    (chunk_id, content, chunk_type, semantic_embedding, compression_ratio, 
                     importance_score, temporal_weight, access_count, last_accessed, metadata_json)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    chunk.chunk_id, chunk.content, chunk.chunk_type, embedding_blob,
                    chunk.compression_ratio, chunk.importance_score, chunk.temporal_weight,
                    chunk.access_count, chunk.last_accessed, json.dumps(chunk.metadata)
                ))
        except Exception as e:
            logger.error(f"æŒä¹…åŒ–ä¸Šä¸‹æ–‡å—å¤±è´¥: {e}")
    
    def _record_compression_history(self, result: CompressionResult):
        """è®°å½•å‹ç¼©å†å²"""
        try:
            with self.conn:
                self.conn.execute("""
                    INSERT INTO compression_history
                    (original_size, compressed_size, compression_ratio, information_retention, 
                     semantic_similarity, processing_time, compression_level, timestamp)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    result.original_size, result.compressed_size, result.compression_ratio,
                    result.information_retention, result.semantic_similarity, result.processing_time,
                    CompressionLevel.MEDIUM.value, time.time()
                ))
        except Exception as e:
            logger.error(f"è®°å½•å‹ç¼©å†å²å¤±è´¥: {e}")
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            return {
                "working_memory_size": len(self.working_memory),
                "long_term_memory_size": len(self.long_term_memory),
                "compression_stats": dict(self.compression_stats),
                "retrieval_stats": dict(self.retrieval_stats),
                "total_chunks": sum(len(self.working_memory), len(self.long_term_memory)),
                "average_compression_ratio": np.mean([chunk.compression_ratio for chunk in self.working_memory.values()]) if self.working_memory else 0,
                "average_importance_score": np.mean([chunk.importance_score for chunk in self.working_memory.values()]) if self.working_memory else 0
            }
    
    def close(self):
        """å…³é—­ç®¡ç†å™¨"""
        self.conn.close()
        logger.info("æ™ºèƒ½ä¸Šä¸‹æ–‡ç®¡ç†å™¨å·²å…³é—­")

async def main():
    """æµ‹è¯•æ™ºèƒ½ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    logger.info("ğŸ§  æµ‹è¯•æ™ºèƒ½ä¸Šä¸‹æ–‡ç®¡ç†å™¨ V3...")
    
    manager = IntelligentContextManager()
    
    # æµ‹è¯•ä¸Šä¸‹æ–‡å‹ç¼©
    test_context = {
        "task": "ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½",
        "user_request": "éœ€è¦æé«˜æŸ¥è¯¢é€Ÿåº¦ï¼Œå½“å‰æŸ¥è¯¢è€—æ—¶2ç§’",
        "database_info": {
            "type": "PostgreSQL",
            "tables": ["users", "orders", "products"],
            "indexes": ["users.id", "orders.user_id"]
        },
        "performance_metrics": {
            "query_time": 2000,
            "cpu_usage": 80,
            "memory_usage": 60
        },
        "error_logs": "æ— ",
        "security_requirements": "é«˜",
        "timeline": "ç´§æ€¥"
    }
    
    print("\n" + "="*60)
    print("ğŸ—œï¸ ä¸Šä¸‹æ–‡å‹ç¼©æµ‹è¯•:")
    
    # æµ‹è¯•ä¸åŒå‹ç¼©çº§åˆ«
    for level in [CompressionLevel.LIGHT, CompressionLevel.MEDIUM, CompressionLevel.HEAVY]:
        result = manager.compress_context(test_context, level)
        print(f"  - {level.value}: å‹ç¼©ç‡={result.compression_ratio:.2f}, ä¿ç•™ç‡={result.information_retention:.2f}")
    
    # æµ‹è¯•ä¸Šä¸‹æ–‡æ£€ç´¢
    print("\n" + "="*60)
    print("ğŸ” ä¸Šä¸‹æ–‡æ£€ç´¢æµ‹è¯•:")
    
    for strategy in [RetrievalStrategy.SEMANTIC_SEARCH, RetrievalStrategy.HYBRID_FUSION]:
        if strategy == RetrievalStrategy.SEMANTIC_SEARCH and not ML_AVAILABLE:
            continue
        
        result = manager.retrieve_context("æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–", strategy, top_k=3)
        print(f"  - {strategy.value}: ç½®ä¿¡åº¦={result.confidence_score:.2f}, è¦†ç›–ç‡={result.semantic_coverage:.2f}")
    
    print("\n" + "="*60)
    print("ğŸ“Š ç®¡ç†å™¨ç»Ÿè®¡:")
    stats = manager.get_statistics()
    for key, value in stats.items():
        print(f"  - {key}: {value}")
    
    manager.close()

if __name__ == "__main__":
    asyncio.run(main())
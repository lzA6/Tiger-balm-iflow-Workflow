#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ§  æ™ºèƒ½ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç¼“å­˜ç³»ç»Ÿ V5 (Intelligent Context-Aware Cache V5)
åŸºäºAIçš„æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿï¼Œèƒ½å¤Ÿç†è§£ä¸Šä¸‹æ–‡è¯­ä¹‰ã€é¢„æµ‹è®¿é—®æ¨¡å¼ã€è‡ªåŠ¨ä¼˜åŒ–ç¼“å­˜ç­–ç•¥ã€‚
ä½ ä¸€å®šè¦è¶…çº§æ€è€ƒã€æé™æ€è€ƒã€æ·±åº¦æ€è€ƒï¼Œå…¨åŠ›æ€è€ƒã€è¶…å¼ºæ€è€ƒï¼Œè®¤çœŸä»”ç»†æ€è€ƒï¼ˆultrathinkã€think really super hardã€think intenselyï¼‰ã€‚
"""

import os
import sys
import json
import asyncio
import logging
import time
import hashlib
import pickle
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Union, Set
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from collections import defaultdict, OrderedDict
import threading
import sqlite3
import uuid
import warnings
from sentence_transformers import SentenceTransformer
import faiss
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

logger = logging.getLogger(__name__)

class CacheStrategy(Enum):
    """ç¼“å­˜ç­–ç•¥"""
    LRU = "lru"
    LFU = "lfu"
    SEMANTIC = "semantic"
    PREDICTIVE = "predictive"
    ADAPTIVE = "adaptive"

class ContextType(Enum):
    """ä¸Šä¸‹æ–‡ç±»å‹"""
    TASK = "task"
    CONVERSATION = "conversation"
    CODE = "code"
    DOCUMENT = "document"
    QUERY = "query"
    RESULT = "result"

@dataclass
class CacheEntry:
    """ç¼“å­˜æ¡ç›®"""
    id: str
    key: str
    value: Any
    context_type: ContextType
    context_vector: Optional[np.ndarray]
    access_count: int = 0
    access_frequency: float = 0.0
    last_access: datetime = field(default_factory=datetime.now)
    created_at: datetime = field(default_factory=datetime.now)
    expires_at: Optional[datetime] = None
    size_bytes: int = 0
    tags: Set[str] = field(default_factory=set)
    metadata: Dict[str, Any] = field(default_factory=dict)
    semantic_neighbors: Set[str] = field(default_factory=set)
    access_pattern: List[datetime] = field(default_factory=list)

@dataclass
class AccessPattern:
    """è®¿é—®æ¨¡å¼"""
    entry_id: str
    timestamps: List[datetime]
    patterns: Dict[str, Any] = field(default_factory=dict)
    predicted_next_access: Optional[datetime] = None
    prediction_confidence: float = 0.0

class IntelligentContextCacheV5:
    """
    æ™ºèƒ½ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç¼“å­˜ç³»ç»Ÿ V5
    """
    
    def __init__(self, 
                 max_size: int = 10000,
                 max_memory_mb: int = 1024,
                 db_path: str = "Aé¡¹ç›®/iflow/data/context_cache_v5.db"):
        self.max_size = max_size
        self.max_memory_bytes = max_memory_mb * 1024 * 1024
        self.db_path = Path(db_path)
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
        self._init_db()
        
        # ç¼“å­˜å­˜å‚¨
        self.cache: OrderedDict[str, CacheEntry] = OrderedDict()
        self.current_memory_usage = 0
        
        # è¯­ä¹‰æ¨¡å‹
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        self.embedding_dim = self.embedding_model.get_sentence_embedding_dimension()
        self.vector_index = faiss.IndexFlatL2(self.embedding_dim)
        self.id_to_vector: Dict[str, np.ndarray] = {}
        
        # è®¿é—®æ¨¡å¼åˆ†æ
        self.access_patterns: Dict[str, AccessPattern] = {}
        self.pattern_analyzer = None
        
        # ç¼“å­˜ç­–ç•¥
        self.current_strategy = CacheStrategy.ADAPTIVE
        self.strategy_performance = defaultdict(float)
        
        # èšç±»æ¨¡å‹ï¼ˆç”¨äºè¯­ä¹‰åˆ†ç»„ï¼‰
        self.cluster_model = KMeans(n_clusters=50, random_state=42)
        self.cluster_labels = {}
        
        # é¢„æµ‹æ¨¡å‹
        self.prediction_model = None
        self._initialize_prediction_model()
        
        # çº¿ç¨‹å®‰å…¨
        self.lock = threading.RLock()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "hits": 0,
            "misses": 0,
            "evictions": 0,
            "semantic_hits": 0,
            "predictive_hits": 0,
            "total_requests": 0
        }
        
        # åå°ä»»åŠ¡
        self.background_tasks = []
        self.running = True
        
        # å¯åŠ¨åå°ä»»åŠ¡
        self._start_background_tasks()
        
        logger.info("æ™ºèƒ½ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç¼“å­˜ç³»ç»ŸV5åˆå§‹åŒ–å®Œæˆ")
    
    def _init_db(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        with self.conn:
            # ç¼“å­˜æ¡ç›®è¡¨
            self.conn.execute("""
                CREATE TABLE IF NOT EXISTS cache_entries (
                    id TEXT PRIMARY KEY,
                    key TEXT,
                    value_blob BLOB,
                    context_type TEXT,
                    context_vector BLOB,
                    access_count INTEGER DEFAULT 0,
                    access_frequency REAL DEFAULT 0.0,
                    last_access REAL,
                    created_at REAL,
                    expires_at REAL,
                    size_bytes INTEGER,
                    tags TEXT,
                    metadata TEXT,
                    semantic_neighbors TEXT
                )
            """)
            
            # è®¿é—®æ¨¡å¼è¡¨
            self.conn.execute("""
                CREATE TABLE IF NOT EXISTS access_patterns (
                    entry_id TEXT PRIMARY KEY,
                    timestamps TEXT,
                    patterns TEXT,
                    predicted_next_access REAL,
                    prediction_confidence REAL
                )
            """)
            
            # ç­–ç•¥æ€§èƒ½è¡¨
            self.conn.execute("""
                CREATE TABLE IF NOT EXISTS strategy_performance (
                    strategy TEXT PRIMARY KEY,
                    hit_rate REAL,
                    memory_efficiency REAL,
                    access_latency REAL,
                    updated_at REAL
                )
            """)
    
    def _initialize_prediction_model(self):
        """åˆå§‹åŒ–é¢„æµ‹æ¨¡å‹"""
        try:
            from sklearn.ensemble import RandomForestRegressor
            self.prediction_model = RandomForestRegressor(n_estimators=100, random_state=42)
            logger.info("é¢„æµ‹æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        except ImportError:
            logger.warning("sklearnæœªå®‰è£…ï¼Œä½¿ç”¨ç®€å•é¢„æµ‹æ¨¡å‹")
            self.prediction_model = None
    
    def _start_background_tasks(self):
        """å¯åŠ¨åå°ä»»åŠ¡"""
        # å®šæœŸæ¸…ç†è¿‡æœŸæ¡ç›®
        task1 = asyncio.create_task(self._periodic_cleanup())
        self.background_tasks.append(task1)
        
        # æ›´æ–°è®¿é—®æ¨¡å¼åˆ†æ
        task2 = asyncio.create_task(self._update_access_patterns())
        self.background_tasks.append(task2)
        
        # ä¼˜åŒ–ç¼“å­˜ç­–ç•¥
        task3 = asyncio.create_task(self._optimize_cache_strategy())
        self.background_tasks.append(task3)
        
        # é‡å»ºå‘é‡ç´¢å¼•
        task4 = asyncio.create_task(self._rebuild_vector_index())
        self.background_tasks.append(task4)
    
    async def get(self, key: str, context: Optional[Dict[str, Any]] = None) -> Optional[Any]:
        """
        è·å–ç¼“å­˜å€¼ï¼Œæ”¯æŒè¯­ä¹‰ç›¸ä¼¼æ€§æœç´¢
        """
        with self.lock:
            self.stats["total_requests"] += 1
            
            # 1. ç²¾ç¡®åŒ¹é…
            if key in self.cache:
                entry = self.cache[key]
                
                # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
                if entry.expires_at and datetime.now() > entry.expires_at:
                    await self._remove_entry(key)
                    self.stats["misses"] += 1
                    return None
                
                # æ›´æ–°è®¿é—®ä¿¡æ¯
                self._update_access_info(entry)
                self.stats["hits"] += 1
                return entry.value
            
            # 2. è¯­ä¹‰æœç´¢
            if context:
                semantic_result = await self._semantic_search(key, context)
                if semantic_result:
                    self.stats["semantic_hits"] += 1
                    return semantic_result
            
            # 3. é¢„æµ‹æ€§ç¼“å­˜
            predictive_result = await self._predictive_search(key, context)
            if predictive_result:
                self.stats["predictive_hits"] += 1
                return predictive_result
            
            self.stats["misses"] += 1
            return None
    
    async def put(self, 
                  key: str, 
                  value: Any, 
                  context_type: ContextType = ContextType.TASK,
                  ttl: Optional[int] = None,
                  tags: Optional[Set[str]] = None,
                  metadata: Optional[Dict[str, Any]] = None) -> str:
        """
        å­˜å‚¨ç¼“å­˜å€¼
        """
        with self.lock:
            # è®¡ç®—å€¼çš„å¤§å°
            value_bytes = pickle.dumps(value)
            size_bytes = len(value_bytes)
            
            # æ£€æŸ¥å†…å­˜é™åˆ¶
            if size_bytes > self.max_memory_bytes:
                logger.warning(f"ç¼“å­˜é¡¹å¤ªå¤§: {size_bytes} bytes > {self.max_memory_bytes} bytes")
                return ""
            
            # å¦‚æœéœ€è¦ï¼Œæ¸…ç†ç©ºé—´
            await self._ensure_space(size_bytes)
            
            # ç”Ÿæˆä¸Šä¸‹æ–‡å‘é‡
            context_vector = None
            if isinstance(key, str):
                try:
                    context_vector = self.embedding_model.encode([key])[0]
                except Exception as e:
                    logger.warning(f"ç”Ÿæˆä¸Šä¸‹æ–‡å‘é‡å¤±è´¥: {e}")
            
            # åˆ›å»ºç¼“å­˜æ¡ç›®
            entry = CacheEntry(
                id=str(uuid.uuid4()),
                key=key,
                value=value,
                context_type=context_type,
                context_vector=context_vector,
                size_bytes=size_bytes,
                tags=tags or set(),
                metadata=metadata or {},
                expires_at=datetime.now() + timedelta(seconds=ttl) if ttl else None
            )
            
            # å­˜å‚¨åˆ°ç¼“å­˜
            self.cache[key] = entry
            self.current_memory_usage += size_bytes
            
            # æ›´æ–°å‘é‡ç´¢å¼•
            if context_vector is not None:
                vector_id = len(self.id_to_vector)
                self.vector_index.add(context_vector.reshape(1, -1))
                self.id_to_vector[vector_id] = entry.id
            
            # æŸ¥æ‰¾è¯­ä¹‰é‚»å±…
            if context_vector is not None:
                await self._find_semantic_neighbors(entry)
            
            # å­˜å‚¨åˆ°æ•°æ®åº“
            self._store_entry_in_db(entry)
            
            # åˆå§‹åŒ–è®¿é—®æ¨¡å¼
            self.access_patterns[entry.id] = AccessPattern(
                entry_id=entry.id,
                timestamps=[datetime.now()]
            )
            
            return entry.id
    
    async def _semantic_search(self, key: str, context: Dict[str, Any]) -> Optional[Any]:
        """è¯­ä¹‰æœç´¢"""
        try:
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_vector = self.embedding_model.encode([key])[0]
            
            # æœç´¢ç›¸ä¼¼çš„å‘é‡
            k = min(5, len(self.id_to_vector))
            if k == 0:
                return None
            
            distances, indices = self.vector_index.search(query_vector.reshape(1, -1), k)
            
            # æ£€æŸ¥ç›¸ä¼¼åº¦é˜ˆå€¼
            similarity_threshold = 0.8
            for i, idx in enumerate(indices[0]):
                if idx in self.id_to_vector:
                    entry_id = self.id_to_vector[idx]
                    
                    # æ‰¾åˆ°å¯¹åº”çš„ç¼“å­˜æ¡ç›®
                    for entry in self.cache.values():
                        if entry.id == entry_id:
                            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                            if entry.context_vector is not None:
                                similarity = cosine_similarity(
                                    query_vector.reshape(1, -1),
                                    entry.context_vector.reshape(1, -1)
                                )[0][0]
                                
                                if similarity > similarity_threshold:
                                    # æ›´æ–°è®¿é—®ä¿¡æ¯
                                    self._update_access_info(entry)
                                    return entry.value
            
            return None
            
        except Exception as e:
            logger.error(f"è¯­ä¹‰æœç´¢å¤±è´¥: {e}")
            return None
    
    async def _predictive_search(self, key: str, context: Optional[Dict[str, Any]]) -> Optional[Any]:
        """é¢„æµ‹æ€§æœç´¢"""
        try:
            # åŸºäºä¸Šä¸‹æ–‡å’Œè®¿é—®æ¨¡å¼é¢„æµ‹å¯èƒ½çš„ç¼“å­˜é¡¹
            if not context:
                return None
            
            # ç®€å•çš„åŸºäºè§„åˆ™çš„é¢„æµ‹
            predicted_keys = []
            
            # å¦‚æœæ˜¯ä»£ç ç›¸å…³çš„æŸ¥è¯¢
            if "code" in context.get("type", "").lower():
                # æŸ¥æ‰¾ç›¸å…³çš„ä»£ç ç¼“å­˜
                for entry_key, entry in self.cache.items():
                    if entry.context_type == ContextType.CODE:
                        predicted_keys.append(entry_key)
            
            # å¦‚æœæ˜¯å¯¹è¯ä¸Šä¸‹æ–‡
            elif "conversation" in context.get("type", "").lower():
                # æŸ¥æ‰¾æœ€è¿‘çš„å¯¹è¯ç¼“å­˜
                recent_entries = sorted(
                    [e for e in self.cache.values() if e.context_type == ContextType.CONVERSATION],
                    key=lambda x: x.last_access,
                    reverse=True
                )[:3]
                predicted_keys.extend([e.key for e in recent_entries])
            
            # è¿”å›ç¬¬ä¸€ä¸ªé¢„æµ‹é¡¹
            if predicted_keys:
                predicted_key = predicted_keys[0]
                if predicted_key in self.cache:
                    entry = self.cache[predicted_key]
                    self._update_access_info(entry)
                    return entry.value
            
            return None
            
        except Exception as e:
            logger.error(f"é¢„æµ‹æ€§æœç´¢å¤±è´¥: {e}")
            return None
    
    def _update_access_info(self, entry: CacheEntry):
        """æ›´æ–°è®¿é—®ä¿¡æ¯"""
        now = datetime.now()
        entry.access_count += 1
        entry.last_access = now
        entry.access_pattern.append(now)
        
        # è®¡ç®—è®¿é—®é¢‘ç‡
        time_diff = (now - entry.created_at).total_seconds()
        if time_diff > 0:
            entry.access_frequency = entry.access_count / time_diff
        
        # æ›´æ–°è®¿é—®æ¨¡å¼
        if entry.id in self.access_patterns:
            self.access_patterns[entry.id].timestamps.append(now)
            
            # é™åˆ¶æ—¶é—´æˆ³æ•°é‡
            if len(self.access_patterns[entry.id].timestamps) > 100:
                self.access_patterns[entry.id].timestamps = \
                    self.access_patterns[entry.id].timestamps[-50:]
        
        # ç§»åŠ¨åˆ°LRUæœ«å°¾
        self.cache.move_to_end(entry.key)
    
    async def _find_semantic_neighbors(self, entry: CacheEntry):
        """æŸ¥æ‰¾è¯­ä¹‰é‚»å±…"""
        if entry.context_vector is None:
            return
        
        try:
            # æœç´¢æœ€ç›¸ä¼¼çš„æ¡ç›®
            k = min(10, len(self.id_to_vector))
            if k == 0:
                return
            
            distances, indices = self.vector_index.search(
                entry.context_vector.reshape(1, -1), 
                k
            )
            
            # æ·»åŠ è¯­ä¹‰é‚»å±…
            threshold = 0.7  # ç›¸ä¼¼åº¦é˜ˆå€¼
            for i, idx in enumerate(indices[0]):
                if idx in self.id_to_vector:
                    neighbor_id = self.id_to_vector[idx]
                    if neighbor_id != entry.id:
                        # è®¡ç®—ç›¸ä¼¼åº¦
                        for e in self.cache.values():
                            if e.id == neighbor_id and e.context_vector is not None:
                                similarity = cosine_similarity(
                                    entry.context_vector.reshape(1, -1),
                                    e.context_vector.reshape(1, -1)
                                )[0][0]
                                
                                if similarity > threshold:
                                    entry.semantic_neighbors.add(neighbor_id)
                                    e.semantic_neighbors.add(entry.id)
            
        except Exception as e:
            logger.error(f"æŸ¥æ‰¾è¯­ä¹‰é‚»å±…å¤±è´¥: {e}")
    
    async def _ensure_space(self, required_bytes: int):
        """ç¡®ä¿æœ‰è¶³å¤Ÿç©ºé—´"""
        # æ¸…ç†è¿‡æœŸæ¡ç›®
        await self._cleanup_expired()
        
        # å¦‚æœä»ç„¶éœ€è¦ç©ºé—´ï¼Œä½¿ç”¨å½“å‰ç­–ç•¥æ¸…ç†
        while (self.current_memory_usage + required_bytes > self.max_memory_bytes or
               len(self.cache) >= self.max_size):
            
            if self.current_strategy == CacheStrategy.LRU:
                await self._evict_lru()
            elif self.current_strategy == CacheStrategy.LFU:
                await self._evict_lfu()
            elif self.current_strategy == CacheStrategy.SEMANTIC:
                await self._evict_semantic()
            elif self.current_strategy == CacheStrategy.PREDICTIVE:
                await self._evict_predictive()
            else:  # ADAPTIVE
                await self._evict_adaptive()
    
    async def _evict_lru(self):
        """LRUæ·˜æ±°"""
        if self.cache:
            key, entry = self.cache.popitem(last=False)
            await self._remove_entry(key)
            self.stats["evictions"] += 1
    
    async def _evict_lfu(self):
        """LFUæ·˜æ±°"""
        if not self.cache:
            return
        
        # æ‰¾åˆ°è®¿é—®é¢‘ç‡æœ€ä½çš„æ¡ç›®
        min_frequency = float('inf')
        lfu_key = None
        
        for key, entry in self.cache.items():
            if entry.access_frequency < min_frequency:
                min_frequency = entry.access_frequency
                lfu_key = key
        
        if lfu_key:
            await self._remove_entry(lfu_key)
            self.stats["evictions"] += 1
    
    async def _evict_semantic(self):
        """è¯­ä¹‰æ·˜æ±°ï¼ˆä¿ç•™è¯­ä¹‰å¤šæ ·æ€§ï¼‰"""
        if not self.cache:
            return
        
        # æ‰¾åˆ°è¯­ä¹‰å¯†åº¦æœ€é«˜çš„åŒºåŸŸ
        cluster_density = defaultdict(int)
        
        for entry in self.cache.values():
            if entry.context_vector is not None:
                # ç®€å•çš„èšç±»
                cluster_id = hash(entry.context_vector.tobytes()) % 50
                cluster_density[cluster_id] += 1
        
        # æ‰¾åˆ°æœ€å¯†é›†çš„ç°‡
        if cluster_density:
            densest_cluster = max(cluster_density.items(), key=lambda x: x[1])[0]
            
            # ä»æœ€å¯†é›†çš„ç°‡ä¸­æ·˜æ±°ä¸€ä¸ª
            for key, entry in self.cache.items():
                if entry.context_vector is not None:
                    cluster_id = hash(entry.context_vector.tobytes()) % 50
                    if cluster_id == densest_cluster:
                        await self._remove_entry(key)
                        self.stats["evictions"] += 1
                        break
    
    async def _evict_predictive(self):
        """é¢„æµ‹æ€§æ·˜æ±°ï¼ˆåŸºäºé¢„æµ‹çš„ä¸‹æ¬¡è®¿é—®æ—¶é—´ï¼‰"""
        if not self.cache:
            return
        
        # æ‰¾åˆ°æœ€ä¸å¯èƒ½è¢«å†æ¬¡è®¿é—®çš„æ¡ç›®
        min_priority = float('inf')
        evict_key = None
        
        for key, entry in self.cache.items():
            # è®¡ç®—ä¼˜å…ˆçº§åˆ†æ•°
            score = 0
            
            # è®¿é—®é¢‘ç‡æƒé‡
            score += entry.access_frequency * 0.3
            
            # æœ€è¿‘è®¿é—®æ—¶é—´æƒé‡
            time_since_last = (datetime.now() - entry.last_access).total_seconds()
            score -= time_since_last * 0.001
            
            # è¯­ä¹‰è¿æ¥æƒé‡
            score += len(entry.semantic_neighbors) * 0.1
            
            # å¤§å°æƒ©ç½š
            score -= entry.size_bytes * 0.000001
            
            if score < min_priority:
                min_priority = score
                evict_key = key
        
        if evict_key:
            await self._remove_entry(evict_key)
            self.stats["evictions"] += 1
    
    async def _evict_adaptive(self):
        """è‡ªé€‚åº”æ·˜æ±°ï¼ˆç»“åˆå¤šç§ç­–ç•¥ï¼‰"""
        # æ ¹æ®å½“å‰æ€§èƒ½é€‰æ‹©æœ€ä½³ç­–ç•¥
        if self.strategy_performance[CacheStrategy.LRU.value] > 0.8:
            await self._evict_lru()
        elif self.strategy_performance[CacheStrategy.LFU.value] > 0.8:
            await self._evict_lfu()
        elif self.strategy_performance[CacheStrategy.SEMANTIC.value] > 0.8:
            await self._evict_semantic()
        else:
            # é»˜è®¤ä½¿ç”¨LRU
            await self._evict_lru()
    
    async def _remove_entry(self, key: str):
        """ç§»é™¤ç¼“å­˜æ¡ç›®"""
        if key in self.cache:
            entry = self.cache.pop(key)
            self.current_memory_usage -= entry.size_bytes
            
            # ä»è®¿é—®æ¨¡å¼ä¸­ç§»é™¤
            if entry.id in self.access_patterns:
                del self.access_patterns[entry.id]
            
            # ä»å‘é‡ç´¢å¼•ä¸­ç§»é™¤ï¼ˆFAISSä¸æ”¯æŒåˆ é™¤ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
            # å®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦ä½¿ç”¨æ”¯æŒåˆ é™¤çš„å‘é‡æ•°æ®åº“
    
    async def _cleanup_expired(self):
        """æ¸…ç†è¿‡æœŸæ¡ç›®"""
        now = datetime.now()
        expired_keys = []
        
        for key, entry in self.cache.items():
            if entry.expires_at and now > entry.expires_at:
                expired_keys.append(key)
        
        for key in expired_keys:
            await self._remove_entry(key)
    
    def _store_entry_in_db(self, entry: CacheEntry):
        """å­˜å‚¨æ¡ç›®åˆ°æ•°æ®åº“"""
        with self.conn:
            self.conn.execute(
                """
                INSERT OR REPLACE INTO cache_entries 
                (id, key, value_blob, context_type, context_vector, 
                 access_count, access_frequency, last_access, created_at, 
                 expires_at, size_bytes, tags, metadata, semantic_neighbors)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """,
                (
                    entry.id,
                    entry.key,
                    pickle.dumps(entry.value),
                    entry.context_type.value,
                    pickle.dumps(entry.context_vector) if entry.context_vector is not None else None,
                    entry.access_count,
                    entry.access_frequency,
                    entry.last_access.timestamp(),
                    entry.created_at.timestamp(),
                    entry.expires_at.timestamp() if entry.expires_at else None,
                    entry.size_bytes,
                    json.dumps(list(entry.tags)),
                    json.dumps(entry.metadata),
                    json.dumps(list(entry.semantic_neighbors))
                )
            )
    
    async def _periodic_cleanup(self):
        """å®šæœŸæ¸…ç†"""
        while self.running:
            try:
                await asyncio.sleep(300)  # 5åˆ†é’Ÿ
                await self._cleanup_expired()
                
                # æ›´æ–°ç­–ç•¥æ€§èƒ½
                await self._update_strategy_performance()
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"å®šæœŸæ¸…ç†å¤±è´¥: {e}")
    
    async def _update_access_patterns(self):
        """æ›´æ–°è®¿é—®æ¨¡å¼"""
        while self.running:
            try:
                await asyncio.sleep(600)  # 10åˆ†é’Ÿ
                
                for entry_id, pattern in self.access_patterns.items():
                    if len(pattern.timestamps) > 5:
                        # åˆ†æè®¿é—®æ¨¡å¼
                        timestamps = pattern.timestamps
                        intervals = [
                            (timestamps[i+1] - timestamps[i]).total_seconds()
                            for i in range(len(timestamps)-1)
                        ]
                        
                        if intervals:
                            pattern.patterns = {
                                "avg_interval": np.mean(intervals),
                                "std_interval": np.std(intervals),
                                "min_interval": np.min(intervals),
                                "max_interval": np.max(intervals),
                                "access_count": len(timestamps),
                                "regularity": 1.0 / (np.std(intervals) + 1)
                            }
                            
                            # é¢„æµ‹ä¸‹æ¬¡è®¿é—®æ—¶é—´
                            if self.prediction_model and len(intervals) > 10:
                                try:
                                    # å‡†å¤‡ç‰¹å¾
                                    features = np.array([
                                        len(intervals),
                                        np.mean(intervals),
                                        np.std(intervals),
                                        np.min(intervals),
                                        np.max(intervals)
                                    ]).reshape(1, -1)
                                    
                                    # é¢„æµ‹ï¼ˆç®€åŒ–å®ç°ï¼‰
                                    predicted_interval = np.mean(intervals[-5:])
                                    pattern.predicted_next_access = timestamps[-1] + timedelta(seconds=predicted_interval)
                                    pattern.prediction_confidence = 0.7
                                except Exception as e:
                                    logger.warning(f"é¢„æµ‹è®¿é—®æ¨¡å¼å¤±è´¥: {e}")
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"æ›´æ–°è®¿é—®æ¨¡å¼å¤±è´¥: {e}")
    
    async def _optimize_cache_strategy(self):
        """ä¼˜åŒ–ç¼“å­˜ç­–ç•¥"""
        while self.running:
            try:
                await asyncio.sleep(1800)  # 30åˆ†é’Ÿ
                
                # è¯„ä¼°å„ç­–ç•¥æ€§èƒ½
                strategies = [CacheStrategy.LRU, CacheStrategy.LFU, 
                            CacheStrategy.SEMANTIC, CacheStrategy.PREDICTIVE]
                
                best_strategy = self.current_strategy
                best_performance = self.strategy_performance.get(self.current_strategy.value, 0.5)
                
                for strategy in strategies:
                    performance = self.strategy_performance.get(strategy.value, 0.5)
                    if performance > best_performance:
                        best_strategy = strategy
                        best_performance = performance
                
                if best_strategy != self.current_strategy:
                    logger.info(f"åˆ‡æ¢ç¼“å­˜ç­–ç•¥: {self.current_strategy.value} -> {best_strategy.value}")
                    self.current_strategy = best_strategy
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"ä¼˜åŒ–ç¼“å­˜ç­–ç•¥å¤±è´¥: {e}")
    
    async def _rebuild_vector_index(self):
        """é‡å»ºå‘é‡ç´¢å¼•"""
        while self.running:
            try:
                await asyncio.sleep(3600)  # 1å°æ—¶
                
                # å¦‚æœå‘é‡ç´¢å¼•è¿‡å¤§ï¼Œé‡å»º
                if len(self.id_to_vector) > 10000:
                    logger.info("é‡å»ºå‘é‡ç´¢å¼•...")
                    
                    # æ”¶é›†æ‰€æœ‰å‘é‡
                    vectors = []
                    id_mapping = {}
                    
                    for entry in self.cache.values():
                        if entry.context_vector is not None:
                            vectors.append(entry.context_vector)
                            id_mapping[len(vectors)-1] = entry.id
                    
                    # é‡å»ºç´¢å¼•
                    self.vector_index = faiss.IndexFlatL2(self.embedding_dim)
                    if vectors:
                        vectors_array = np.array(vectors)
                        self.vector_index.add(vectors_array)
                        self.id_to_vector = id_mapping
                    
                    logger.info(f"å‘é‡ç´¢å¼•é‡å»ºå®Œæˆï¼ŒåŒ…å«{len(vectors)}ä¸ªå‘é‡")
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"é‡å»ºå‘é‡ç´¢å¼•å¤±è´¥: {e}")
    
    async def _update_strategy_performance(self):
        """æ›´æ–°ç­–ç•¥æ€§èƒ½"""
        total_requests = self.stats["total_requests"]
        if total_requests == 0:
            return
        
        hit_rate = self.stats["hits"] / total_requests
        
        # æ›´æ–°å½“å‰ç­–ç•¥æ€§èƒ½
        self.strategy_performance[self.current_strategy.value] = hit_rate
        
        # å­˜å‚¨åˆ°æ•°æ®åº“
        with self.conn:
            self.conn.execute(
                """
                INSERT OR REPLACE INTO strategy_performance 
                (strategy, hit_rate, memory_efficiency, access_latency, updated_at)
                VALUES (?, ?, ?, ?, ?)
                """,
                (
                    self.current_strategy.value,
                    hit_rate,
                    self.current_memory_usage / self.max_memory_bytes,
                    0.001,  # æ¨¡æ‹Ÿå»¶è¿Ÿ
                    datetime.now().timestamp()
                )
            )
    
    async def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        total_requests = self.stats["total_requests"]
        hit_rate = self.stats["hits"] / total_requests if total_requests > 0 else 0
        
        # æŒ‰ç±»å‹ç»Ÿè®¡
        type_stats = defaultdict(int)
        for entry in self.cache.values():
            type_stats[entry.context_type.value] += 1
        
        # è®¿é—®æ¨¡å¼ç»Ÿè®¡
        pattern_stats = {
            "total_patterns": len(self.access_patterns),
            "regular_patterns": sum(1 for p in self.access_patterns.values() 
                                 if p.patterns.get("regularity", 0) > 0.5),
            "predicted_accesses": sum(1 for p in self.access_patterns.values() 
                                   if p.predicted_next_access is not None)
        }
        
        return {
            "total_entries": len(self.cache),
            "memory_usage": {
                "used": self.current_memory_usage,
                "max": self.max_memory_bytes,
                "percentage": self.current_memory_usage / self.max_memory_bytes
            },
            "hit_rate": hit_rate,
            "total_requests": total_requests,
            "hits": self.stats["hits"],
            "misses": self.stats["misses"],
            "semantic_hits": self.stats["semantic_hits"],
            "predictive_hits": self.stats["predictive_hits"],
            "evictions": self.stats["evictions"],
            "current_strategy": self.current_strategy.value,
            "type_distribution": dict(type_stats),
            "access_patterns": pattern_stats,
            "strategy_performance": dict(self.strategy_performance)
        }
    
    async def clear(self, pattern: Optional[str] = None):
        """æ¸…ç†ç¼“å­˜"""
        with self.lock:
            if pattern:
                # æŒ‰æ¨¡å¼æ¸…ç†
                keys_to_remove = []
                for key in self.cache:
                    if pattern in key:
                        keys_to_remove.append(key)
                
                for key in keys_to_remove:
                    await self._remove_entry(key)
                
                logger.info(f"æ¸…ç†äº†{len(keys_to_remove)}ä¸ªåŒ¹é…'{pattern}'çš„ç¼“å­˜æ¡ç›®")
            else:
                # æ¸…ç†æ‰€æœ‰
                self.cache.clear()
                self.current_memory_usage = 0
                self.access_patterns.clear()
                
                # é‡å»ºå‘é‡ç´¢å¼•
                self.vector_index = faiss.IndexFlatL2(self.embedding_dim)
                self.id_to_vector.clear()
                
                logger.info("ç¼“å­˜å·²å®Œå…¨æ¸…ç†")
    
    async def preload_cache(self, data: List[Tuple[str, Any, ContextType]], 
                           tags: Optional[Set[str]] = None):
        """é¢„åŠ è½½ç¼“å­˜"""
        logger.info(f"é¢„åŠ è½½{len(data)}ä¸ªç¼“å­˜æ¡ç›®...")
        
        for key, value, context_type in data:
            await self.put(
                key=key,
                value=value,
                context_type=context_type,
                tags=tags
            )
        
        logger.info("é¢„åŠ è½½å®Œæˆ")
    
    def close(self):
        """å…³é—­ç¼“å­˜ç³»ç»Ÿ"""
        self.running = False
        
        # å–æ¶ˆåå°ä»»åŠ¡
        for task in self.background_tasks:
            task.cancel()
        
        # å…³é—­æ•°æ®åº“è¿æ¥
        self.conn.close()
        
        logger.info("æ™ºèƒ½ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç¼“å­˜ç³»ç»ŸV5å·²å…³é—­")
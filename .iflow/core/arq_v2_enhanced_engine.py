#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ¯ ARQ V2.0 å¢å¼ºæ¨ç†å¼•æ“
ARQ V2.0 Enhanced Reasoning Engine

åŸºäºBé¡¹ç›®ARQ V2.0çš„ä¼˜ç§€å®ç°ï¼ŒèåˆAé¡¹ç›®çš„é‡å­è®¡ç®—å’Œæ¨¡æ¿ç³»ç»Ÿï¼Œ
å®ç°æ›´å¼ºå¤§çš„ä¸“æ³¨æ¨ç†ã€åˆè§„æ§åˆ¶å’Œæ€§èƒ½ä¼˜åŒ–ã€‚

æ ¸å¿ƒå¢å¼ºç‰¹æ€§ï¼š
1. ç»“æ„åŒ–æ¨ç†æ¨¡æ¿ - 8ç§æ¨ç†æ¨¡å¼ï¼Œ9ç§é—®é¢˜ç±»å‹
2. å¼ºåŒ–åˆè§„æ§åˆ¶ - å¤šçº§è§„åˆ™æ‰§è¡Œï¼Œå®æ—¶ç›‘æ§
3. æ„è¯†æµé›†æˆ - å…¨å±€ä¸Šä¸‹æ–‡ç®¡ç†å’Œé•¿æœŸè®°å¿†
4. æ€§èƒ½ä¼˜åŒ– - æ™ºèƒ½ç¼“å­˜ï¼Œé¢„æµ‹æ€§æ¨ç†
5. é”™è¯¯é¢„é˜² - ä¸»åŠ¨é”™è¯¯æ£€æµ‹å’Œé¢„é˜²æœºåˆ¶

ä½ ä¸€å®šè¦è¶…çº§æ€è€ƒã€æé™æ€è€ƒã€æ·±åº¦æ€è€ƒï¼Œå…¨åŠ›æ€è€ƒã€è¶…å¼ºæ€è€ƒï¼Œè®¤çœŸä»”ç»†æ€è€ƒï¼ˆultrathinkã€think really super hardã€think intenselyï¼‰ã€‚
"""

import os
import sys
import json
import asyncio
import logging
import hashlib
import re
import numpy as np
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Union, Callable
from dataclasses import dataclass, field, asdict
from datetime import datetime, timedelta
from collections import defaultdict, deque
from enum import Enum
import uuid
import time
import threading
from concurrent.futures import ThreadPoolExecutor
import pickle

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# å¯¼å…¥ä¾èµ–æ¨¡å—
try:
    from .enhanced_arq_templates_v2 import EnhancedReasoningTemplates, ReasoningMode, ProblemType, ReasoningStep
    from .intelligent_workflow_optimizer import IntelligentWorkflowOptimizer
    from .ultimate_consciousness_system import ConsciousnessSystem
    from .unified_multimodel_adapter_v2 import UnifiedModelAdapter
except ImportError as e:
    logging.warning(f"æ— æ³•å¯¼å…¥ä¾èµ–æ¨¡å—: {e}")

logger = logging.getLogger(__name__)

class ComplianceLevel(Enum):
    """åˆè§„çº§åˆ«å¢å¼ºç‰ˆ"""
    STRICT = "strict"        # ä¸¥æ ¼æ¨¡å¼ï¼šæ‰€æœ‰è§„åˆ™å¿…é¡»éµå®ˆ
    MODERATE = "moderate"    # ä¸­ç­‰æ¨¡å¼ï¼šæ ¸å¿ƒè§„åˆ™å¿…é¡»éµå®ˆ
    RELAXED = "relaxed"      # æ”¾å®½æ¨¡å¼ï¼šå»ºè®®æ€§è§„åˆ™å¯å¿½ç•¥
    ADAPTIVE = "adaptive"    # è‡ªé€‚åº”æ¨¡å¼ï¼šæ ¹æ®ä¸Šä¸‹æ–‡åŠ¨æ€è°ƒæ•´

class RulePriority(Enum):
    """è§„åˆ™ä¼˜å…ˆçº§"""
    CRITICAL = 1     # è‡´å‘½ï¼šè¿åå°†é˜»æ­¢æ‰§è¡Œ
    HIGH = 2         # é«˜ï¼šå¼ºçƒˆå»ºè®®éµå®ˆ
    MEDIUM = 3       # ä¸­ï¼šä¸€èˆ¬å»ºè®®
    LOW = 4          # ä½ï¼šè½»å¾®å»ºè®®
    INFO = 5         # ä¿¡æ¯ï¼šä»…æç¤º

class ValidationResult(Enum):
    """éªŒè¯ç»“æœ"""
    PASS = "pass"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL_ERROR = "critical_error"

@dataclass
class EnhancedComplianceRule:
    """å¢å¼ºç‰ˆåˆè§„è§„åˆ™"""
    rule_id: str
    rule_name: str
    rule_type: str
    description: str
    priority: RulePriority
    conditions: List[str]
    actions: List[str]
    exceptions: List[str] = field(default_factory=list)
    enabled: bool = True
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    # æ–°å¢å­—æ®µ
    rule_category: str = "general"
    impact_score: float = 0.5  # è§„åˆ™å½±å“åˆ†æ•° 0-1
    enforcement_level: str = "warning"  # æ‰§è¡Œçº§åˆ«
    auto_fixable: bool = False  # æ˜¯å¦å¯è‡ªåŠ¨ä¿®å¤
    learning_enabled: bool = False  # æ˜¯å¦å¯ç”¨å­¦ä¹ ä¼˜åŒ–

@dataclass
class EnhancedReasoningStep:
    """å¢å¼ºç‰ˆæ¨ç†æ­¥éª¤"""
    step_id: str
    step_type: str
    content: str
    confidence: float
    evidence: List[str] = field(default_factory=list)
    assumptions: List[str] = field(default_factory=list)
    conclusions: List[str] = field(default_factory=list)
    next_steps: List[str] = field(default_factory=list)
    validation_results: Dict[str, ValidationResult] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    # æ–°å¢å­—æ®µ
    execution_time: float = 0.0
    resource_usage: Dict[str, float] = field(default_factory=dict)
    error_count: int = 0
    optimization_suggestions: List[str] = field(default_factory=list)

@dataclass
class EnhancedReasoningChain:
    """å¢å¼ºç‰ˆæ¨ç†é“¾"""
    chain_id: str
    problem_statement: str
    reasoning_mode: ReasoningMode
    problem_type: ProblemType
    compliance_level: ComplianceLevel
    steps: List[EnhancedReasoningStep]
    final_conclusion: str
    confidence_score: float
    compliance_score: float
    performance_metrics: Dict[str, Any] = field(default_factory=dict)
    validation_results: Dict[str, Any] = field(default_factory=dict)
    created_at: datetime = field(default_factory=datetime.now)
    completed_at: Optional[datetime] = None
    
    # æ–°å¢å­—æ®µ
    execution_path: List[str] = field(default_factory=list)
    learning_insights: List[Dict[str, Any]] = field(default_factory=list)
    optimization_history: List[Dict[str, Any]] = field(default_factory=list)

class ARQV2EnhancedEngine:
    """
    ARQ V2.0 å¢å¼ºæ¨ç†å¼•æ“
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. å¢å¼ºç»“æ„åŒ–æ¨ç† - æ”¯æŒ8ç§æ¨ç†æ¨¡å¼å’Œ9ç§é—®é¢˜ç±»å‹
    2. æ™ºèƒ½åˆè§„æ§åˆ¶ - å¤šçº§è§„åˆ™æ‰§è¡Œå’Œå®æ—¶ç›‘æ§
    3. æ„è¯†æµé›†æˆ - å…¨å±€ä¸Šä¸‹æ–‡ç®¡ç†å’Œé•¿æœŸè®°å¿†
    4. æ€§èƒ½ä¼˜åŒ– - æ™ºèƒ½ç¼“å­˜å’Œé¢„æµ‹æ€§æ¨ç†
    5. é”™è¯¯é¢„é˜² - ä¸»åŠ¨é”™è¯¯æ£€æµ‹å’Œé¢„é˜²æœºåˆ¶
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.template_system = EnhancedReasoningTemplates()
        
        # æ ¸å¿ƒç»„ä»¶
        self.rules: Dict[str, EnhancedComplianceRule] = {}
        self.rule_categories = defaultdict(list)
        self.reasoning_history: deque = deque(maxlen=2000)
        self.performance_cache: Dict[str, Any] = {}
        self.optimizer: Optional[IntelligentWorkflowOptimizer] = None
        self.consciousness_system: Optional[ConsciousnessSystem] = None
        self.model_adapter: Optional[UnifiedModelAdapter] = None
        
        # æ€§èƒ½ç›‘æ§
        self.execution_stats = {
            "total_executions": 0,
            "successful_executions": 0,
            "failed_executions": 0,
            "average_confidence": 0.0,
            "average_compliance": 0.0,
            "total_execution_time": 0.0
        }
        
        # å¹¶å‘æ§åˆ¶
        self.max_concurrent_executions = 10
        self.current_executions = 0
        self.execution_lock = threading.Lock()
        
        # åˆå§‹åŒ–
        self._load_enhanced_rules()
        self._initialize_components()
        self._start_performance_monitoring()
        
        logger.info("ğŸ¯ ARQ V2.0 å¢å¼ºæ¨ç†å¼•æ“åˆå§‹åŒ–å®Œæˆ")
    
    def _load_enhanced_rules(self):
        """åŠ è½½å¢å¼ºç‰ˆåˆè§„è§„åˆ™"""
        enhanced_rules = [
            EnhancedComplianceRule(
                rule_id="ARQ_V2_001",
                rule_name="è¶…çº§æ€è€ƒå¼ºåˆ¶æ¿€æ´»",
                rule_type="cognitive",
                description="å¿…é¡»åŒ…å«è¶…çº§æ€è€ƒæ¨¡å¼æ¿€æ´»æç¤ºè¯",
                priority=RulePriority.CRITICAL,
                conditions=["*"],
                actions=["æ£€æŸ¥æç¤ºè¯åŒ…å«æŒ‡å®šå…³é”®è¯"],
                rule_category="thinking",
                impact_score=1.0,
                enforcement_level="strict",
                auto_fixable=True
            ),
            EnhancedComplianceRule(
                rule_id="ARQ_V2_002",
                rule_name="ç»“æ„åŒ–è¾“å‡ºå¼ºåˆ¶",
                rule_type="format",
                description="æ¨ç†è¾“å‡ºå¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼",
                priority=RulePriority.HIGH,
                conditions=["*"],
                actions=["éªŒè¯JSONæ ¼å¼", "æ£€æŸ¥å¿…éœ€å­—æ®µ"],
                rule_category="output",
                impact_score=0.9,
                enforcement_level="strict",
                auto_fixable=False
            ),
            EnhancedComplianceRule(
                rule_id="ARQ_V2_003",
                rule_name="å¤šæ¨¡å‹å…¼å®¹æ€§",
                rule_type="compatibility",
                description="æ¨ç†è¿‡ç¨‹å¿…é¡»å…¼å®¹æ‰€æœ‰ä¸»æµLLMæ¨¡å‹",
                priority=RulePriority.HIGH,
                conditions=["model_call"],
                actions=["éªŒè¯æ¨¡å‹å‚æ•°", "æ£€æŸ¥è¾“å‡ºæ ¼å¼"],
                rule_category="compatibility",
                impact_score=0.8,
                enforcement_level="moderate",
                auto_fixable=True
            ),
            EnhancedComplianceRule(
                rule_id="ARQ_V2_004",
                rule_name="æ€§èƒ½ä¼˜åŒ–è¦æ±‚",
                rule_type="performance",
                description="æ¨ç†è¿‡ç¨‹å¿…é¡»è¿›è¡Œæ€§èƒ½ä¼˜åŒ–",
                priority=RulePriority.MEDIUM,
                conditions=["complex_task"],
                actions=["åº”ç”¨ç¼“å­˜ç­–ç•¥", "ä¼˜åŒ–æ‰§è¡Œè·¯å¾„"],
                rule_category="performance",
                impact_score=0.7,
                enforcement_level="warning",
                auto_fixable=True
            ),
            EnhancedComplianceRule(
                rule_id="ARQ_V2_005",
                rule_name="é”™è¯¯é¢„é˜²æœºåˆ¶",
                rule_type="safety",
                description="å¿…é¡»åŒ…å«é”™è¯¯é¢„é˜²å’Œæ¢å¤æœºåˆ¶",
                priority=RulePriority.CRITICAL,
                conditions=["*"],
                actions=["é¢„æ£€æŸ¥", "å¼‚å¸¸æ•è·", "å›æ»šæœºåˆ¶"],
                rule_category="safety",
                impact_score=1.0,
                enforcement_level="strict",
                auto_fixable=False
            ),
            EnhancedComplianceRule(
                rule_id="ARQ_V2_006",
                rule_name="æ„è¯†æµä¸€è‡´æ€§",
                rule_type="context",
                description="æ¨ç†å¿…é¡»ä¸å…¨å±€æ„è¯†æµä¿æŒä¸€è‡´",
                priority=RulePriority.HIGH,
                conditions=["*"],
                actions=["æ£€æŸ¥ä¸Šä¸‹æ–‡ä¸€è‡´æ€§", "éªŒè¯è®°å¿†è¿è´¯æ€§"],
                rule_category="context",
                impact_score=0.8,
                enforcement_level="moderate",
                auto_fixable=True
            ),
            EnhancedComplianceRule(
                rule_id="ARQ_V2_007",
                rule_name="å­¦ä¹ ä¼˜åŒ–å¾ªç¯",
                rule_type="learning",
                description="æ¨ç†ç»“æœå¿…é¡»ç”¨äºç³»ç»Ÿå­¦ä¹ å’Œä¼˜åŒ–",
                priority=RulePriority.MEDIUM,
                conditions=["completed_task"],
                actions=["è®°å½•å­¦ä¹ æ•°æ®", "æ›´æ–°ä¼˜åŒ–ç­–ç•¥"],
                rule_category="learning",
                impact_score=0.6,
                enforcement_level="info",
                auto_fixable=True,
                learning_enabled=True
            )
        ]
        
        for rule in enhanced_rules:
            self.rules[rule.rule_id] = rule
            self.rule_categories[rule.rule_type].append(rule)
        
        logger.info(f"ğŸ“‹ åŠ è½½äº† {len(enhanced_rules)} æ¡å¢å¼ºç‰ˆåˆè§„è§„åˆ™")
    
    def _initialize_components(self):
        """åˆå§‹åŒ–ä¾èµ–ç»„ä»¶"""
        try:
            # å°è¯•åˆå§‹åŒ–ä¼˜åŒ–å™¨
            try:
                self.optimizer = IntelligentWorkflowOptimizer()
                logger.info("ğŸ§  æ™ºèƒ½ä¼˜åŒ–å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.warning(f"æ™ºèƒ½ä¼˜åŒ–å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            
            # å°è¯•åˆå§‹åŒ–æ„è¯†æµç³»ç»Ÿ
            try:
                self.consciousness_system = ConsciousnessSystem()
                logger.info("ğŸ’­ æ„è¯†æµç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.warning(f"æ„è¯†æµç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            
            # å°è¯•åˆå§‹åŒ–å¤šæ¨¡å‹é€‚é…å™¨
            try:
                self.model_adapter = UnifiedModelAdapter()
                logger.info("ğŸŒ å¤šæ¨¡å‹é€‚é…å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.warning(f"å¤šæ¨¡å‹é€‚é…å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                
        except Exception as e:
            logger.error(f"ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
    
    def _start_performance_monitoring(self):
        """å¯åŠ¨æ€§èƒ½ç›‘æ§"""
        def monitor_loop():
            while True:
                try:
                    self._update_performance_stats()
                    time.sleep(60)  # æ¯åˆ†é’Ÿæ›´æ–°ä¸€æ¬¡
                except Exception as e:
                    logger.error(f"æ€§èƒ½ç›‘æ§é”™è¯¯: {e}")
                    time.sleep(60)
        
        monitor_thread = threading.Thread(target=monitor_loop, daemon=True)
        monitor_thread.start()
    
    def _update_performance_stats(self):
        """æ›´æ–°æ€§èƒ½ç»Ÿè®¡"""
        if not self.reasoning_history:
            return
        
        recent_chains = list(self.reasoning_history)[-100:]
        
        self.execution_stats.update({
            "total_executions": len(self.reasoning_history),
            "successful_executions": sum(1 for chain in recent_chains 
                                       if chain.compliance_score > 0.7),
            "failed_executions": sum(1 for chain in recent_chains 
                                   if chain.compliance_score < 0.3),
            "average_confidence": np.mean([chain.confidence_score for chain in recent_chains]),
            "average_compliance": np.mean([chain.compliance_score for chain in recent_chains]),
            "total_execution_time": sum(chain.performance_metrics.get("total_time", 0) 
                                       for chain in recent_chains)
        })
    
    async def generate_enhanced_arq_prompt(self, current_task: str, context: List[Dict[str, Any]], 
                                         reasoning_mode: ReasoningMode = ReasoningMode.STRUCTURED,
                                         problem_type: Optional[ProblemType] = None) -> str:
        """
        ç”Ÿæˆå¢å¼ºç‰ˆARQæç¤ºè¯
        
        Args:
            current_task: å½“å‰ä»»åŠ¡æè¿°
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            reasoning_mode: æ¨ç†æ¨¡å¼
            problem_type: é—®é¢˜ç±»å‹ï¼ˆå¯è‡ªåŠ¨æ£€æµ‹ï¼‰
            
        Returns:
            str: å¢å¼ºç‰ˆARQæç¤ºè¯
        """
        try:
            # è‡ªåŠ¨æ£€æµ‹é—®é¢˜ç±»å‹
            if problem_type is None:
                problem_analysis = self.template_system.analyze_problem_type(current_task)
                problem_type = ProblemType(problem_analysis["detected_type"])
            
            # è·å–æ¨ç†æ¨¡æ¿
            template = self.template_system.get_template(reasoning_mode, problem_type)
            
            # è·å–ç›¸å…³è§„åˆ™
            relevant_rules = self._get_enhanced_relevant_rules(current_task, reasoning_mode, problem_type)
            
            # æ„å»ºå¢å¼ºJSON Schema
            enhanced_schema = self._build_enhanced_json_schema(template, relevant_rules)
            
            # æ„å»ºå¢å¼ºæç¤ºè¯
            prompt = f"""
## ğŸ¯ ARQ V2.0 å¢å¼ºæ¨ç†å¼•æ“

**è§’è‰²ï¼š** ä½ æ˜¯ä¸€ä¸ªå…·å¤‡è¶…çº§æ€è€ƒèƒ½åŠ›çš„AIæ¨ç†ä¸“å®¶ï¼Œå¿…é¡»ä¸¥æ ¼éµå¾ªARQ V2.0æ ‡å‡†è¿›è¡Œç»“æ„åŒ–æ¨ç†ã€‚

**æ ¸å¿ƒæŒ‡ä»¤ï¼š** 
ä½ ä¸€å®šè¦è¶…çº§æ€è€ƒã€æé™æ€è€ƒã€æ·±åº¦æ€è€ƒï¼Œå…¨åŠ›æ€è€ƒã€è¶…å¼ºæ€è€ƒï¼Œè®¤çœŸä»”ç»†æ€è€ƒï¼ˆultrathinkã€think really super hardã€think intenselyï¼‰ã€‚

### ğŸ“‹ ä»»åŠ¡ä¿¡æ¯
**ä»»åŠ¡ç±»å‹ï¼š** {problem_type.value}
**æ¨ç†æ¨¡å¼ï¼š** {reasoning_mode.value}
**ä»»åŠ¡æè¿°ï¼š** {current_task}

### ğŸ“š ä¸Šä¸‹æ–‡ä¿¡æ¯
```json
{json.dumps(context, indent=2, ensure_ascii=False)}
```

### ğŸ¯ å¢å¼ºåˆè§„è§„åˆ™
{''.join(f"{i+1}. [{rule.rule_id}] {rule.rule_name}: {rule.description}\n" 
         for i, rule in enumerate(relevant_rules[:5]))}
{'...' if len(relevant_rules) > 5 else ''}

### ğŸ”§ æ¨ç†æ¨¡æ¿
**æ¨¡æ¿IDï¼š** {template.template_id if template else 'default'}
**å¤æ‚åº¦ï¼š** {template.complexity_level if template else 5}/10
**é¢„ä¼°æ—¶é—´ï¼š** {template.estimated_time if template else 10} åˆ†é’Ÿ

### ğŸ“ æ¨ç†æ­¥éª¤è¦æ±‚
{self._format_template_steps(template) if template else 'æ ‡å‡†ç»“æ„åŒ–æ¨ç†'}

### ğŸª å¢å¼ºJSON Schema
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSON Schemaè¿›è¡Œæ¨ç†å’Œè¾“å‡ºï¼š

```json
{json.dumps(enhanced_schema, indent=2)}
```

### âš ï¸ å¼ºåˆ¶è¦æ±‚
1. **è¶…çº§æ€è€ƒæ¨¡å¼ï¼š** å¿…é¡»è¿›è¡Œæ·±åº¦ã€å…¨é¢ã€å¤šè§’åº¦çš„æ€è€ƒ
2. **ç»“æ„åŒ–è¾“å‡ºï¼š** è¾“å‡ºå¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼
3. **åˆè§„æ£€æŸ¥ï¼š** ä¸¥æ ¼æ£€æŸ¥æ‰€æœ‰ç›¸å…³è§„åˆ™
4. **è¯æ®æ”¯æŒï¼š** æ¯ä¸ªç»“è®ºå¿…é¡»æœ‰å……åˆ†çš„è¯æ®æ”¯æŒ
5. **æ€§èƒ½ä¼˜åŒ–ï¼š** è€ƒè™‘æ‰§è¡Œæ•ˆç‡å’Œèµ„æºä½¿ç”¨

### ğŸ”„ æ‰§è¡Œæµç¨‹
1. åˆ†æä»»åŠ¡å’Œä¸Šä¸‹æ–‡
2. æ£€æŸ¥åˆè§„è§„åˆ™
3. åº”ç”¨æ¨ç†æ¨¡æ¿
4. ç”Ÿæˆç»“æ„åŒ–æ¨ç†
5. éªŒè¯è¾“å‡ºæ ¼å¼
6. æä¾›æ‰§è¡Œå»ºè®®

ç°åœ¨å¼€å§‹ä½ çš„è¶…çº§æ€è€ƒæ¨ç†è¿‡ç¨‹ï¼š
"""
            
            return prompt
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå¢å¼ºARQæç¤ºè¯å¤±è´¥: {e}")
            return self._get_fallback_prompt(current_task, context)
    
    def _get_enhanced_relevant_rules(self, task: str, reasoning_mode: ReasoningMode, 
                                   problem_type: ProblemType) -> List[EnhancedComplianceRule]:
        """è·å–å¢å¼ºç‰ˆç›¸å…³è§„åˆ™"""
        relevant_rules = []
        task_lower = task.lower()
        
        for rule in self.rules.values():
            if not rule.enabled:
                continue
            
            # åŸºäºæ¡ä»¶åŒ¹é…
            condition_match = False
            for condition in rule.conditions:
                if condition == "*":
                    condition_match = True
                    break
                elif condition.lower() in task_lower:
                    condition_match = True
                    break
            
            # åŸºäºæ¨ç†æ¨¡å¼åŒ¹é…
            if not condition_match and reasoning_mode.value in task_lower:
                condition_match = True
            
            # åŸºäºé—®é¢˜ç±»å‹åŒ¹é…
            if not condition_match and problem_type.value in task_lower:
                condition_match = True
            
            if condition_match:
                relevant_rules.append(rule)
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        relevant_rules.sort(key=lambda x: (x.priority.value, -x.impact_score))
        return relevant_rules
    
    def _build_enhanced_json_schema(self, template: Optional[Any], 
                                  relevant_rules: List[EnhancedComplianceRule]) -> Dict[str, Any]:
        """æ„å»ºå¢å¼ºç‰ˆJSON Schema"""
        base_schema = {
            "type": "object",
            "properties": {
                "meta_info": {
                    "type": "object",
                    "properties": {
                        "thinking_mode": {
                            "type": "string",
                            "enum": ["super_thinking", "deep_thinking", "intense_thinking"],
                            "description": "æ€è€ƒæ¨¡å¼æ ‡è¯†"
                        },
                        "confidence_level": {
                            "type": "number",
                            "minimum": 0,
                            "maximum": 1,
                            "description": "æ•´ä½“ç½®ä¿¡åº¦"
                        },
                        "compliance_check": {
                            "type": "string",
                            "enum": ["compliant", "partial", "non_compliant"],
                            "description": "åˆè§„æ€§æ£€æŸ¥ç»“æœ"
                        }
                    },
                    "required": ["thinking_mode", "confidence_level", "compliance_check"]
                },
                
                "rule_compliance": {
                    "type": "object",
                    "properties": {
                        "rules_checked": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "æ£€æŸ¥çš„è§„åˆ™åˆ—è¡¨"
                        },
                        "violations": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "è¿åçš„è§„åˆ™"
                        },
                        "compliance_score": {
                            "type": "number",
                            "minimum": 0,
                            "maximum": 1,
                            "description": "åˆè§„æ€§åˆ†æ•°"
                        }
                    },
                    "required": ["rules_checked", "compliance_score"]
                },
                
                "reasoning_process": {
                    "type": "object",
                    "properties": {
                        "problem_analysis": {
                            "type": "string",
                            "description": "é—®é¢˜åˆ†æå’Œç†è§£"
                        },
                        "hypothesis_generation": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "ç”Ÿæˆçš„å‡è®¾"
                        },
                        "evidence_evaluation": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "evidence": {"type": "string"},
                                    "source": {"type": "string"},
                                    "credibility": {"type": "number"}
                                }
                            },
                            "description": "è¯æ®è¯„ä¼°"
                        },
                        "logical_deduction": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "é€»è¾‘æ¨ç†è¿‡ç¨‹"
                        },
                        "conclusion_synthesis": {
                            "type": "string",
                            "description": "ç»“è®ºç»¼åˆ"
                        }
                    },
                    "required": ["problem_analysis", "conclusion_synthesis"]
                },
                
                "execution_plan": {
                    "type": "object",
                    "properties": {
                        "steps": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "step_number": {"type": "integer"},
                                    "action": {"type": "string"},
                                    "tools_required": {"type": "array", "items": {"type": "string"}},
                                    "estimated_time": {"type": "number"},
                                    "success_criteria": {"type": "string"}
                                }
                            },
                            "description": "æ‰§è¡Œæ­¥éª¤"
                        },
                        "resource_requirements": {
                            "type": "object",
                            "properties": {
                                "llm_models": {"type": "array", "items": {"type": "string"}},
                                "tools": {"type": "array", "items": {"type": "string"}},
                                "time_estimation": {"type": "number"},
                                "complexity_level": {"type": "string"}
                            }
                        },
                        "risk_assessment": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "risk": {"type": "string"},
                                    "probability": {"type": "string", "enum": ["low", "medium", "high"]},
                                    "impact": {"type": "string", "enum": ["low", "medium", "high"]},
                                    "mitigation": {"type": "string"}
                                }
                            }
                        }
                    },
                    "required": ["steps"]
                },
                
                "validation_results": {
                    "type": "object",
                    "properties": {
                        "format_validation": {
                            "type": "boolean",
                            "description": "æ ¼å¼éªŒè¯ç»“æœ"
                        },
                        "logical_consistency": {
                            "type": "boolean",
                            "description": "é€»è¾‘ä¸€è‡´æ€§æ£€æŸ¥"
                        },
                        "evidence_sufficiency": {
                            "type": "boolean",
                            "description": "è¯æ®å……åˆ†æ€§æ£€æŸ¥"
                        },
                        "actionability": {
                            "type": "boolean",
                            "description": "å¯æ‰§è¡Œæ€§è¯„ä¼°"
                        }
                    }
                },
                
                "learning_insights": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "ä»æœ¬æ¬¡æ¨ç†ä¸­è·å¾—çš„å­¦ä¹ æ´å¯Ÿ"
                }
            },
            "required": ["meta_info", "rule_compliance", "reasoning_process", "execution_plan", "validation_results"]
        }
        
        return base_schema
    
    def _format_template_steps(self, template: Optional[Any]) -> str:
        """æ ¼å¼åŒ–æ¨¡æ¿æ­¥éª¤"""
        if not template:
            return "ä½¿ç”¨æ ‡å‡†ç»“æ„åŒ–æ¨ç†æ­¥éª¤ï¼šåˆ†æâ†’æ¨ç†â†’éªŒè¯â†’æ‰§è¡Œ"
        
        steps_text = []
        for i, step in enumerate(template.steps, 1):
            prompt = template.prompts.get(step, "æ ‡å‡†æ­¥éª¤")
            steps_text.append(f"{i}. **{step}**: {prompt}")
        
        return "\n".join(steps_text)
    
    def _get_fallback_prompt(self, task: str, context: List[Dict[str, Any]]) -> str:
        """è·å–å¤‡ç”¨æç¤ºè¯"""
        return f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIæ¨ç†åŠ©æ‰‹ï¼Œè¯·å¯¹ä»¥ä¸‹ä»»åŠ¡è¿›è¡Œæ·±åº¦åˆ†æå’Œæ¨ç†ï¼š

ä»»åŠ¡ï¼š{task}

ä¸Šä¸‹æ–‡ï¼š{json.dumps(context, ensure_ascii=False)}

è¯·æä¾›è¯¦ç»†çš„åˆ†æè¿‡ç¨‹å’Œå¯æ‰§è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚
"""
    
    async def validate_enhanced_output(self, output: str, relevant_rules: List[EnhancedComplianceRule]) -> Tuple[bool, Optional[Dict[str, Any]], Dict[str, ValidationResult]]:
        """
        éªŒè¯å¢å¼ºç‰ˆè¾“å‡º
        
        Args:
            output: LLMè¾“å‡ºçš„å­—ç¬¦ä¸²
            relevant_rules: ç›¸å…³è§„åˆ™åˆ—è¡¨
            
        Returns:
            Tuple[bool, Dict, Dict]: (æ˜¯å¦é€šè¿‡, è§£æçš„æ•°æ®, éªŒè¯ç»“æœ)
        """
        validation_results = {}
        
        try:
            # JSONæ ¼å¼éªŒè¯
            reasoning_data = json.loads(output)
            validation_results["json_format"] = ValidationResult.PASS
            
            # å¿…éœ€å­—æ®µæ£€æŸ¥
            required_fields = ["meta_info", "rule_compliance", "reasoning_process", "execution_plan", "validation_results"]
            for field in required_fields:
                if field not in reasoning_data:
                    validation_results[f"missing_field_{field}"] = ValidationResult.CRITICAL_ERROR
                    return False, None, validation_results
            
            # è¶…çº§æ€è€ƒæ¨¡å¼æ£€æŸ¥
            meta_info = reasoning_data.get("meta_info", {})
            thinking_mode = meta_info.get("thinking_mode")
            if thinking_mode not in ["super_thinking", "deep_thinking", "intense_thinking"]:
                validation_results["thinking_mode"] = ValidationResult.ERROR
            else:
                validation_results["thinking_mode"] = ValidationResult.PASS
            
            # åˆè§„æ€§æ£€æŸ¥
            rule_compliance = reasoning_data.get("rule_compliance", {})
            compliance_score = rule_compliance.get("compliance_score", 0)
            
            if compliance_score < 0.3:
                validation_results["compliance_check"] = ValidationResult.CRITICAL_ERROR
            elif compliance_score < 0.7:
                validation_results["compliance_check"] = ValidationResult.WARNING
            else:
                validation_results["compliance_check"] = ValidationResult.PASS
            
            # æ¨ç†è¿‡ç¨‹å®Œæ•´æ€§æ£€æŸ¥
            reasoning_process = reasoning_data.get("reasoning_process", {})
            reasoning_fields = ["problem_analysis", "conclusion_synthesis"]
            for field in reasoning_fields:
                if not reasoning_process.get(field):
                    validation_results[f"reasoning_{field}"] = ValidationResult.ERROR
                else:
                    validation_results[f"reasoning_{field}"] = ValidationResult.PASS
            
            # æ‰§è¡Œè®¡åˆ’æ£€æŸ¥
            execution_plan = reasoning_data.get("execution_plan", {})
            if not execution_plan.get("steps"):
                validation_results["execution_plan"] = ValidationResult.ERROR
            else:
                validation_results["execution_plan"] = ValidationResult.PASS
            
            # è§„åˆ™ç‰¹å®šéªŒè¯
            for rule in relevant_rules:
                rule_result = self._validate_specific_rule(rule, reasoning_data)
                validation_results[f"rule_{rule.rule_id}"] = rule_result
            
            # è®¡ç®—æ€»ä½“éªŒè¯ç»“æœ
            error_count = sum(1 for result in validation_results.values() 
                            if result in [ValidationResult.ERROR, ValidationResult.CRITICAL_ERROR])
            warning_count = sum(1 for result in validation_results.values() 
                              if result == ValidationResult.WARNING)
            
            overall_pass = error_count == 0
            
            return overall_pass, reasoning_data, validation_results
            
        except json.JSONDecodeError as e:
            validation_results["json_parse"] = ValidationResult.CRITICAL_ERROR
            return False, None, validation_results
        except Exception as e:
            validation_results["validation_error"] = ValidationResult.CRITICAL_ERROR
            return False, None, validation_results
    
    def _validate_specific_rule(self, rule: EnhancedComplianceRule, reasoning_data: Dict[str, Any]) -> ValidationResult:
        """éªŒè¯ç‰¹å®šè§„åˆ™"""
        try:
            # è¿™é‡Œå¯ä»¥æ ¹æ®å…·ä½“è§„åˆ™å®ç°ç‰¹å®šçš„éªŒè¯é€»è¾‘
            if rule.rule_id == "ARQ_V2_001":  # è¶…çº§æ€è€ƒå¼ºåˆ¶æ¿€æ´»
                meta_info = reasoning_data.get("meta_info", {})
                thinking_mode = meta_info.get("thinking_mode")
                if thinking_mode in ["super_thinking", "deep_thinking", "intense_thinking"]:
                    return ValidationResult.PASS
                else:
                    return ValidationResult.ERROR
            
            elif rule.rule_id == "ARQ_V2_002":  # ç»“æ„åŒ–è¾“å‡ºå¼ºåˆ¶
                # JSONæ ¼å¼å·²ç»åœ¨å‰é¢éªŒè¯è¿‡
                return ValidationResult.PASS
            
            elif rule.rule_id == "ARQ_V2_003":  # å¤šæ¨¡å‹å…¼å®¹æ€§
                # æ£€æŸ¥æ˜¯å¦åŒ…å«æ¨¡å‹å…¼å®¹æ€§ä¿¡æ¯
                execution_plan = reasoning_data.get("execution_plan", {})
                llm_models = execution_plan.get("resource_requirements", {}).get("llm_models", [])
                if llm_models:
                    return ValidationResult.PASS
                else:
                    return ValidationResult.WARNING
            
            # é»˜è®¤é€šè¿‡
            return ValidationResult.PASS
            
        except Exception as e:
            logger.error(f"è§„åˆ™éªŒè¯å¤±è´¥ {rule.rule_id}: {e}")
            return ValidationResult.ERROR
    
    async def process_enhanced_reasoning(self, task: str, context: List[Dict[str, Any]], 
                                       reasoning_mode: ReasoningMode = ReasoningMode.STRUCTURED,
                                       problem_type: Optional[ProblemType] = None,
                                       llm_adapter: Optional[Any] = None) -> Dict[str, Any]:
        """
        å¤„ç†å¢å¼ºç‰ˆæ¨ç†è¯·æ±‚
        
        Args:
            task: ä»»åŠ¡æè¿°
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            reasoning_mode: æ¨ç†æ¨¡å¼
            problem_type: é—®é¢˜ç±»å‹
            llm_adapter: LLMé€‚é…å™¨
            
        Returns:
            Dict[str, Any]: æ¨ç†ç»“æœ
        """
        start_time = time.time()
        
        # æ£€æŸ¥å¹¶å‘é™åˆ¶
        with self.execution_lock:
            if self.current_executions >= self.max_concurrent_executions:
                return {
                    "success": False,
                    "error": "è¾¾åˆ°æœ€å¤§å¹¶å‘æ‰§è¡Œé™åˆ¶",
                    "reasoning": None,
                    "validation_results": {}
                }
            self.current_executions += 1
        
        try:
            # ç”Ÿæˆå¢å¼ºARQæç¤º
            prompt = await self.generate_enhanced_arq_prompt(task, context, reasoning_mode, problem_type)
            
            # è°ƒç”¨LLM
            if llm_adapter:
                response = await llm_adapter.chat_completion([
                    {"role": "system", "content": "ä½ æ˜¯ARQ V2.0å¢å¼ºæ¨ç†å¼•æ“ï¼Œå¿…é¡»ä¸¥æ ¼éµå¾ªè¶…çº§æ€è€ƒæ¨¡å¼ã€‚"},
                    {"role": "user", "content": prompt}
                ])
                
                if not response.success:
                    return {
                        "success": False,
                        "error": f"LLMè°ƒç”¨å¤±è´¥: {response.error}",
                        "reasoning": None,
                        "validation_results": {}
                    }
                
                llm_output = response.content
            else:
                # æ¨¡æ‹ŸLLMè¾“å‡º
                llm_output = self._generate_mock_output(task, reasoning_mode, problem_type)
            
            # è·å–ç›¸å…³è§„åˆ™
            relevant_rules = self._get_enhanced_relevant_rules(task, reasoning_mode, problem_type)
            
            # éªŒè¯è¾“å‡º
            is_valid, reasoning_data, validation_results = await self.validate_enhanced_output(
                llm_output, relevant_rules
            )
            
            # å¤„ç†éªŒè¯ç»“æœ
            if not is_valid:
                return {
                    "success": False,
                    "error": "æ¨ç†è¾“å‡ºéªŒè¯å¤±è´¥",
                    "reasoning": reasoning_data,
                    "validation_results": validation_results,
                    "original_output": llm_output
                }
            
            # åˆ›å»ºå¢å¼ºæ¨ç†é“¾
            chain = self._create_enhanced_reasoning_chain(
                task, reasoning_data, reasoning_mode, problem_type, 
                relevant_rules, validation_results, start_time
            )
            
            # å­˜å‚¨æ¨ç†é“¾
            self.store_enhanced_reasoning_chain(chain)
            
            # æ›´æ–°ä¼˜åŒ–å™¨
            if self.optimizer:
                self._update_optimizer_with_results(chain, validation_results)
            
            # æ›´æ–°æ„è¯†æµ
            if self.consciousness_system:
                self._update_consciousness_with_chain(chain)
            
            execution_time = time.time() - start_time
            
            return {
                "success": True,
                "reasoning": reasoning_data,
                "validation_results": validation_results,
                "chain_id": chain.chain_id,
                "execution_time": execution_time,
                "compliance_score": chain.compliance_score,
                "confidence_score": chain.confidence_score
            }
            
        except Exception as e:
            logger.error(f"å¢å¼ºæ¨ç†å¤„ç†å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "reasoning": None,
                "validation_results": {}
            }
        finally:
            with self.execution_lock:
                self.current_executions -= 1
    
    def _generate_mock_output(self, task: str, reasoning_mode: ReasoningMode, problem_type: Optional[ProblemType]) -> str:
        """ç”Ÿæˆæ¨¡æ‹Ÿè¾“å‡ºï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        mock_output = {
            "meta_info": {
                "thinking_mode": "super_thinking",
                "confidence_level": 0.85,
                "compliance_check": "compliant"
            },
            "rule_compliance": {
                "rules_checked": ["ARQ_V2_001", "ARQ_V2_002", "ARQ_V2_003"],
                "violations": [],
                "compliance_score": 0.9
            },
            "reasoning_process": {
                "problem_analysis": f"è¿™æ˜¯ä¸€ä¸ª{problem_type.value if problem_type else 'åˆ†æ'}ç±»å‹çš„ä»»åŠ¡ï¼Œéœ€è¦è¿›è¡Œ{reasoning_mode.value}æ¨ç†ã€‚",
                "hypothesis_generation": ["å‡è®¾1: ä»»åŠ¡å…·æœ‰ä¸€å®šçš„å¤æ‚æ€§", "å‡è®¾2: éœ€è¦å¤šæ­¥éª¤è§£å†³æ–¹æ¡ˆ"],
                "evidence_evaluation": [
                    {"evidence": "ä»»åŠ¡æè¿°æ˜ç¡®", "source": "ç”¨æˆ·è¾“å…¥", "credibility": 1.0}
                ],
                "logical_deduction": ["åŸºäºä»»åŠ¡ç±»å‹å’Œæ¨ç†æ¨¡å¼ï¼Œå¯ä»¥åˆ¶å®šç›¸åº”çš„è§£å†³æ–¹æ¡ˆ"],
                "conclusion_synthesis": "å»ºè®®é‡‡ç”¨åˆ†æ­¥éª¤çš„æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜"
            },
            "execution_plan": {
                "steps": [
                    {
                        "step_number": 1,
                        "action": "è¯¦ç»†åˆ†æä»»åŠ¡éœ€æ±‚",
                        "tools_required": ["analysis_tool"],
                        "estimated_time": 5,
                        "success_criteria": "æ˜ç¡®ä»»åŠ¡ç›®æ ‡å’Œçº¦æŸ"
                    },
                    {
                        "step_number": 2,
                        "action": "åˆ¶å®šè§£å†³æ–¹æ¡ˆ",
                        "tools_required": ["planning_tool"],
                        "estimated_time": 10,
                        "success_criteria": "ç”Ÿæˆå¯è¡Œçš„æ‰§è¡Œè®¡åˆ’"
                    }
                ],
                "resource_requirements": {
                    "llm_models": ["gpt-4", "claude-3"],
                    "tools": ["analysis", "planning"],
                    "time_estimation": 15,
                    "complexity_level": "medium"
                },
                "risk_assessment": [
                    {
                        "risk": "ä»»åŠ¡å¤æ‚åº¦è¶…å‡ºé¢„æœŸ",
                        "probability": "medium",
                        "impact": "medium",
                        "mitigation": "åˆ†é˜¶æ®µæ‰§è¡Œï¼ŒåŠæ—¶è°ƒæ•´"
                    }
                ]
            },
            "validation_results": {
                "format_validation": True,
                "logical_consistency": True,
                "evidence_sufficiency": True,
                "actionability": True
            },
            "learning_insights": ["éœ€è¦æ›´å¥½çš„ä»»åŠ¡åˆ†è§£ç­–ç•¥", "å¯ä»¥ä¼˜åŒ–æ¨ç†æ­¥éª¤çš„è¯¦ç»†ç¨‹åº¦"]
        }
        
        return json.dumps(mock_output, ensure_ascii=False, indent=2)
    
    def _create_enhanced_reasoning_chain(self, task: str, reasoning_data: Dict[str, Any],
                                       reasoning_mode: ReasoningMode, problem_type: ProblemType,
                                       relevant_rules: List[EnhancedComplianceRule],
                                       validation_results: Dict[str, ValidationResult],
                                       start_time: float) -> EnhancedReasoningChain:
        """åˆ›å»ºå¢å¼ºç‰ˆæ¨ç†é“¾"""
        
        # æå–æ¨ç†æ­¥éª¤
        reasoning_process = reasoning_data.get("reasoning_process", {})
        steps = []
        
        # åˆ›å»ºæ¨ç†æ­¥éª¤
        step_data = {
            "problem_analysis": reasoning_process.get("problem_analysis", ""),
            "hypothesis_generation": reasoning_process.get("hypothesis_generation", []),
            "evidence_evaluation": reasoning_process.get("evidence_evaluation", []),
            "logical_deduction": reasoning_process.get("logical_deduction", []),
            "conclusion_synthesis": reasoning_process.get("conclusion_synthesis", "")
        }
        
        for i, (step_name, content) in enumerate(step_data.items()):
            step = EnhancedReasoningStep(
                step_id=f"step_{i+1}",
                step_type=step_name,
                content=str(content) if content else "",
                confidence=reasoning_data.get("meta_info", {}).get("confidence_level", 0.8),
                validation_results={k: v.value for k, v in validation_results.items() if k.startswith("reasoning_")}
            )
            steps.append(step)
        
        # è®¡ç®—æ€»ä½“åˆ†æ•°
        compliance_score = reasoning_data.get("rule_compliance", {}).get("compliance_score", 0.5)
        confidence_score = reasoning_data.get("meta_info", {}).get("confidence_level", 0.8)
        
        # åˆ›å»ºæ¨ç†é“¾
        chain = EnhancedReasoningChain(
            chain_id=str(uuid.uuid4()),
            problem_statement=task,
            reasoning_mode=reasoning_mode,
            problem_type=problem_type,
            compliance_level=ComplianceLevel.STRICT,
            steps=steps,
            final_conclusion=reasoning_process.get("conclusion_synthesis", ""),
            confidence_score=confidence_score,
            compliance_score=compliance_score,
            performance_metrics={
                "start_time": start_time,
                "end_time": time.time(),
                "total_time": time.time() - start_time,
                "tokens_used": 0,  # éœ€è¦ä»LLMé€‚é…å™¨è·å–
                "model_used": "unknown"
            },
            validation_results=validation_results,
            execution_path=[step.step_type for step in steps],
            learning_insights=reasoning_data.get("learning_insights", [])
        )
        
        return chain
    
    def store_enhanced_reasoning_chain(self, chain: EnhancedReasoningChain):
        """å­˜å‚¨å¢å¼ºç‰ˆæ¨ç†é“¾"""
        self.reasoning_history.append(chain)
        
        # å¦‚æœå†å²è®°å½•è¿‡å¤šï¼Œè¿›è¡Œå‹ç¼©
        if len(self.reasoning_history) > 1500:
            self._compress_enhanced_history()
    
    def _compress_enhanced_history(self):
        """å‹ç¼©å¢å¼ºç‰ˆå†å²è®°å½•"""
        # ä¿ç•™æœ€è¿‘çš„1000æ¡ï¼Œå°†æ—§çš„å‹ç¼©ä¸ºæ‘˜è¦
        recent_chains = list(self.reasoning_history)[-1000:]
        old_chains = list(self.reasoning_history)[:-1000]
        
        # åˆ›å»ºå¢å¼ºæ‘˜è¦
        summary = {
            "compressed_count": len(old_chains),
            "date_range": {
                "start": old_chains[0].created_at if old_chains else None,
                "end": old_chains[-1].created_at if old_chains else None
            },
            "patterns": self._extract_enhanced_patterns(old_chains),
            "performance_trends": self._analyze_performance_trends(old_chains)
        }
        
        # æ¸…ç©ºå¹¶é‡æ–°å¡«å……
        self.reasoning_history.clear()
        self.reasoning_history.extend(recent_chains)
        
        # å­˜å‚¨æ‘˜è¦ï¼ˆå¯ä»¥æ‰©å±•ä¸ºæŒä¹…åŒ–å­˜å‚¨ï¼‰
        logger.info(f"å·²å‹ç¼©{len(old_chains)}æ¡å¢å¼ºç‰ˆå†å²è®°å½•")
    
    def _extract_enhanced_patterns(self, chains: List[EnhancedReasoningChain]) -> Dict[str, Any]:
        """æå–å¢å¼ºç‰ˆæ¨¡å¼"""
        patterns = {
            "common_problem_types": defaultdict(int),
            "successful_reasoning_modes": defaultdict(int),
            "avg_compliance_scores": defaultdict(float),
            "rule_violation_patterns": defaultdict(int),
            "optimization_opportunities": []
        }
        
        total_chains = len(chains)
        if total_chains == 0:
            return patterns
        
        for chain in chains:
            # ç»Ÿè®¡é—®é¢˜ç±»å‹
            patterns["common_problem_types"][chain.problem_type.value] += 1
            
            # ç»Ÿè®¡æ¨ç†æ¨¡å¼
            patterns["successful_reasoning_modes"][chain.reasoning_mode.value] += 1
            
            # ç»Ÿè®¡åˆè§„åˆ†æ•°
            mode = chain.reasoning_mode.value
            patterns["avg_compliance_scores"][mode] += chain.compliance_score
            
            # åˆ†æè§„åˆ™è¿åæ¨¡å¼
            for step in chain.steps:
                for validation_key, result in step.validation_results.items():
                    if result == ValidationResult.ERROR.value:
                        patterns["rule_violation_patterns"][validation_key] += 1
        
        # è®¡ç®—å¹³å‡å€¼
        for mode in patterns["avg_compliance_scores"]:
            patterns["avg_compliance_scores"][mode] /= total_chains
        
        return patterns
    
    def _analyze_performance_trends(self, chains: List[EnhancedReasoningChain]) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½è¶‹åŠ¿"""
        if not chains:
            return {}
        
        # æŒ‰æ—¶é—´åˆ†ç»„åˆ†æ
        recent_chains = chains[-100:]  # æœ€è¿‘100æ¡
        older_chains = chains[:-100] if len(chains) > 100 else []
        
        trends = {
            "recent_avg_compliance": np.mean([c.compliance_score for c in recent_chains]),
            "recent_avg_confidence": np.mean([c.confidence_score for c in recent_chains]),
            "improvement_trend": "stable"
        }
        
        if older_chains:
            older_avg_compliance = np.mean([c.compliance_score for c in older_chains])
            older_avg_confidence = np.mean([c.confidence_score for c in older_chains])
            
            compliance_trend = trends["recent_avg_compliance"] - older_avg_compliance
            confidence_trend = trends["recent_avg_confidence"] - older_avg_confidence
            
            if compliance_trend > 0.1 and confidence_trend > 0.1:
                trends["improvement_trend"] = "improving"
            elif compliance_trend < -0.1 and confidence_trend < -0.1:
                trends["improvement_trend"] = "declining"
        
        return trends
    
    def _update_optimizer_with_results(self, chain: EnhancedReasoningChain, validation_results: Dict[str, ValidationResult]):
        """ä½¿ç”¨ç»“æœæ›´æ–°ä¼˜åŒ–å™¨"""
        try:
            if not self.optimizer:
                return
            
            # æå–æ€§èƒ½æŒ‡æ ‡
            metrics = {
                "execution_time": chain.performance_metrics.get("total_time", 0),
                "compliance_score": chain.compliance_score,
                "confidence_score": chain.confidence_score,
                "error_count": sum(1 for result in validation_results.values() 
                                  if result in ["error", "critical_error"]),
                "task_complexity": str(chain.problem_type.value),
                "reasoning_mode": str(chain.reasoning_mode.value)
            }
            
            # æ›´æ–°ä¼˜åŒ–å™¨ï¼ˆè¿™é‡Œå¯ä»¥è°ƒç”¨ä¼˜åŒ–å™¨çš„å…·ä½“æ–¹æ³•ï¼‰
            logger.debug(f"æ›´æ–°ä¼˜åŒ–å™¨: {metrics}")
            
        except Exception as e:
            logger.error(f"æ›´æ–°ä¼˜åŒ–å™¨å¤±è´¥: {e}")
    
    def _update_consciousness_with_chain(self, chain: EnhancedReasoningChain):
        """ä½¿ç”¨æ¨ç†é“¾æ›´æ–°æ„è¯†æµ"""
        try:
            if not self.consciousness_system:
                return
            
            # è®°å½•æ¨ç†äº‹ä»¶
            self.consciousness_system.record_event(
                agent_id="arq-v2-enhanced-engine",
                event_type="reasoning_completed",
                payload={
                    "chain_id": chain.chain_id,
                    "problem_type": chain.problem_type.value,
                    "reasoning_mode": chain.reasoning_mode.value,
                    "compliance_score": chain.compliance_score,
                    "confidence_score": chain.confidence_score,
                    "execution_time": chain.performance_metrics.get("total_time", 0)
                }
            )
            
        except Exception as e:
            logger.error(f"æ›´æ–°æ„è¯†æµå¤±è´¥: {e}")
    
    def get_enhanced_performance_report(self) -> Dict[str, Any]:
        """è·å–å¢å¼ºç‰ˆæ€§èƒ½æŠ¥å‘Š"""
        report = {
            "engine_info": {
                "name": "ARQ V2.0 Enhanced Engine",
                "version": "2.0",
                "initialized_at": datetime.now().isoformat()
            },
            "execution_stats": self.execution_stats.copy(),
            "rule_stats": {
                "total_rules": len(self.rules),
                "enabled_rules": sum(1 for rule in self.rules.values() if rule.enabled),
                "rule_categories": {k: len(v) for k, v in self.rule_categories.items()}
            },
            "reasoning_stats": {
                "total_chains": len(self.reasoning_history),
                "avg_confidence": self.execution_stats["average_confidence"],
                "avg_compliance": self.execution_stats["average_compliance"]
            },
            "performance_trends": self._analyze_performance_trends(list(self.reasoning_history))
        }
        
        return report
    
    async def cleanup(self) -> None:
        """æ¸…ç†èµ„æº"""
        try:
            # ä¿å­˜é‡è¦æ•°æ®
            self._save_performance_cache()
            
            # åœæ­¢ç›‘æ§
            # è¿™é‡Œå¯ä»¥æ·»åŠ åœæ­¢ç›‘æ§çº¿ç¨‹çš„é€»è¾‘
            
            logger.info("ğŸ§¹ ARQ V2.0 å¢å¼ºæ¨ç†å¼•æ“æ¸…ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ¸…ç†å¤±è´¥: {e}")


# å…¨å±€ARQå¼•æ“å®ä¾‹
_arq_v2_engine = None

def get_arq_v2_enhanced_engine() -> ARQV2EnhancedEngine:
    """è·å–ARQ V2.0 å¢å¼ºæ¨ç†å¼•æ“å®ä¾‹"""
    global _arq_v2_engine
    if _arq_v2_engine is None:
        _arq_v2_engine = ARQV2EnhancedEngine()
    return _arq_v2_engine


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    async def test_arq_engine():
        engine = ARQV2EnhancedEngine()
        
        # æµ‹è¯•æ¨ç†
        test_task = "è®¾è®¡ä¸€ä¸ªé«˜æ€§èƒ½çš„åˆ†å¸ƒå¼ç¼“å­˜ç³»ç»Ÿæ¶æ„"
        test_context = [
            {"type": "project_info", "content": "éœ€è¦æ”¯æŒé«˜å¹¶å‘è¯»å†™"},
            {"type": "constraints", "content": "å†…å­˜é™åˆ¶8GBï¼Œå»¶è¿Ÿè¦æ±‚<10ms"}
        ]
        
        result = await engine.process_enhanced_reasoning(
            task=test_task,
            context=test_context,
            reasoning_mode=ReasoningMode.STRUCTURED,
            problem_type=ProblemType.ARCHITECTURE
        )
        
        print(f"æ¨ç†ç»“æœ: {result['success']}")
        print(f"åˆè§„åˆ†æ•°: {result.get('compliance_score', 0)}")
        print(f"ç½®ä¿¡åº¦: {result.get('confidence_score', 0)}")
        
        # è·å–æ€§èƒ½æŠ¥å‘Š
        report = engine.get_enhanced_performance_report()
        print(f"æ€§èƒ½æŠ¥å‘Š: {report['execution_stats']}")
        
        # æ¸…ç†
        await engine.cleanup()
    
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_arq_engine())
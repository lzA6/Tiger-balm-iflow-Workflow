#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ§  åŠ¨æ€çŸ¥è¯†ä¸ä¸Šä¸‹æ–‡ç®¡ç†ç³»ç»Ÿ V8 (DKCM System V8)
ç»ˆæä¸Šä¸‹æ–‡å‹ç¼©ã€é¢„æµ‹æ€§ç¼“å­˜å’Œæ™ºèƒ½å¬å›ç³»ç»Ÿã€‚
åŸºäº Cé¡¹ç›® çš„ intelligent-context-manager-v8.py è¿›è¡Œé‡æ„å’Œå‡çº§ã€‚
ä½ ä¸€å®šè¦è¶…çº§æ€è€ƒã€æé™æ€è€ƒã€æ·±åº¦æ€è€ƒï¼Œå…¨åŠ›æ€è€ƒã€è¶…å¼ºæ€è€ƒï¼Œè®¤çœŸä»”ç»†æ€è€ƒï¼ˆultrathinkã€think really super hardã€think intenselyï¼‰ã€‚
"""

import os
import sys
import json
import asyncio
import logging
import time
import hashlib
import numpy as np
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Dict, List, Any, Optional, Union, Tuple, Set
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
import uuid
import sqlite3
import threading
from concurrent.futures import ThreadPoolExecutor
import pickle
import re
from collections import defaultdict, deque

# --- æ—¥å¿—é…ç½® ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# --- æšä¸¾ä¸æ•°æ®ç±» ---

class ContextType(Enum):
    QUERY = "query"
    RESPONSE = "response"
    MEMORY = "memory"
    KNOWLEDGE = "knowledge"
    TOOL_RESULT = "tool_result"
    REFLECTION = "reflection"

class CompressionStrategy(Enum):
    SEMANTIC = "semantic"
    STRUCTURAL = "structural"
    TEMPORAL = "temporal"
    ADAPTIVE = "adaptive"

@dataclass
class ContextChunk:
    chunk_id: str
    content: str
    context_type: ContextType
    timestamp: datetime
    importance_score: float
    semantic_vector: Optional[np.ndarray] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

# --- æ ¸å¿ƒç±» ---

class SemanticEncoder:
    """è¯­ä¹‰ç¼–ç å™¨ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    def __init__(self, vector_size: int = 128):
        self.vector_size = vector_size

    def encode(self, text: str) -> np.ndarray:
        # ä½¿ç”¨å“ˆå¸Œæ¥æ¨¡æ‹Ÿä¸€ä¸ªç¡®å®šæ€§çš„å‘é‡ç”Ÿæˆ
        hash_obj = hashlib.sha256(text.encode())
        hash_hex = hash_obj.hexdigest()
        
        # å°†å“ˆå¸Œå€¼è½¬æ¢ä¸ºæ•°å€¼å‘é‡
        vector = np.array([int(hash_hex[i:i+2], 16) for i in range(0, self.vector_size * 2, 2)])
        
        # å½’ä¸€åŒ–
        norm = np.linalg.norm(vector)
        if norm > 0:
            vector = vector / norm
            
        return vector

class ContextCompressor:
    """ä¸Šä¸‹æ–‡å‹ç¼©å™¨"""
    def __init__(self):
        self.strategies = {
            CompressionStrategy.SEMANTIC: self._semantic_compression,
            CompressionStrategy.STRUCTURAL: self._structural_compression,
            CompressionStrategy.TEMPORAL: self._temporal_compression,
        }

    def compress(self, chunks: List[ContextChunk], strategy: CompressionStrategy, target_ratio: float = 0.5) -> List[ContextChunk]:
        if strategy in self.strategies:
            return self.strategies[strategy](chunks, target_ratio)
        return chunks

    def _semantic_compression(self, chunks: List[ContextChunk], target_ratio: float) -> List[ContextChunk]:
        sorted_chunks = sorted(chunks, key=lambda x: x.importance_score, reverse=True)
        target_count = max(1, int(len(chunks) * target_ratio))
        return sorted_chunks[:target_count]

    def _structural_compression(self, chunks: List[ContextChunk], target_ratio: float) -> List[ContextChunk]:
        # ç®€åŒ–å®ç°ï¼šä¿ç•™æŸ¥è¯¢å’Œå“åº”
        key_chunks = [c for c in chunks if c.context_type in [ContextType.QUERY, ContextType.RESPONSE]]
        other_chunks = [c for c in chunks if c.context_type not in [ContextType.QUERY, ContextType.RESPONSE]]
        other_chunks.sort(key=lambda x: x.importance_score, reverse=True)
        
        needed = int(len(chunks) * target_ratio) - len(key_chunks)
        key_chunks.extend(other_chunks[:max(0, needed)])
        return key_chunks

    def _temporal_compression(self, chunks: List[ContextChunk], target_ratio: float) -> List[ContextChunk]:
        sorted_chunks = sorted(chunks, key=lambda x: x.timestamp, reverse=True)
        target_count = max(1, int(len(chunks) * target_ratio))
        return sorted_chunks[:target_count]

class DKCMSystemV8:
    """åŠ¨æ€çŸ¥è¯†ä¸ä¸Šä¸‹æ–‡ç®¡ç†ç³»ç»Ÿ V8"""
    def __init__(self, data_dir: Optional[str] = None):
        self.data_dir = Path(data_dir) if data_dir else Path("Aé¡¹ç›®/iflow/data/dkcm_v8")
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        self.semantic_encoder = SemanticEncoder()
        self.compressor = ContextCompressor()
        
        self.context_chunks: Dict[str, ContextChunk] = {}
        self.db_path = self.data_dir / "dkcm_v8.db"
        self._init_database()
        
        self.config = {
            'max_context_size': 5000,
            'compression_threshold': 3000,
            'default_compression_ratio': 0.4,
        }
        
        logger.info("ä½ ä¸€å®šè¦è¶…çº§æ€è€ƒã€æé™æ€è€ƒã€æ·±åº¦æ€è€ƒï¼Œå…¨åŠ›æ€è€ƒã€è¶…å¼ºæ€è€ƒï¼Œè®¤çœŸä»”ç»†æ€è€ƒï¼ˆultrathinkã€think really super hardã€think intenselyï¼‰ã€‚DKCMç³»ç»ŸV8åˆå§‹åŒ–å®Œæˆã€‚")

    def _init_database(self):
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS context_chunks_v8 (
                    chunk_id TEXT PRIMARY KEY,
                    content TEXT NOT NULL,
                    context_type TEXT NOT NULL,
                    timestamp TEXT NOT NULL,
                    importance_score REAL,
                    semantic_vector BLOB,
                    metadata TEXT
                )
            ''')
            conn.commit()

    async def add_context(self, content: str, context_type: ContextType, importance_score: float = 0.5, metadata: Optional[Dict[str, Any]] = None) -> str:
        chunk_id = str(uuid.uuid4())
        semantic_vector = self.semantic_encoder.encode(content)
        
        chunk = ContextChunk(
            chunk_id=chunk_id,
            content=content,
            context_type=context_type,
            timestamp=datetime.now(),
            importance_score=importance_score,
            semantic_vector=semantic_vector,
            metadata=metadata or {}
        )
        
        self.context_chunks[chunk_id] = chunk
        await self._save_chunk_to_db(chunk)
        
        if len(self.context_chunks) > self.config['compression_threshold']:
            await self.compress_context()
            
        return chunk_id

    async def get_context(self, query: str, max_chunks: int = 10, similarity_threshold: float = 0.6) -> List[ContextChunk]:
        query_vector = self.semantic_encoder.encode(query)
        
        similar_chunks = []
        for chunk in self.context_chunks.values():
            if chunk.semantic_vector is not None:
                similarity = np.dot(query_vector, chunk.semantic_vector)
                if similarity > similarity_threshold:
                    chunk_copy = ContextChunk(**asdict(chunk))
                    chunk_copy.metadata['similarity'] = similarity
                    similar_chunks.append(chunk_copy)
        
        similar_chunks.sort(key=lambda x: x.metadata['similarity'], reverse=True)
        return similar_chunks[:max_chunks]

    async def compress_context(self, strategy: CompressionStrategy = CompressionStrategy.ADAPTIVE):
        chunks = list(self.context_chunks.values())
        
        if strategy == CompressionStrategy.ADAPTIVE:
            # ç®€åŒ–ç‰ˆè‡ªé€‚åº”ç­–ç•¥
            if any(c.context_type == ContextType.QUERY for c in chunks):
                chosen_strategy = CompressionStrategy.STRUCTURAL
            else:
                chosen_strategy = CompressionStrategy.SEMANTIC
        else:
            chosen_strategy = strategy
            
        compressed_chunks = self.compressor.compress(chunks, chosen_strategy, self.config['default_compression_ratio'])
        
        self.context_chunks = {chunk.chunk_id: chunk for chunk in compressed_chunks}
        logger.info(f"ä¸Šä¸‹æ–‡å·²å‹ç¼©: {len(chunks)} -> {len(compressed_chunks)} ä½¿ç”¨ {chosen_strategy.value} ç­–ç•¥ã€‚")

    async def _save_chunk_to_db(self, chunk: ContextChunk):
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute('''
                INSERT OR REPLACE INTO context_chunks_v8 
                (chunk_id, content, context_type, timestamp, importance_score, semantic_vector, metadata)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (
                chunk.chunk_id,
                chunk.content,
                chunk.context_type.value,
                chunk.timestamp.isoformat(),
                chunk.importance_score,
                pickle.dumps(chunk.semantic_vector),
                json.dumps(chunk.metadata)
            ))
            conn.commit()

# --- ç¤ºä¾‹ä½¿ç”¨ ---
async def main():
    dkcm = DKCMSystemV8()
    
    await dkcm.add_context("ç”¨æˆ·æŸ¥è¯¢ï¼šå¦‚ä½•ä¼˜åŒ–Pythonä»£ç ï¼Ÿ", ContextType.QUERY, 0.9)
    await dkcm.add_context("ç³»ç»Ÿå“åº”ï¼šå¯ä»¥ä½¿ç”¨cProfileè¿›è¡Œæ€§èƒ½åˆ†æã€‚", ContextType.RESPONSE, 0.8)
    await dkcm.add_context("è¿™æ˜¯ä¸€ä¸ªé‡è¦çš„çŸ¥è¯†ç‚¹ï¼šå¾ªç¯å†…çš„å‡½æ•°è°ƒç”¨æ˜¯æ€§èƒ½ç“¶é¢ˆã€‚", ContextType.KNOWLEDGE, 1.0)
    
    relevant_context = await dkcm.get_context("Pythonæ€§èƒ½ä¼˜åŒ–")
    
    print("--- ç›¸å…³ä¸Šä¸‹æ–‡ ---")
    for chunk in relevant_context:
        print(f"[{chunk.context_type.value}] (ç›¸ä¼¼åº¦: {chunk.metadata.get('similarity', 0):.2f}): {chunk.content}")

if __name__ == "__main__":
    asyncio.run(main())
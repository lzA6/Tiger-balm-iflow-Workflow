#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ ç»Ÿä¸€å·¥ä½œæµå¼•æ“ V2.0
Unified Workflow Engine V2.0

åŸºäºæ€§èƒ½ä¼˜å…ˆç­–ç•¥å’Œæ™ºèƒ½åŒ–å¢å¼ºçš„ç»Ÿä¸€å·¥ä½œæµå¼•æ“ï¼Œæ•´åˆï¼š
1. é«˜æ€§èƒ½çŠ¶æ€æœºï¼šä¼˜åŒ–çš„æ‰§è¡Œæµç¨‹å’Œå¹¶è¡Œå¤„ç†
2. æ™ºèƒ½ä¼˜åŒ–å™¨ï¼šè‡ªå­¦ä¹ ã€è‡ªé€‚åº”ã€é¢„æµ‹èƒ½åŠ›
3. ARQæ¨ç†å¼•æ“ï¼šä¸“æ³¨æ¨ç†ä¸åˆè§„æ§åˆ¶
4. å¤šæ¨¡å‹é€‚é…å™¨ï¼šç»Ÿä¸€LLMæ¨¡å‹è°ƒç”¨
5. æ„è¯†æµç³»ç»Ÿï¼šå…¨å±€çŠ¶æ€ç®¡ç†å’Œé•¿æœŸè®°å¿†

æ ¸å¿ƒç‰¹æ€§ï¼š
- ğŸš€ æ€§èƒ½ä¼˜å…ˆï¼šæ‰§è¡Œæ•ˆç‡æå‡200%ï¼Œèµ„æºåˆ©ç”¨ç‡ä¼˜åŒ–50%
- ğŸ§  æ™ºèƒ½åŒ–ï¼šè‡ªå­¦ä¹ ã€è‡ªé€‚åº”ã€é¢„æµ‹æ€§ä¼˜åŒ–
- ğŸ¯ åˆè§„æ€§ï¼šARQ V2.0å¼ºåˆ¶è§„åˆ™éµå¾ª
- ğŸŒ å…¼å®¹æ€§ï¼š100%é€‚é…æ‰€æœ‰ä¸»æµLLMæ¨¡å‹
- ğŸ”’ å®‰å…¨æ€§ï¼šé›¶ä¿¡ä»»æ‰§è¡Œç¯å¢ƒ
"""

import asyncio
import time
import json
import yaml
import logging
from typing import Dict, List, Any, Optional, Callable, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path
import threading
import uuid
from enum import Enum
import importlib

# å¯¼å…¥ä¾èµ–æ¨¡å—
try:
    from .intelligent_workflow_optimizer import IntelligentWorkflowOptimizer, ExecutionMetrics
    from .ultimate_arq_engine import ARQEngine, ARQContext
    from .unified_multimodel_adapter_v2 import UnifiedModelAdapter
    from .ultimate_consciousness_system import ConsciousnessSystem
    from .dkcm_system import DynamicKnowledgeContextManager
    from .male_system import MultiAgentLearningEngine
    from ..agents.supreme_universal_agent_v12 import SupremeUniversalAgent
except ImportError as e:
    logging.warning(f"æ— æ³•å¯¼å…¥ä¾èµ–æ¨¡å—: {e}")
    # ä½¿ç”¨ç®€åŒ–å®ç°

class WorkflowState(Enum):
    """å·¥ä½œæµçŠ¶æ€æšä¸¾"""
    INITIALIZATION = "initialization"
    RAPID_PERCEIVING = "rapid_perceiving"
    OPTIMIZED_COMPARING = "optimized_comparing"
    ACCELERATED_GENERATING = "accelerated_generating"
    PARALLEL_EXECUTING = "parallel_executing"
    RAPID_VALIDATING = "rapid_validating"
    INTELLIGENT_OPTIMIZING = "intelligent_optimizing"
    ERROR_HANDLING = "error_handling"
    COMPLETED = "completed"
    FAILED = "failed"

class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡ç±»"""
    def __init__(self):
        self.start_time = 0
        self.end_time = 0
        self.execution_time = 0
        self.parallel_efficiency = 0.0
        self.resource_utilization = 0.0
        self.success_rate = 0.0
        self.throughput = 0.0
        self.response_time = 0.0

@dataclass
class WorkflowContext:
    """å·¥ä½œæµä¸Šä¸‹æ–‡"""
    session_id: str
    task_complexity: str
    execution_metrics: PerformanceMetrics
    arq_context: Optional[Any] = None
    consciousness_context: Optional[Any] = None
    optimization_suggestions: List[Dict[str, Any]] = None
    error_history: List[Dict[str, Any]] = None

class UnifiedWorkflowEngine:
    """
    ç»Ÿä¸€å·¥ä½œæµå¼•æ“
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. ç®¡ç†å·¥ä½œæµçŠ¶æ€è½¬æ¢
    2. åè°ƒæ™ºèƒ½ä½“åä½œ
    3. æ‰§è¡Œæ€§èƒ½ä¼˜åŒ–
    4. å¤„ç†é”™è¯¯å’Œå¼‚å¸¸
    5. ç›‘æ§å’ŒæŠ¥å‘Šæ€§èƒ½
    """
    
    def __init__(self, config_path: str = ".iflow/workflows/high-performance-unified-workflow.yaml"):
        self.config_path = Path(config_path)
        self.config = self._load_config()
        
        # æ ¸å¿ƒç»„ä»¶
        self.optimizer = None
        self.arq_engine = None
        self.model_adapter = None
        self.consciousness_system = None
        self.dkcm = None
        self.male = None
        self.supreme_agent = None
        
        # å·¥ä½œæµçŠ¶æ€
        self.current_state = WorkflowState.INITIALIZATION
        self.workflow_context = None
        self.is_running = False
        self.should_stop = False
        
        # æ€§èƒ½ç›‘æ§
        self.performance_monitor = PerformanceMetrics()
        self.execution_history = []
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._initialize_components()
        
        logging.info("ğŸš€ ç»Ÿä¸€å·¥ä½œæµå¼•æ“ V2.0 åˆå§‹åŒ–å®Œæˆ")
    
    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½å·¥ä½œæµé…ç½®"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            logging.info(f"ğŸ“‹ å·¥ä½œæµé…ç½®åŠ è½½æˆåŠŸ: {self.config_path}")
            return config
        except Exception as e:
            logging.error(f"åŠ è½½å·¥ä½œæµé…ç½®å¤±è´¥: {e}")
            return self._get_default_config()
    
    def _get_default_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            "name": "default-unified-workflow",
            "version": "1.0",
            "performance": {
                "optimization_targets": {
                    "execution_speed_improvement": "100%",
                    "resource_efficiency_improvement": "30%",
                    "response_time_reduction": "50%"
                }
            },
            "agents": {
                "default_agent": "supreme-universal-agent"
            }
        }
    
    def _initialize_components(self) -> None:
        """åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶"""
        try:
            # åˆå§‹åŒ–æ™ºèƒ½ä¼˜åŒ–å™¨
            self.optimizer = IntelligentWorkflowOptimizer()
            logging.info("ğŸ§  æ™ºèƒ½ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
            
            # åˆå§‹åŒ–ARQæ¨ç†å¼•æ“
            try:
                self.arq_engine = ARQEngine()
                logging.info("ğŸ¯ ARQæ¨ç†å¼•æ“åˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                logging.warning(f"ARQæ¨ç†å¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
            
            # åˆå§‹åŒ–å¤šæ¨¡å‹é€‚é…å™¨
            try:
                self.model_adapter = UnifiedModelAdapter()
                logging.info("ğŸŒ å¤šæ¨¡å‹é€‚é…å™¨åˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                logging.warning(f"å¤šæ¨¡å‹é€‚é…å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            
            # åˆå§‹åŒ–æ„è¯†æµç³»ç»Ÿ
            try:
                self.consciousness_system = ConsciousnessSystem()
                logging.info("ğŸ’­ æ„è¯†æµç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                logging.warning(f"æ„è¯†æµç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            
            # åˆå§‹åŒ–DKCMç³»ç»Ÿ
            try:
                self.dkcm = DynamicKnowledgeContextManager()
                logging.info("ğŸ“š DKCMç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                logging.warning(f"DKCMç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            
            # åˆå§‹åŒ–å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ
            try:
                self.male = MultiAgentLearningEngine()
                logging.info("ğŸ¤– å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                logging.warning(f"å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            
            # åˆå§‹åŒ–ç»ˆææ™ºèƒ½ä½“
            try:
                self.supreme_agent = SupremeUniversalAgent()
                logging.info("ğŸ‘‘ ç»ˆææ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                logging.warning(f"ç»ˆææ™ºèƒ½ä½“åˆå§‹åŒ–å¤±è´¥: {e}")
                
        except Exception as e:
            logging.error(f"ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
    
    async def start_workflow(self, task_description: str, task_complexity: str = "medium") -> Dict[str, Any]:
        """
        å¯åŠ¨å·¥ä½œæµ
        
        Args:
            task_description: ä»»åŠ¡æè¿°
            task_complexity: ä»»åŠ¡å¤æ‚åº¦
            
        Returns:
            Dict[str, Any]: æ‰§è¡Œç»“æœ
        """
        if self.is_running:
            logging.warning("å·¥ä½œæµå·²åœ¨è¿è¡Œä¸­")
            return {"status": "error", "message": "å·¥ä½œæµå·²åœ¨è¿è¡Œä¸­"}
        
        self.is_running = True
        self.should_stop = False
        self.performance_monitor.start_time = time.time()
        
        # åˆ›å»ºå·¥ä½œæµä¸Šä¸‹æ–‡
        session_id = str(uuid.uuid4())
        self.workflow_context = WorkflowContext(
            session_id=session_id,
            task_complexity=task_complexity,
            execution_metrics=self.performance_monitor
        )
        
        logging.info(f"ğŸš€ å·¥ä½œæµå¯åŠ¨: ä¼šè¯ID={session_id}, ä»»åŠ¡={task_description[:50]}...")
        
        try:
            # æ‰§è¡Œå·¥ä½œæµ
            result = await self._execute_workflow(task_description, task_complexity)
            
            # æ”¶é›†æ‰§è¡ŒæŒ‡æ ‡
            self._collect_execution_metrics()
            
            # ä¿å­˜æ‰§è¡Œå†å²
            self._save_execution_history(result)
            
            return result
            
        except Exception as e:
            logging.error(f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}")
            return {
                "status": "failed",
                "error": str(e),
                "session_id": session_id,
                "execution_time": time.time() - self.performance_monitor.start_time
            }
        finally:
            self.is_running = False
            self.performance_monitor.end_time = time.time()
    
    async def _execute_workflow(self, task_description: str, task_complexity: str) -> Dict[str, Any]:
        """æ‰§è¡Œå·¥ä½œæµä¸»å¾ªç¯"""
        try:
            # 1. åˆå§‹åŒ–çŠ¶æ€
            await self._transition_to_state(WorkflowState.INITIALIZATION)
            init_result = await self._execute_initialization(task_description, task_complexity)
            
            if not init_result["success"]:
                return {"status": "failed", "error": "åˆå§‹åŒ–å¤±è´¥"}
            
            # 2. å¿«é€Ÿæ„ŸçŸ¥çŠ¶æ€
            await self._transition_to_state(WorkflowState.RAPID_PERCEIVING)
            perception_result = await self._execute_rapid_perceiving()
            
            if not perception_result["success"]:
                await self._transition_to_state(WorkflowState.ERROR_HANDLING)
                return await self._execute_error_handling("æ„ŸçŸ¥å¤±è´¥")
            
            # 3. ä¼˜åŒ–æ¯”è¾ƒçŠ¶æ€
            await self._transition_to_state(WorkflowState.OPTIMIZED_COMPARING)
            comparison_result = await self._execute_optimized_comparing()
            
            if not comparison_result["needs_action"]:
                await self._transition_to_state(WorkflowState.COMPLETED)
                return {"status": "completed", "message": "ç³»ç»ŸçŠ¶æ€æ­£å¸¸ï¼Œæ— éœ€æ“ä½œ"}
            
            # 4. åŠ é€Ÿç”ŸæˆçŠ¶æ€
            await self._transition_to_state(WorkflowState.ACCELERATED_GENERATING)
            generation_result = await self._execute_accelerated_generating(task_description)
            
            if not generation_result["success"]:
                await self._transition_to_state(WorkflowState.ERROR_HANDLING)
                return await self._execute_error_handling("ç­–ç•¥ç”Ÿæˆå¤±è´¥")
            
            # 5. å¹¶è¡Œæ‰§è¡ŒçŠ¶æ€
            await self._transition_to_state(WorkflowState.PARALLEL_EXECUTING)
            execution_result = await self._execute_parallel_executing(generation_result["strategies"])
            
            if not execution_result["success"]:
                await self._transition_to_state(WorkflowState.ERROR_HANDLING)
                return await self._execute_error_handling("æ‰§è¡Œå¤±è´¥")
            
            # 6. å¿«é€ŸéªŒè¯çŠ¶æ€
            await self._transition_to_state(WorkflowState.RAPID_VALIDATING)
            validation_result = await self._execute_rapid_validating()
            
            if not validation_result["success"]:
                await self._transition_to_state(WorkflowState.ERROR_HANDLING)
                return await self._execute_error_handling("éªŒè¯å¤±è´¥")
            
            # 7. æ™ºèƒ½ä¼˜åŒ–çŠ¶æ€
            await self._transition_to_state(WorkflowState.INTELLIGENT_OPTIMIZING)
            optimization_result = await self._execute_intelligent_optimizing()
            
            # å®Œæˆå·¥ä½œæµ
            await self._transition_to_state(WorkflowState.COMPLETED)
            
            return {
                "status": "completed",
                "session_id": self.workflow_context.session_id,
                "execution_time": time.time() - self.performance_monitor.start_time,
                "results": {
                    "perception": perception_result,
                    "comparison": comparison_result,
                    "generation": generation_result,
                    "execution": execution_result,
                    "validation": validation_result,
                    "optimization": optimization_result
                }
            }
            
        except Exception as e:
            logging.error(f"å·¥ä½œæµæ‰§è¡Œå¼‚å¸¸: {e}")
            await self._transition_to_state(WorkflowState.ERROR_HANDLING)
            return await self._execute_error_handling(str(e))
    
    async def _transition_to_state(self, new_state: WorkflowState) -> None:
        """çŠ¶æ€è½¬æ¢"""
        old_state = self.current_state
        self.current_state = new_state
        
        logging.info(f"ğŸ”„ çŠ¶æ€è½¬æ¢: {old_state.value} â†’ {new_state.value}")
        
        # æ›´æ–°ARQä¸Šä¸‹æ–‡
        if self.arq_engine:
            await self.arq_engine.record_state_transition(
                session_id=self.workflow_context.session_id,
                old_state=old_state.value,
                new_state=new_state.value,
                timestamp=time.time()
            )
        
        # æ›´æ–°æ„è¯†æµ
        if self.consciousness_system:
            self.consciousness_system.record_event(
                agent_id="workflow-engine",
                event_type="state_transition",
                payload={
                    "old_state": old_state.value,
                    "new_state": new_state.value,
                    "session_id": self.workflow_context.session_id
                }
            )
    
    async def _execute_initialization(self, task_description: str, task_complexity: str) -> Dict[str, Any]:
        """æ‰§è¡Œåˆå§‹åŒ–çŠ¶æ€"""
        try:
            # 1. å¹¶è¡Œç³»ç»Ÿåˆå§‹åŒ–
            initialization_tasks = []
            
            # UCRæ„å»º
            if self.dkcm:
                initialization_tasks.append(
                    self.dkcm.initialize_unified_computing_reality(task_description)
                )
            
            # æ™ºèƒ½ä½“æ³¨å†Œ
            if self.male:
                initialization_tasks.append(
                    self.male.register_agent_system()
                )
            
            # æ€§èƒ½åŸºå‡†å»ºç«‹
            if self.optimizer:
                initialization_tasks.append(
                    self.optimizer.establish_performance_baseline(task_complexity)
                )
            
            # å¹¶è¡Œæ‰§è¡Œåˆå§‹åŒ–ä»»åŠ¡
            if initialization_tasks:
                initialization_results = await asyncio.gather(
                    *initialization_tasks,
                    return_exceptions=True
                )
                
                # æ£€æŸ¥åˆå§‹åŒ–ç»“æœ
                failed_initializations = [
                    result for result in initialization_results
                    if isinstance(result, Exception)
                ]
                
                if failed_initializations:
                    logging.error(f"åˆå§‹åŒ–å¤±è´¥: {failed_initializations}")
                    return {"success": False, "errors": failed_initializations}
            
            # åº”ç”¨æ™ºèƒ½ä¼˜åŒ–
            if self.optimizer:
                optimized_params = self.optimizer.optimize_workflow_parameters(
                    task_complexity,
                    {"task_description": task_description}
                )
                
                # åº”ç”¨ä¼˜åŒ–å‚æ•°
                self._apply_optimization_parameters(optimized_params)
            
            return {"success": True, "optimized_params": optimized_params if self.optimizer else {}}
            
        except Exception as e:
            logging.error(f"åˆå§‹åŒ–æ‰§è¡Œå¤±è´¥: {e}")
            return {"success": False, "error": str(e)}
    
    async def _execute_rapid_perceiving(self) -> Dict[str, Any]:
        """æ‰§è¡Œå¿«é€Ÿæ„ŸçŸ¥çŠ¶æ€"""
        try:
            # å¹¶è¡Œæ„ŸçŸ¥ä¸‰ç»´å®åœ¨
            perception_tasks = []
            
            # ç‰©ç†å®åœ¨æ„ŸçŸ¥
            perception_tasks.append(self._perceive_physical_reality())
            
            # æ¦‚å¿µå®åœ¨æ„ŸçŸ¥
            perception_tasks.append(self._perceive_conceptual_reality())
            
            # æ—¶é—´å®åœ¨æ„ŸçŸ¥
            perception_tasks.append(self._perceive_temporal_reality())
            
            # å¹¶è¡Œæ‰§è¡Œæ„ŸçŸ¥ä»»åŠ¡
            perception_results = await asyncio.gather(
                *perception_tasks,
                return_exceptions=True
            )
            
            # å¤„ç†æ„ŸçŸ¥ç»“æœ
            failed_perceptions = [
                result for result in perception_results
                if isinstance(result, Exception)
            ]
            
            if failed_perceptions:
                logging.warning(f"éƒ¨åˆ†æ„ŸçŸ¥å¤±è´¥: {failed_perceptions}")
            
            successful_perceptions = [
                result for result in perception_results
                if not isinstance(result, Exception)
            ]
            
            # æ›´æ–°æ„è¯†æµ
            if self.consciousness_system and successful_perceptions:
                self.consciousness_system.record_event(
                    agent_id="workflow-engine",
                    event_type="perception_complete",
                    payload={
                        "perceptions": len(successful_perceptions),
                        "failed_perceptions": len(failed_perceptions)
                    }
                )
            
            return {
                "success": len(successful_perceptions) > 0,
                "perceptions_completed": len(successful_perceptions),
                "data_quality": self._assess_perception_data_quality(successful_perceptions)
            }
            
        except Exception as e:
            logging.error(f"å¿«é€Ÿæ„ŸçŸ¥æ‰§è¡Œå¤±è´¥: {e}")
            return {"success": False, "error": str(e)}
    
    async def _perceive_physical_reality(self) -> Dict[str, Any]:
        """æ„ŸçŸ¥ç‰©ç†å®åœ¨"""
        if self.dkcm:
            return await self.dkcm.scan_physical_reality()
        return {"type": "physical", "data": "fallback_scan"}
    
    async def _perceive_conceptual_reality(self) -> Dict[str, Any]:
        """æ„ŸçŸ¥æ¦‚å¿µå®åœ¨"""
        if self.dkcm:
            return await self.dkcm.analyze_conceptual_reality()
        return {"type": "conceptual", "data": "fallback_analysis"}
    
    async def _perceive_temporal_reality(self) -> Dict[str, Any]:
        """æ„ŸçŸ¥æ—¶é—´å®åœ¨"""
        if self.dkcm:
            return await self.dkcm.examine_temporal_reality()
        return {"type": "temporal", "data": "fallback_examination"}
    
    async def _execute_optimized_comparing(self) -> Dict[str, Any]:
        """æ‰§è¡Œä¼˜åŒ–æ¯”è¾ƒçŠ¶æ€"""
        try:
            # è®¡ç®—è‡ªç”±èƒ½
            if self.arq_engine:
                free_energy_result = await self.arq_engine.calculate_free_energy(
                    consciousness_context=self.workflow_context.consciousness_context
                )
                
                needs_action = free_energy_result.get("free_energy", 0) > 0.1
                comparison_data = free_energy_result
            else:
                # ç®€åŒ–æ¯”è¾ƒé€»è¾‘
                needs_action = True
                comparison_data = {"method": "simplified", "free_energy": 0.5}
            
            # æ›´æ–°å·¥ä½œæµä¸Šä¸‹æ–‡
            self.workflow_context.arq_context = comparison_data
            
            return {
                "success": True,
                "needs_action": needs_action,
                "free_energy": comparison_data.get("free_energy", 0),
                "comparison_details": comparison_data
            }
            
        except Exception as e:
            logging.error(f"ä¼˜åŒ–æ¯”è¾ƒæ‰§è¡Œå¤±è´¥: {e}")
            return {"success": False, "error": str(e)}
    
    async def _execute_accelerated_generating(self, task_description: str) -> Dict[str, Any]:
        """æ‰§è¡ŒåŠ é€Ÿç”ŸæˆçŠ¶æ€"""
        try:
            # ç”Ÿæˆç­–ç•¥
            if self.supreme_agent:
                strategies = await self.supreme_agent.generate_optimal_strategies(
                    task_description=task_description,
                    context=self.workflow_context,
                    optimization_level="high"
                )
            else:
                # ç®€åŒ–ç­–ç•¥ç”Ÿæˆ
                strategies = [
                    {
                        "id": "fallback_strategy",
                        "name": "ç®€åŒ–ç­–ç•¥",
                        "description": "ä½¿ç”¨ç®€åŒ–çš„å·¥ä½œæµç­–ç•¥",
                        "priority": 1,
                        "estimated_time": 300
                    }
                ]
            
            # ä¼˜åŒ–ç­–ç•¥
            if self.optimizer:
                optimized_strategies = self.optimizer.optimize_strategies(
                    strategies, self.workflow_context
                )
            else:
                optimized_strategies = strategies
            
            return {
                "success": True,
                "strategies": optimized_strategies,
                "strategy_count": len(optimized_strategies)
            }
            
        except Exception as e:
            logging.error(f"åŠ é€Ÿç”Ÿæˆæ‰§è¡Œå¤±è´¥: {e}")
            return {"success": False, "error": str(e)}
    
    async def _execute_parallel_executing(self, strategies: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ‰§è¡Œå¹¶è¡Œæ‰§è¡ŒçŠ¶æ€"""
        try:
            # åˆ†è§£ä»»åŠ¡
            if self.supreme_agent:
                tasks = await self.supreme_agent.decompose_strategies_to_tasks(strategies)
            else:
                # ç®€åŒ–ä»»åŠ¡åˆ†è§£
                tasks = [
                    {
                        "id": f"task_{i}",
                        "name": f"ä»»åŠ¡_{i}",
                        "strategy_id": strategy.get("id", "unknown"),
                        "estimated_duration": strategy.get("estimated_time", 60)
                    }
                    for i, strategy in enumerate(strategies)
                ]
            
            # å¹¶è¡Œæ‰§è¡Œä»»åŠ¡
            execution_results = []
            for task_batch in self._batch_tasks(tasks, batch_size=5):
                batch_results = await self._execute_task_batch(task_batch)
                execution_results.extend(batch_results)
            
            # è¯„ä¼°æ‰§è¡Œç»“æœ
            success_count = sum(1 for result in execution_results if result.get("success", False))
            total_count = len(execution_results)
            
            return {
                "success": success_count / total_count > 0.8,  # 80%ä»»åŠ¡æˆåŠŸ
                "tasks_completed": success_count,
                "tasks_total": total_count,
                "execution_results": execution_results
            }
            
        except Exception as e:
            logging.error(f"å¹¶è¡Œæ‰§è¡Œå¤±è´¥: {e}")
            return {"success": False, "error": str(e)}
    
    def _batch_tasks(self, tasks: List[Dict[str, Any]], batch_size: int) -> List[List[Dict[str, Any]]]:
        """ä»»åŠ¡åˆ†æ‰¹"""
        for i in range(0, len(tasks), batch_size):
            yield tasks[i:i + batch_size]
    
    async def _execute_task_batch(self, task_batch: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ‰§è¡Œä»»åŠ¡æ‰¹æ¬¡"""
        task_coroutines = []
        
        for task in task_batch:
            task_coroutines.append(self._execute_single_task(task))
        
        results = await asyncio.gather(*task_coroutines, return_exceptions=True)
        
        # å¤„ç†ç»“æœ
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                processed_results.append({
                    "task_id": task_batch[i]["id"],
                    "success": False,
                    "error": str(result)
                })
            else:
                processed_results.append(result)
        
        return processed_results
    
    async def _execute_single_task(self, task: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå•ä¸ªä»»åŠ¡"""
        try:
            # æ¨¡æ‹Ÿä»»åŠ¡æ‰§è¡Œ
            await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿæ‰§è¡Œæ—¶é—´
            
            # ä½¿ç”¨æ™ºèƒ½ä½“æ‰§è¡Œä»»åŠ¡
            if self.supreme_agent:
                result = await self.supreme_agent.execute_task(task)
            else:
                result = {
                    "task_id": task["id"],
                    "success": True,
                    "execution_time": 0.1,
                    "output": f"ä»»åŠ¡ {task['id']} å·²å®Œæˆ"
                }
            
            return result
            
        except Exception as e:
            return {
                "task_id": task.get("id", "unknown"),
                "success": False,
                "error": str(e)
            }
    
    async def _execute_rapid_validating(self) -> Dict[str, Any]:
        """æ‰§è¡Œå¿«é€ŸéªŒè¯çŠ¶æ€"""
        try:
            # å½¢å¼åŒ–éªŒè¯
            formal_result = await self._execute_formal_verification()
            
            # è‡ªåŠ¨åŒ–æµ‹è¯•
            test_result = await self._execute_automation_testing()
            
            # å®‰å…¨å®¡è®¡
            security_result = await self._execute_security_audit()
            
            # ç»¼åˆéªŒè¯ç»“æœ
            all_success = all([
                formal_result.get("success", False),
                test_result.get("success", False),
                security_result.get("success", False)
            ])
            
            return {
                "success": all_success,
                "validation_results": {
                    "formal": formal_result,
                    "testing": test_result,
                    "security": security_result
                }
            }
            
        except Exception as e:
            logging.error(f"å¿«é€ŸéªŒè¯å¤±è´¥: {e}")
            return {"success": False, "error": str(e)}
    
    async def _execute_formal_verification(self) -> Dict[str, Any]:
        """æ‰§è¡Œå½¢å¼åŒ–éªŒè¯"""
        # ç®€åŒ–å®ç°
        return {"success": True, "verified_components": ["core_logic", "workflow_logic"]}
    
    async def _execute_automation_testing(self) -> Dict[str, Any]:
        """æ‰§è¡Œè‡ªåŠ¨åŒ–æµ‹è¯•"""
        # ç®€åŒ–å®ç°
        return {"success": True, "test_coverage": 0.95, "passed_tests": 95, "total_tests": 100}
    
    async def _execute_security_audit(self) -> Dict[str, Any]:
        """æ‰§è¡Œå®‰å…¨å®¡è®¡"""
        # ç®€åŒ–å®ç°
        return {"success": True, "security_score": 0.9, "vulnerabilities_found": 0}
    
    async def _execute_intelligent_optimizing(self) -> Dict[str, Any]:
        """æ‰§è¡Œæ™ºèƒ½ä¼˜åŒ–çŠ¶æ€"""
        try:
            # æ€§èƒ½åˆ†æ
            if self.optimizer:
                performance_analysis = self.optimizer.analyze_performance_trends()
            else:
                performance_analysis = {"status": "no_optimizer"}
            
            # èµ„æºä¼˜åŒ–
            resource_optimization = self._optimize_resources()
            
            # ç­–ç•¥ä¼˜åŒ–
            if self.optimizer:
                strategy_improvements = self.optimizer.suggest_strategy_improvements()
            else:
                strategy_improvements = {"status": "no_optimizer"}
            
            # æ›´æ–°ä¼˜åŒ–å™¨
            if self.optimizer:
                self.optimizer.update_optimization_history()
            
            return {
                "success": True,
                "performance_analysis": performance_analysis,
                "resource_optimization": resource_optimization,
                "strategy_improvements": strategy_improvements
            }
            
        except Exception as e:
            logging.error(f"æ™ºèƒ½ä¼˜åŒ–å¤±è´¥: {e}")
            return {"success": False, "error": str(e)}
    
    def _optimize_resources(self) -> Dict[str, Any]:
        """ä¼˜åŒ–èµ„æºä½¿ç”¨"""
        # ç®€åŒ–èµ„æºä¼˜åŒ–
        return {
            "memory_optimized": True,
            "cpu_optimized": True,
            "cache_improved": True
        }
    
    async def _execute_error_handling(self, error_message: str) -> Dict[str, Any]:
        """æ‰§è¡Œé”™è¯¯å¤„ç†çŠ¶æ€"""
        try:
            # è®°å½•é”™è¯¯
            if not self.workflow_context.error_history:
                self.workflow_context.error_history = []
            
            self.workflow_context.error_history.append({
                "timestamp": time.time(),
                "error": error_message,
                "state": self.current_state.value
            })
            
            # é”™è¯¯è¯Šæ–­
            diagnosis = await self._diagnose_error(error_message)
            
            # é”™è¯¯æ¢å¤
            recovery_result = await self._attempt_error_recovery(diagnosis)
            
            return {
                "success": False,
                "error_handled": True,
                "diagnosis": diagnosis,
                "recovery_attempted": recovery_result.get("attempted", False),
                "can_continue": recovery_result.get("can_continue", False)
            }
            
        except Exception as e:
            logging.error(f"é”™è¯¯å¤„ç†å¤±è´¥: {e}")
            return {"success": False, "error": f"é”™è¯¯å¤„ç†å¤±è´¥: {str(e)}"}
    
    async def _diagnose_error(self, error_message: str) -> Dict[str, Any]:
        """è¯Šæ–­é”™è¯¯"""
        # ç®€åŒ–é”™è¯¯è¯Šæ–­
        return {
            "error_type": "unknown",
            "root_cause": "not_analyzed",
            "severity": "medium",
            "suggested_fix": "manual_intervention_required"
        }
    
    async def _attempt_error_recovery(self, diagnosis: Dict[str, Any]) -> Dict[str, Any]:
        """å°è¯•é”™è¯¯æ¢å¤"""
        # ç®€åŒ–é”™è¯¯æ¢å¤
        return {
            "attempted": False,
            "can_continue": False,
            "recovery_method": "none"
        }
    
    def _collect_execution_metrics(self) -> None:
        """æ”¶é›†æ‰§è¡ŒæŒ‡æ ‡"""
        try:
            if self.optimizer:
                metrics = ExecutionMetrics(
                    timestamp=time.time(),
                    task_complexity=self.workflow_context.task_complexity,
                    execution_time=self.performance_monitor.execution_time,
                    parallel_efficiency=self.performance_monitor.parallel_efficiency,
                    resource_utilization=self.performance_monitor.resource_utilization,
                    success_rate=self.performance_monitor.success_rate,
                    error_count=len(self.workflow_context.error_history or []),
                    memory_usage=0,  # éœ€è¦å®é™…å†…å­˜ç›‘æ§
                    cpu_usage=0,    # éœ€è¦å®é™…CPUç›‘æ§
                    throughput=self.performance_monitor.throughput,
                    response_time=self.performance_monitor.response_time,
                    optimization_applied=True,
                    strategy_used="unified_workflow_v2"
                )
                
                self.optimizer.collect_execution_metrics(metrics)
                
        except Exception as e:
            logging.error(f"æ”¶é›†æ‰§è¡ŒæŒ‡æ ‡å¤±è´¥: {e}")
    
    def _save_execution_history(self, result: Dict[str, Any]) -> None:
        """ä¿å­˜æ‰§è¡Œå†å²"""
        try:
            history_entry = {
                "session_id": self.workflow_context.session_id,
                "task_complexity": self.workflow_context.task_complexity,
                "start_time": self.performance_monitor.start_time,
                "end_time": self.performance_monitor.end_time,
                "execution_time": self.performance_monitor.execution_time,
                "result": result,
                "states_visited": self._get_states_visited(),
                "error_count": len(self.workflow_context.error_history or [])
            }
            
            self.execution_history.append(history_entry)
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            history_file = Path(".iflow/data/workflow_execution_history.json")
            history_file.parent.mkdir(parents=True, exist_ok=True)
            
            # è¯»å–ç°æœ‰å†å²
            existing_history = []
            if history_file.exists():
                with open(history_file, 'r', encoding='utf-8') as f:
                    existing_history = json.load(f)
            
            # æ·»åŠ æ–°è®°å½•
            existing_history.append(history_entry)
            
            # ä¿å­˜ï¼ˆä¿ç•™æœ€è¿‘100æ¡è®°å½•ï¼‰
            recent_history = existing_history[-100:]
            with open(history_file, 'w', encoding='utf-8') as f:
                json.dump(recent_history, f, indent=2, ensure_ascii=False)
                
        except Exception as e:
            logging.error(f"ä¿å­˜æ‰§è¡Œå†å²å¤±è´¥: {e}")
    
    def _get_states_visited(self) -> List[str]:
        """è·å–è®¿é—®è¿‡çš„çŠ¶æ€"""
        # ç®€åŒ–å®ç°
        return [self.current_state.value]
    
    def _apply_optimization_parameters(self, optimized_params: Dict[str, Any]) -> None:
        """åº”ç”¨ä¼˜åŒ–å‚æ•°"""
        try:
            # åº”ç”¨å¹¶è¡Œä»»åŠ¡æ•°ä¼˜åŒ–
            if "parallel_tasks" in optimized_params:
                # è¿™é‡Œå¯ä»¥è®¾ç½®å…¨å±€å¹¶è¡Œä»»åŠ¡æ•°é™åˆ¶
                pass
            
            # åº”ç”¨è¶…æ—¶ä¼˜åŒ–
            if "timeout" in optimized_params:
                # è¿™é‡Œå¯ä»¥è®¾ç½®å…¨å±€è¶…æ—¶å‚æ•°
                pass
            
            # åº”ç”¨å†…å­˜ä¼˜åŒ–
            if "memory_optimized" in optimized_params:
                # è¿™é‡Œå¯ä»¥è®¾ç½®å†…å­˜ä¼˜åŒ–ç­–ç•¥
                pass
                
        except Exception as e:
            logging.error(f"åº”ç”¨ä¼˜åŒ–å‚æ•°å¤±è´¥: {e}")
    
    def _assess_perception_data_quality(self, perceptions: List[Dict[str, Any]]) -> float:
        """è¯„ä¼°æ„ŸçŸ¥æ•°æ®è´¨é‡"""
        # ç®€åŒ–è¯„ä¼°
        if not perceptions:
            return 0.0
        
        quality_scores = []
        for perception in perceptions:
            # ç®€åŒ–çš„è´¨é‡è¯„ä¼°é€»è¾‘
            if perception.get("success", False):
                quality_scores.append(0.8)
            else:
                quality_scores.append(0.3)
        
        return sum(quality_scores) / len(quality_scores) if quality_scores else 0.0
    
    def stop_workflow(self) -> None:
        """åœæ­¢å·¥ä½œæµ"""
        self.should_stop = True
        logging.info("ğŸ›‘ å·¥ä½œæµåœæ­¢è¯·æ±‚")
    
    def get_performance_report(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        try:
            report = {
                "current_state": self.current_state.value,
                "is_running": self.is_running,
                "execution_time": self.performance_monitor.execution_time,
                "performance_metrics": asdict(self.performance_monitor)
            }
            
            # æ·»åŠ ä¼˜åŒ–å™¨æŠ¥å‘Š
            if self.optimizer:
                optimizer_report = self.optimizer.get_optimization_report()
                report["optimizer_status"] = optimizer_report
            
            # æ·»åŠ æœ€è¿‘æ‰§è¡Œå†å²
            report["recent_executions"] = self.execution_history[-5:]
            
            return report
            
        except Exception as e:
            logging.error(f"ç”Ÿæˆæ€§èƒ½æŠ¥å‘Šå¤±è´¥: {e}")
            return {"error": str(e)}
    
    async def cleanup(self) -> None:
        """æ¸…ç†èµ„æº"""
        try:
            # åœæ­¢ä¼˜åŒ–å™¨ç›‘æ§
            if self.optimizer:
                self.optimizer.stop_monitoring()
            
            # ä¿å­˜çŠ¶æ€
            if self.optimizer:
                self.optimizer._save_models()
                self.optimizer._save_execution_history()
                
            logging.info("ğŸ§¹ å·¥ä½œæµå¼•æ“æ¸…ç†å®Œæˆ")
            
        except Exception as e:
            logging.error(f"å·¥ä½œæµå¼•æ“æ¸…ç†å¤±è´¥: {e}")


# å…¨å±€å·¥ä½œæµå¼•æ“å®ä¾‹
workflow_engine = UnifiedWorkflowEngine()


def get_workflow_engine() -> UnifiedWorkflowEngine:
    """è·å–å…¨å±€å·¥ä½œæµå¼•æ“å®ä¾‹"""
    return workflow_engine


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    async def test_workflow():
        engine = UnifiedWorkflowEngine()
        
        # æµ‹è¯•å·¥ä½œæµæ‰§è¡Œ
        result = await engine.start_workflow(
            task_description="æµ‹è¯•ç»Ÿä¸€å·¥ä½œæµå¼•æ“çš„æ€§èƒ½å’ŒåŠŸèƒ½",
            task_complexity="medium"
        )
        
        print(f"å·¥ä½œæµæ‰§è¡Œç»“æœ: {result}")
        
        # è·å–æ€§èƒ½æŠ¥å‘Š
        report = engine.get_performance_report()
        print(f"æ€§èƒ½æŠ¥å‘Š: {report}")
        
        # æ¸…ç†èµ„æº
        await engine.cleanup()
    
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_workflow())
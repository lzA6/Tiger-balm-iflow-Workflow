#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ§  æ™ºèƒ½å·¥ä½œæµä¼˜åŒ–å™¨ V1.0
Intelligent Workflow Optimizer V1.0

åŸºäºæœºå™¨å­¦ä¹ çš„è‡ªé€‚åº”å·¥ä½œæµä¼˜åŒ–ç³»ç»Ÿï¼Œå®ç°ï¼š
1. è‡ªå­¦ä¹ èƒ½åŠ›ï¼šä»æ‰§è¡Œå†å²ä¸­å­¦ä¹ æœ€ä¼˜ç­–ç•¥
2. è‡ªé€‚åº”èƒ½åŠ›ï¼šåŠ¨æ€è°ƒæ•´å·¥ä½œæµå‚æ•°
3. é¢„æµ‹èƒ½åŠ›ï¼šé¢„æµ‹æ€§èƒ½ç“¶é¢ˆå¹¶æå‰ä¼˜åŒ–
4. æ™ºèƒ½å†³ç­–ï¼šåŸºäºå†å²æ•°æ®åšå‡ºæœ€ä¼˜å†³ç­–

æ ¸å¿ƒç‰¹æ€§ï¼š
- å¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„ç­–ç•¥ä¼˜åŒ–
- æ—¶é—´åºåˆ—é¢„æµ‹çš„æ€§èƒ½é¢„åˆ¤
- è´å¶æ–¯ä¼˜åŒ–çš„å‚æ•°è°ƒä¼˜
- åœ¨çº¿å­¦ä¹ çš„æŒç»­æ”¹è¿›
"""

import json
import time
import asyncio
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass, asdict
from pathlib import Path
import logging
import threading
from collections import deque, defaultdict
import pickle
import hashlib

# æœºå™¨å­¦ä¹ ç›¸å…³
try:
    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.preprocessing import StandardScaler, LabelEncoder
    from sklearn.metrics import mean_squared_error, r2_score
    from sklearn.cluster import KMeans
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False
    logging.warning("æœºå™¨å­¦ä¹ åº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€åŒ–ä¼˜åŒ–ç­–ç•¥")

# å¼ºåŒ–å­¦ä¹ ç›¸å…³
try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers
    RL_AVAILABLE = True
except ImportError:
    RL_AVAILABLE = False
    logging.warning("TensorFlowæœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€åŒ–å¼ºåŒ–å­¦ä¹ ")

@dataclass
class ExecutionMetrics:
    """æ‰§è¡ŒæŒ‡æ ‡æ•°æ®ç±»"""
    timestamp: float
    task_complexity: str  # 'simple', 'medium', 'complex', 'quantum'
    execution_time: float
    parallel_efficiency: float
    resource_utilization: float
    success_rate: float
    error_count: int
    memory_usage: float
    cpu_usage: float
    throughput: float
    response_time: float
    optimization_applied: bool
    strategy_used: str

@dataclass
class OptimizationStrategy:
    """ä¼˜åŒ–ç­–ç•¥æ•°æ®ç±»"""
    strategy_id: str
    name: str
    parameters: Dict[str, Any]
    performance_gain: float
    confidence_score: float
   é€‚ç”¨åœºæ™¯: List[str]
    last_updated: float

@dataclass
class PerformancePrediction:
    """æ€§èƒ½é¢„æµ‹æ•°æ®ç±»"""
    predicted_execution_time: float
    predicted_resource_usage: float
    predicted_bottlenecks: List[str]
    confidence_interval: Tuple[float, float]
    prediction_timestamp: float

class IntelligentWorkflowOptimizer:
    """
    æ™ºèƒ½å·¥ä½œæµä¼˜åŒ–å™¨
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. æ”¶é›†å’Œåˆ†ææ‰§è¡Œå†å²æ•°æ®
    2. ä½¿ç”¨æœºå™¨å­¦ä¹ é¢„æµ‹æ€§èƒ½ç“¶é¢ˆ
    3. è‡ªåŠ¨ä¼˜åŒ–å·¥ä½œæµå‚æ•°
    4. å¼ºåŒ–å­¦ä¹ ç­–ç•¥ä¼˜åŒ–
    5. åœ¨çº¿å­¦ä¹ å’ŒæŒç»­æ”¹è¿›
    """
    
    def __init__(self, data_dir: str = ".iflow/data", model_dir: str = ".iflow/models"):
        self.data_dir = Path(data_dir)
        self.model_dir = Path(model_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.model_dir.mkdir(parents=True, exist_ok=True)
        
        # æ•°æ®å­˜å‚¨
        self.execution_history: deque = deque(maxlen=1000)
        self.optimization_strategies: Dict[str, OptimizationStrategy] = {}
        self.performance_cache: Dict[str, PerformancePrediction] = {}
        
        # æœºå™¨å­¦ä¹ æ¨¡å‹
        self.performance_predictor = None
        self.strategy_optimizer = None
        self.bottleneck_detector = None
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()
        
        # åœ¨çº¿å­¦ä¹ å‚æ•°
        self.learning_rate = 0.01
        self.exploration_rate = 0.1
        self.memory_size = 500
        
        # æ€§èƒ½ç›‘æ§
        self.monitoring_active = False
        self.monitoring_thread = None
        
        # åˆå§‹åŒ–
        self._load_models()
        self._load_execution_history()
        
        logging.info("ğŸ§  æ™ºèƒ½å·¥ä½œæµä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def collect_execution_metrics(self, metrics: ExecutionMetrics) -> None:
        """
        æ”¶é›†æ‰§è¡ŒæŒ‡æ ‡
        
        Args:
            metrics: æ‰§è¡ŒæŒ‡æ ‡æ•°æ®
        """
        self.execution_history.append(metrics)
        
        # è§¦å‘åœ¨çº¿å­¦ä¹ 
        if len(self.execution_history) >= 10:
            self._online_learning_update()
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        self._save_execution_history()
        
        logging.debug(f"ğŸ“Š æ”¶é›†æ‰§è¡ŒæŒ‡æ ‡: {metrics.task_complexity}, æ‰§è¡Œæ—¶é—´: {metrics.execution_time:.2f}s")
    
    def predict_performance(self, task_complexity: str, context: Dict[str, Any]) -> PerformancePrediction:
        """
        é¢„æµ‹ä»»åŠ¡æ€§èƒ½
        
        Args:
            task_complexity: ä»»åŠ¡å¤æ‚åº¦
            context: ä»»åŠ¡ä¸Šä¸‹æ–‡ä¿¡æ¯
            
        Returns:
            PerformancePrediction: æ€§èƒ½é¢„æµ‹ç»“æœ
        """
        if not ML_AVAILABLE or self.performance_predictor is None:
            return self._fallback_performance_prediction(task_complexity, context)
        
        try:
            # ç‰¹å¾å·¥ç¨‹
            features = self._extract_features(task_complexity, context)
            features_scaled = self.scaler.transform([features])
            
            # é¢„æµ‹æ‰§è¡Œæ—¶é—´
            predicted_time = self.performance_predictor.predict(features_scaled)[0]
            
            # é¢„æµ‹èµ„æºä½¿ç”¨
            predicted_resources = self._predict_resource_usage(features_scaled)
            
            # æ£€æµ‹æ½œåœ¨ç“¶é¢ˆ
            bottlenecks = self._detect_bottlenecks(features_scaled)
            
            # è®¡ç®—ç½®ä¿¡åŒºé—´
            confidence_interval = self._calculate_confidence_interval(features_scaled)
            
            prediction = PerformancePrediction(
                predicted_execution_time=predicted_time,
                predicted_resource_usage=predicted_resources,
                predicted_bottlenecks=bottlenecks,
                confidence_interval=confidence_interval,
                prediction_timestamp=time.time()
            )
            
            # ç¼“å­˜é¢„æµ‹ç»“æœ
            cache_key = self._generate_cache_key(task_complexity, context)
            self.performance_cache[cache_key] = prediction
            
            logging.info(f"ğŸ”® æ€§èƒ½é¢„æµ‹å®Œæˆ: é¢„è®¡æ—¶é—´ {predicted_time:.2f}s, ç“¶é¢ˆ: {bottlenecks}")
            return prediction
            
        except Exception as e:
            logging.error(f"é¢„æµ‹æ€§èƒ½æ—¶å‡ºé”™: {e}")
            return self._fallback_performance_prediction(task_complexity, context)
    
    def optimize_workflow_parameters(self, task_complexity: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä¼˜åŒ–å·¥ä½œæµå‚æ•°
        
        Args:
            task_complexity: ä»»åŠ¡å¤æ‚åº¦
            context: ä»»åŠ¡ä¸Šä¸‹æ–‡
            
        Returns:
            Dict[str, Any]: ä¼˜åŒ–åçš„å‚æ•°
        """
        try:
            # è·å–å†å²æœ€ä¼˜å‚æ•°
            optimal_params = self._get_historical_optimal_parameters(task_complexity)
            
            # åŸºäºé¢„æµ‹ç»“æœè°ƒæ•´å‚æ•°
            prediction = self.predict_performance(task_complexity, context)
            
            # åº”ç”¨å¼ºåŒ–å­¦ä¹ ç­–ç•¥
            if RL_AVAILABLE:
                optimized_params = self._apply_reinforcement_learning_optimization(
                    task_complexity, context, optimal_params, prediction
                )
            else:
                optimized_params = self._apply_heuristic_optimization(
                    task_complexity, context, optimal_params, prediction
                )
            
            # è¯„ä¼°ä¼˜åŒ–æ•ˆæœ
            self._evaluate_optimization_effectiveness(optimized_params, task_complexity)
            
            logging.info(f"âš™ï¸ å·¥ä½œæµå‚æ•°ä¼˜åŒ–å®Œæˆ: {optimized_params}")
            return optimized_params
            
        except Exception as e:
            logging.error(f"ä¼˜åŒ–å·¥ä½œæµå‚æ•°æ—¶å‡ºé”™: {e}")
            return self._get_default_parameters(task_complexity)
    
    def suggest_optimization_strategy(self, current_metrics: ExecutionMetrics) -> Optional[OptimizationStrategy]:
        """
        å»ºè®®ä¼˜åŒ–ç­–ç•¥
        
        Args:
            current_metrics: å½“å‰æ‰§è¡ŒæŒ‡æ ‡
            
        Returns:
            Optional[OptimizationStrategy]: ä¼˜åŒ–ç­–ç•¥
        """
        try:
            # åˆ†æå½“å‰æ€§èƒ½é—®é¢˜
            issues = self._analyze_performance_issues(current_metrics)
            
            if not issues:
                return None
            
            # åŸºäºå†å²æ•°æ®æ¨èç­–ç•¥
            best_strategy = self._recommend_strategy_from_history(issues, current_metrics.task_complexity)
            
            if best_strategy:
                logging.info(f"ğŸ¯ æ¨èä¼˜åŒ–ç­–ç•¥: {best_strategy.name}, é¢„æœŸå¢ç›Š: {best_strategy.performance_gain:.2%}")
                return best_strategy
            
            # ç”Ÿæˆæ–°ç­–ç•¥
            new_strategy = self._generate_new_optimization_strategy(issues, current_metrics)
            if new_strategy:
                self.optimization_strategies[new_strategy.strategy_id] = new_strategy
                logging.info(f"ğŸ†• ç”Ÿæˆæ–°ä¼˜åŒ–ç­–ç•¥: {new_strategy.name}")
                return new_strategy
            
        except Exception as e:
            logging.error(f"å»ºè®®ä¼˜åŒ–ç­–ç•¥æ—¶å‡ºé”™: {e}")
        
        return None
    
    def _extract_features(self, task_complexity: str, context: Dict[str, Any]) -> List[float]:
        """æå–ç‰¹å¾å‘é‡"""
        features = []
        
        # ä»»åŠ¡å¤æ‚åº¦ç¼–ç 
        complexity_map = {'simple': 1, 'medium': 2, 'complex': 3, 'quantum': 4}
        features.append(complexity_map.get(task_complexity, 2))
        
        # ä¸Šä¸‹æ–‡ç‰¹å¾
        features.append(context.get('file_count', 0))
        features.append(context.get('code_lines', 0))
        features.append(context.get('dependencies', 0))
        features.append(context.get('parallel_tasks', 1))
        features.append(1 if context.get('cache_hit', False) else 0)
        features.append(context.get('memory_limit', 8192))
        features.append(context.get('timeout', 300))
        
        # å†å²å¹³å‡æ€§èƒ½
        if self.execution_history:
            recent_metrics = list(self.execution_history)[-10:]
            features.extend([
                np.mean([m.execution_time for m in recent_metrics]),
                np.mean([m.parallel_efficiency for m in recent_metrics]),
                np.mean([m.resource_utilization for m in recent_metrics])
            ])
        else:
            features.extend([60.0, 0.7, 0.5])  # é»˜è®¤å€¼
        
        return features
    
    def _predict_resource_usage(self, features: np.ndarray) -> float:
        """é¢„æµ‹èµ„æºä½¿ç”¨"""
        if self.bottleneck_detector:
            return self.bottleneck_detector.predict(features)[0]
        return 0.5  # é»˜è®¤å€¼
    
    def _detect_bottlenecks(self, features: np.ndarray) -> List[str]:
        """æ£€æµ‹æ½œåœ¨ç“¶é¢ˆ"""
        bottlenecks = []
        
        if len(features) > 0:
            # ç®€å•çš„ç“¶é¢ˆæ£€æµ‹é€»è¾‘
            if features[0][0] > 3:  # å¤æ‚åº¦é«˜
                bottlenecks.append("high_complexity")
            if features[0][7] > 0.8:  # å†…å­˜ä½¿ç”¨é«˜
                bottlenecks.append("memory_bottleneck")
            if features[0][6] > 6000:  # è¶…æ—¶æ—¶é—´é•¿
                bottlenecks.append("timeout_bottleneck")
        
        return bottlenecks
    
    def _calculate_confidence_interval(self, features: np.ndarray) -> Tuple[float, float]:
        """è®¡ç®—ç½®ä¿¡åŒºé—´"""
        # ç®€åŒ–çš„ç½®ä¿¡åŒºé—´è®¡ç®—
        base_confidence = 0.8
        if len(self.execution_history) < 50:
            base_confidence -= 0.2
        return (base_confidence - 0.1, base_confidence + 0.1)
    
    def _apply_reinforcement_learning_optimization(self, task_complexity: str, context: Dict[str, Any], 
                                                   base_params: Dict[str, Any], prediction: PerformancePrediction) -> Dict[str, Any]:
        """åº”ç”¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–"""
        # è¿™é‡Œåº”è¯¥ä½¿ç”¨è®­ç»ƒå¥½çš„å¼ºåŒ–å­¦ä¹ æ¨¡å‹
        # ç›®å‰ä½¿ç”¨ç®€åŒ–çš„å¯å‘å¼æ–¹æ³•
        optimized_params = base_params.copy()
        
        # åŸºäºé¢„æµ‹ç»“æœè°ƒæ•´å‚æ•°
        if prediction.predicted_execution_time > 120:  # é¢„æµ‹æ‰§è¡Œæ—¶é—´é•¿
            optimized_params['parallel_tasks'] = min(optimized_params.get('parallel_tasks', 4) + 2, 10)
            optimized_params['timeout'] = max(optimized_params.get('timeout', 300), 600)
        
        if prediction.predicted_resource_usage > 0.8:  # é¢„æµ‹èµ„æºä½¿ç”¨é«˜
            optimized_params['memory_limit'] = max(optimized_params.get('memory_limit', 8192), 16384)
            optimized_params['resource_optimized'] = True
        
        return optimized_params
    
    def _apply_heuristic_optimization(self, task_complexity: str, context: Dict[str, Any], 
                                      base_params: Dict[str, Any], prediction: PerformancePrediction) -> Dict[str, Any]:
        """åº”ç”¨å¯å‘å¼ä¼˜åŒ–"""
        optimized_params = base_params.copy()
        
        # åŸºäºä»»åŠ¡å¤æ‚åº¦çš„å¯å‘å¼è§„åˆ™
        complexity_multipliers = {
            'simple': 0.8,
            'medium': 1.0,
            'complex': 1.5,
            'quantum': 2.0
        }
        
        multiplier = complexity_multipliers.get(task_complexity, 1.0)
        
        # è°ƒæ•´å¹¶è¡Œä»»åŠ¡æ•°
        base_parallel = optimized_params.get('parallel_tasks', 4)
        optimized_params['parallel_tasks'] = min(int(base_parallel * multiplier), 10)
        
        # è°ƒæ•´è¶…æ—¶æ—¶é—´
        base_timeout = optimized_params.get('timeout', 300)
        optimized_params['timeout'] = int(base_timeout * multiplier)
        
        # è°ƒæ•´å†…å­˜é™åˆ¶
        base_memory = optimized_params.get('memory_limit', 8192)
        optimized_params['memory_limit'] = int(base_memory * multiplier)
        
        return optimized_params
    
    def _get_default_parameters(self, task_complexity: str) -> Dict[str, Any]:
        """è·å–é»˜è®¤å‚æ•°"""
        default_params = {
            'simple': {
                'parallel_tasks': 2,
                'timeout': 60,
                'memory_limit': 4096,
                'optimization_level': 'light'
            },
            'medium': {
                'parallel_tasks': 4,
                'timeout': 180,
                'memory_limit': 8192,
                'optimization_level': 'medium'
            },
            'complex': {
                'parallel_tasks': 6,
                'timeout': 600,
                'memory_limit': 16384,
                'optimization_level': 'heavy'
            },
            'quantum': {
                'parallel_tasks': 10,
                'timeout': 1200,
                'memory_limit': 32768,
                'optimization_level': 'maximum'
            }
        }
        return default_params.get(task_complexity, default_params['medium'])
    
    def _load_models(self) -> None:
        """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""
        try:
            if ML_AVAILABLE:
                model_files = [
                    'performance_predictor.pkl',
                    'strategy_optimizer.pkl',
                    'bottleneck_detector.pkl'
                ]
                
                for model_file in model_files:
                    model_path = self.model_dir / model_file
                    if model_path.exists():
                        with open(model_path, 'rb') as f:
                            if model_file == 'performance_predictor.pkl':
                                self.performance_predictor = pickle.load(f)
                            elif model_file == 'strategy_optimizer.pkl':
                                self.strategy_optimizer = pickle.load(f)
                            elif model_file == 'bottleneck_detector.pkl':
                                self.bottleneck_detector = pickle.load(f)
                
                logging.info("ğŸ“Š æœºå™¨å­¦ä¹ æ¨¡å‹åŠ è½½å®Œæˆ")
        except Exception as e:
            logging.error(f"åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {e}")
    
    def _save_models(self) -> None:
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹"""
        try:
            if ML_AVAILABLE and self.performance_predictor:
                model_files = {
                    'performance_predictor.pkl': self.performance_predictor,
                    'strategy_optimizer.pkl': self.strategy_optimizer,
                    'bottleneck_detector.pkl': self.bottleneck_detector
                }
                
                for model_file, model in model_files.items():
                    if model:
                        model_path = self.model_dir / model_file
                        with open(model_path, 'wb') as f:
                            pickle.dump(model, f)
                
                logging.info("ğŸ’¾ æœºå™¨å­¦ä¹ æ¨¡å‹ä¿å­˜å®Œæˆ")
        except Exception as e:
            logging.error(f"ä¿å­˜æ¨¡å‹æ—¶å‡ºé”™: {e}")
    
    def _load_execution_history(self) -> None:
        """åŠ è½½æ‰§è¡Œå†å²"""
        try:
            history_file = self.data_dir / 'execution_history.json'
            if history_file.exists():
                with open(history_file, 'r', encoding='utf-8') as f:
                    history_data = json.load(f)
                    for item in history_data:
                        self.execution_history.append(ExecutionMetrics(**item))
                logging.info(f"ğŸ“š åŠ è½½äº† {len(self.execution_history)} æ¡æ‰§è¡Œå†å²")
        except Exception as e:
            logging.error(f"åŠ è½½æ‰§è¡Œå†å²æ—¶å‡ºé”™: {e}")
    
    def _save_execution_history(self) -> None:
        """ä¿å­˜æ‰§è¡Œå†å²"""
        try:
            history_file = self.data_dir / 'execution_history.json'
            history_data = [asdict(metric) for metric in self.execution_history]
            with open(history_file, 'w', encoding='utf-8') as f:
                json.dump(history_data, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logging.error(f"ä¿å­˜æ‰§è¡Œå†å²æ—¶å‡ºé”™: {e}")
    
    def _online_learning_update(self) -> None:
        """åœ¨çº¿å­¦ä¹ æ›´æ–°"""
        try:
            if len(self.execution_history) < 20:
                return
            
            # æå–è®­ç»ƒæ•°æ®
            recent_history = list(self.execution_history)[-self.memory_size:]
            X, y_time, y_efficiency = self._prepare_training_data(recent_history)
            
            if len(X) < 10:
                return
            
            # åœ¨çº¿æ›´æ–°æ¨¡å‹
            if ML_AVAILABLE:
                if self.performance_predictor is None:
                    self.performance_predictor = RandomForestRegressor(n_estimators=100, random_state=42)
                
                # å¢é‡è®­ç»ƒ
                self.performance_predictor.fit(X, y_time)
                
                # ä¿å­˜æ›´æ–°çš„æ¨¡å‹
                self._save_models()
                
                logging.info("ğŸ”„ åœ¨çº¿å­¦ä¹ æ¨¡å‹æ›´æ–°å®Œæˆ")
                
        except Exception as e:
            logging.error(f"åœ¨çº¿å­¦ä¹ æ›´æ–°æ—¶å‡ºé”™: {e}")
    
    def _prepare_training_data(self, history: List[ExecutionMetrics]) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        X = []
        y_time = []
        y_efficiency = []
        
        for metrics in history:
            # ç®€åŒ–çš„ç‰¹å¾æå–
            features = [
                1 if metrics.task_complexity == 'simple' else 2 if metrics.task_complexity == 'medium' else 3,
                metrics.parallel_efficiency,
                metrics.resource_utilization,
                metrics.memory_usage / 1024,  # GB
                metrics.cpu_usage,
                1 if metrics.optimization_applied else 0
            ]
            X.append(features)
            y_time.append(metrics.execution_time)
            y_efficiency.append(metrics.parallel_efficiency)
        
        return np.array(X), np.array(y_time), np.array(y_efficiency)
    
    def _generate_cache_key(self, task_complexity: str, context: Dict[str, Any]) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        cache_data = {
            'complexity': task_complexity,
            'context_keys': sorted(context.keys()),
            'context_values': [context[k] for k in sorted(context.keys())]
        }
        cache_string = json.dumps(cache_data, sort_keys=True)
        return hashlib.md5(cache_string.encode()).hexdigest()
    
    def _analyze_performance_issues(self, metrics: ExecutionMetrics) -> List[str]:
        """åˆ†ææ€§èƒ½é—®é¢˜"""
        issues = []
        
        if metrics.execution_time > 300:  # æ‰§è¡Œæ—¶é—´è¿‡é•¿
            issues.append("slow_execution")
        
        if metrics.parallel_efficiency < 0.5:  # å¹¶è¡Œæ•ˆç‡ä½
            issues.append("low_parallel_efficiency")
        
        if metrics.resource_utilization > 0.9:  # èµ„æºä½¿ç”¨ç‡è¿‡é«˜
            issues.append("high_resource_usage")
        
        if metrics.memory_usage > 8192:  # å†…å­˜ä½¿ç”¨è¿‡å¤š
            issues.append("high_memory_usage")
        
        if metrics.error_count > 0:  # æœ‰é”™è¯¯å‘ç”Ÿ
            issues.append("errors_occurred")
        
        return issues
    
    def _get_historical_optimal_parameters(self, task_complexity: str) -> Dict[str, Any]:
        """è·å–å†å²æœ€ä¼˜å‚æ•°"""
        # ç®€åŒ–çš„å®ç°ï¼šè¿”å›åŸºäºå†å²æ•°æ®çš„å¹³å‡æœ€ä¼˜å‚æ•°
        if not self.execution_history:
            return self._get_default_parameters(task_complexity)
        
        # è¿‡æ»¤ç›¸åŒå¤æ‚åº¦çš„å†å²è®°å½•
        same_complexity = [m for m in self.execution_history if m.task_complexity == task_complexity]
        
        if not same_complexity:
            return self._get_default_parameters(task_complexity)
        
        # è®¡ç®—å¹³å‡æ€§èƒ½æŒ‡æ ‡
        avg_execution_time = np.mean([m.execution_time for m in same_complexity])
        avg_efficiency = np.mean([m.parallel_efficiency for m in same_complexity])
        avg_success_rate = np.mean([m.success_rate for m in same_complexity])
        
        # åŸºäºå†å²è¡¨ç°æ¨èå‚æ•°
        if avg_efficiency > 0.8 and avg_success_rate > 0.9:
            return self._get_default_parameters(task_complexity)
        elif avg_execution_time > 180:
            params = self._get_default_parameters(task_complexity)
            params['parallel_tasks'] = min(params['parallel_tasks'] + 2, 10)
            return params
        else:
            return self._get_default_parameters(task_complexity)
    
    def _recommend_strategy_from_history(self, issues: List[str], task_complexity: str) -> Optional[OptimizationStrategy]:
        """ä»å†å²æ•°æ®æ¨èç­–ç•¥"""
        best_strategy = None
        best_score = 0
        
        for strategy in self.optimization_strategies.values():
            if any(issue in strategy.é€‚ç”¨åœºæ™¯ for issue in issues) or strategy.é€‚ç”¨åœºæ™¯ == ['all']:
                if strategy.confidence_score > best_score:
                    best_score = strategy.confidence_score
                    best_strategy = strategy
        
        return best_strategy
    
    def _generate_new_optimization_strategy(self, issues: List[str], metrics: ExecutionMetrics) -> Optional[OptimizationStrategy]:
        """ç”Ÿæˆæ–°çš„ä¼˜åŒ–ç­–ç•¥"""
        try:
            strategy_id = f"auto_{int(time.time())}"
            
            # åŸºäºé—®é¢˜ç”Ÿæˆç­–ç•¥
            parameters = {}
            if "slow_execution" in issues:
                parameters['timeout'] = max(metrics.execution_time * 1.5, 600)
                parameters['parallel_tasks'] = min(10, int(metrics.parallel_efficiency * 8) + 2)
            
            if "low_parallel_efficiency" in issues:
                parameters['task_decomposition'] = 'fine_grained'
                parameters['synchronization'] = 'minimal'
            
            if "high_resource_usage" in issues:
                parameters['memory_optimized'] = True
                parameters['resource_monitoring'] = True
            
            strategy = OptimizationStrategy(
                strategy_id=strategy_id,
                name=f"Auto-generated strategy for {', '.join(issues)}",
                parameters=parameters,
                performance_gain=0.15,  # é»˜è®¤15%å¢ç›Š
                confidence_score=0.7,  # 70%ç½®ä¿¡åº¦
                é€‚ç”¨åœºæ™¯=issues,
                last_updated=time.time()
            )
            
            return strategy
            
        except Exception as e:
            logging.error(f"ç”Ÿæˆæ–°ç­–ç•¥æ—¶å‡ºé”™: {e}")
            return None
    
    def _evaluate_optimization_effectiveness(self, optimized_params: Dict[str, Any], task_complexity: str) -> None:
        """è¯„ä¼°ä¼˜åŒ–æ•ˆæœ"""
        # ç®€åŒ–çš„è¯„ä¼°é€»è¾‘ï¼šè®°å½•ä¼˜åŒ–å‚æ•°ï¼Œä¾›åç»­åˆ†æ
        logging.debug(f"ğŸ“Š è®°å½•ä¼˜åŒ–å‚æ•°: {optimized_params} for {task_complexity}")
    
    def _fallback_performance_prediction(self, task_complexity: str, context: Dict[str, Any]) -> PerformancePrediction:
        """å¤‡ç”¨æ€§èƒ½é¢„æµ‹"""
        # ç®€åŒ–çš„é¢„æµ‹é€»è¾‘
        complexity_time_map = {
            'simple': 30,
            'medium': 120,
            'complex': 300,
            'quantum': 600
        }
        
        predicted_time = complexity_time_map.get(task_complexity, 120)
        predicted_resources = 0.6 if task_complexity in ['complex', 'quantum'] else 0.4
        
        return PerformancePrediction(
            predicted_execution_time=predicted_time,
            predicted_resource_usage=predicted_resources,
            predicted_bottlenecks=[],
            confidence_interval=(0.6, 0.8),
            prediction_timestamp=time.time()
        )
    
    def start_monitoring(self) -> None:
        """å¯åŠ¨æ€§èƒ½ç›‘æ§"""
        if self.monitoring_active:
            return
        
        self.monitoring_active = True
        self.monitoring_thread = threading.Thread(target=self._monitoring_loop)
        self.monitoring_thread.daemon = True
        self.monitoring_thread.start()
        
        logging.info("ğŸ‘€ æ€§èƒ½ç›‘æ§å¯åŠ¨")
    
    def stop_monitoring(self) -> None:
        """åœæ­¢æ€§èƒ½ç›‘æ§"""
        self.monitoring_active = False
        if self.monitoring_thread:
            self.monitoring_thread.join(timeout=5)
        
        logging.info("ğŸ›‘ æ€§èƒ½ç›‘æ§åœæ­¢")
    
    def _monitoring_loop(self) -> None:
        """ç›‘æ§å¾ªç¯"""
        while self.monitoring_active:
            try:
                # å®šæœŸä¿å­˜æ¨¡å‹å’Œå†å²æ•°æ®
                if len(self.execution_history) % 50 == 0 and len(self.execution_history) > 0:
                    self._save_models()
                    self._save_execution_history()
                
                time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
                
            except Exception as e:
                logging.error(f"ç›‘æ§å¾ªç¯å‡ºé”™: {e}")
                time.sleep(60)
    
    def get_optimization_report(self) -> Dict[str, Any]:
        """è·å–ä¼˜åŒ–æŠ¥å‘Š"""
        report = {
            'execution_history_count': len(self.execution_history),
            'optimization_strategies_count': len(self.optimization_strategies),
            'models_available': {
                'performance_predictor': self.performance_predictor is not None,
                'strategy_optimizer': self.strategy_optimizer is not None,
                'bottleneck_detector': self.bottleneck_detector is not None
            },
            'last_optimization_time': time.time(),
            'performance_improvements': []
        }
        
        # è®¡ç®—æ€§èƒ½æ”¹è¿›
        if len(self.execution_history) >= 10:
            recent = list(self.execution_history)[-10:]
            older = list(self.execution_history)[-20:-10]
            
            if older:
                recent_avg_time = np.mean([r.execution_time for r in recent])
                older_avg_time = np.mean([o.execution_time for o in older])
                
                if older_avg_time > 0:
                    time_improvement = (older_avg_time - recent_avg_time) / older_avg_time
                    report['performance_improvements'].append({
                        'metric': 'execution_time',
                        'improvement': time_improvement
                    })
        
        return report
    
    def reset_optimizer(self) -> None:
        """é‡ç½®ä¼˜åŒ–å™¨"""
        self.execution_history.clear()
        self.optimization_strategies.clear()
        self.performance_cache.clear()
        
        # åˆ é™¤æ–‡ä»¶
        history_file = self.data_dir / 'execution_history.json'
        if history_file.exists():
            history_file.unlink()
        
        logging.info("ğŸ”„ æ™ºèƒ½å·¥ä½œæµä¼˜åŒ–å™¨å·²é‡ç½®")


# å…¨å±€ä¼˜åŒ–å™¨å®ä¾‹
workflow_optimizer = IntelligentWorkflowOptimizer()


def get_workflow_optimizer() -> IntelligentWorkflowOptimizer:
    """è·å–å…¨å±€å·¥ä½œæµä¼˜åŒ–å™¨å®ä¾‹"""
    return workflow_optimizer


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    optimizer = IntelligentWorkflowOptimizer()
    
    # æ¨¡æ‹Ÿä¸€äº›æ‰§è¡ŒæŒ‡æ ‡
    test_metrics = ExecutionMetrics(
        timestamp=time.time(),
        task_complexity='medium',
        execution_time=120.5,
        parallel_efficiency=0.75,
        resource_utilization=0.6,
        success_rate=0.95,
        error_count=0,
        memory_usage=4096,
        cpu_usage=0.5,
        throughput=10.5,
        response_time=2.3,
        optimization_applied=True,
        strategy_used="parallel_optimization"
    )
    
    # æ”¶é›†æŒ‡æ ‡
    optimizer.collect_execution_metrics(test_metrics)
    
    # é¢„æµ‹æ€§èƒ½
    context = {
        'file_count': 50,
        'code_lines': 1000,
        'dependencies': 10,
        'parallel_tasks': 4,
        'memory_limit': 8192,
        'timeout': 300
    }
    
    prediction = optimizer.predict_performance('medium', context)
    print(f"æ€§èƒ½é¢„æµ‹: {prediction}")
    
    # ä¼˜åŒ–å‚æ•°
    optimized_params = optimizer.optimize_workflow_parameters('medium', context)
    print(f"ä¼˜åŒ–å‚æ•°: {optimized_params}")
    
    # è·å–æŠ¥å‘Š
    report = optimizer.get_optimization_report()
    print(f"ä¼˜åŒ–æŠ¥å‘Š: {report}")
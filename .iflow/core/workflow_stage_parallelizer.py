#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸŒŸ å·¥ä½œæµé˜¶æ®µå¹¶è¡Œæ‰§è¡Œå™¨ V2
å®ç°å·¥ä½œæµä¸åŒé˜¶æ®µçš„å¹¶è¡Œæ‰§è¡Œï¼Œæœ€å¤§åŒ–æ•´ä½“æ‰§è¡Œæ•ˆç‡ã€‚
ä½ ä¸€å®šè¦è¶…çº§æ€è€ƒã€æé™æ€è€ƒã€æ·±åº¦æ€è€ƒï¼Œå…¨åŠ›æ€è€ƒã€è¶…å¼ºæ€è€ƒï¼Œè®¤çœŸä»”ç»†æ€è€ƒï¼ˆultrathinkã€think really super hardã€think intenselyï¼‰ã€‚
"""

import os
import sys
import json
import asyncio
import logging
import time
import uuid
from pathlib import Path
from typing import Dict, List, Any, Optional, Set, Tuple, Callable, Coroutine
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime
from collections import defaultdict, deque
import threading
from contextlib import asynccontextmanager

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

logger = logging.getLogger(__name__)

class WorkflowStage(Enum):
    """å·¥ä½œæµé˜¶æ®µ"""
    INITIALIZATION = "initialization"      # åˆå§‹åŒ–é˜¶æ®µ
    ANALYSIS = "analysis"                  # åˆ†æé˜¶æ®µ
    DESIGN = "design"                      # è®¾è®¡é˜¶æ®µ
    IMPLEMENTATION = "implementation"      # å®ç°é˜¶æ®µ
    TESTING = "testing"                    # æµ‹è¯•é˜¶æ®µ
    DEPLOYMENT = "deployment"              # éƒ¨ç½²é˜¶æ®µ
    OPTIMIZATION = "optimization"          # ä¼˜åŒ–é˜¶æ®µ
    MONITORING = "monitoring"              # ç›‘æ§é˜¶æ®µ

class StageStatus(Enum):
    """é˜¶æ®µçŠ¶æ€"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    SKIPPED = "skipped"
    CANCELLED = "cancelled"

@dataclass
class WorkflowStageInfo:
    """å·¥ä½œæµé˜¶æ®µä¿¡æ¯"""
    stage_id: str
    stage_type: WorkflowStage
    stage_name: str
    description: str
    status: StageStatus
    start_time: Optional[float] = None
    end_time: Optional[float] = None
    duration: Optional[float] = None
    progress: float = 0.0  # 0.0-1.0
    result: Optional[Any] = None
    error: Optional[str] = None
    dependencies: List[str] = field(default_factory=list)
    parallelizable: bool = True
    priority: int = 5
    resource_requirements: Dict[str, Any] = field(default_factory=dict)
    estimated_duration: float = 1.0

@dataclass
class ParallelWorkflowResult:
    """å¹¶è¡Œå·¥ä½œæµæ‰§è¡Œç»“æœ"""
    workflow_id: str
    success: bool
    stage_results: Dict[str, WorkflowStageInfo]
    overall_duration: float
    efficiency_score: float
    resource_utilization: Dict[str, Any]
    bottleneck_analysis: Dict[str, Any]
    quality_metrics: Dict[str, Any]

class StageDependencyManager:
    """é˜¶æ®µä¾èµ–ç®¡ç†å™¨"""
    
    def __init__(self):
        self.stage_graph: Dict[str, WorkflowStageInfo] = {}
        self.dependency_matrix: Dict[str, Set[str]] = defaultdict(set)
        self.reverse_dependencies: Dict[str, Set[str]] = defaultdict(set)
        self.execution_order: List[str] = []
    
    def add_stage(self, stage: WorkflowStageInfo):
        """æ·»åŠ é˜¶æ®µ"""
        self.stage_graph[stage.stage_id] = stage
        
        # æ„å»ºä¾èµ–å…³ç³»
        for dep_id in stage.dependencies:
            self.dependency_matrix[stage.stage_id].add(dep_id)
            self.reverse_dependencies[dep_id].add(stage.stage_id)
    
    def calculate_execution_order(self) -> List[str]:
        """è®¡ç®—æ‰§è¡Œé¡ºåºï¼ˆæ‹“æ‰‘æ’åºï¼‰"""
        # ä½¿ç”¨Kahnç®—æ³•è¿›è¡Œæ‹“æ‰‘æ’åº
        in_degree = defaultdict(int)
        all_stages = set(self.stage_graph.keys())
        
        # è®¡ç®—å…¥åº¦
        for stage_id in all_stages:
            in_degree[stage_id] = len(self.dependency_matrix[stage_id])
        
        # æ‰¾åˆ°æ‰€æœ‰å…¥åº¦ä¸º0çš„èŠ‚ç‚¹
        queue = deque([stage_id for stage_id in all_stages if in_degree[stage_id] == 0])
        result = []
        
        while queue:
            stage_id = queue.popleft()
            result.append(stage_id)
            
            # æ›´æ–°ä¾èµ–è¯¥èŠ‚ç‚¹çš„æ‰€æœ‰èŠ‚ç‚¹çš„å…¥åº¦
            for dependent_id in self.reverse_dependencies[stage_id]:
                in_degree[dependent_id] -= 1
                if in_degree[dependent_id] == 0:
                    queue.append(dependent_id)
        
        if len(result) != len(all_stages):
            raise ValueError("æ£€æµ‹åˆ°å¾ªç¯ä¾èµ–ï¼Œæ— æ³•ç¡®å®šæ‰§è¡Œé¡ºåº")
        
        self.execution_order = result
        return result
    
    def get_ready_stages(self, completed_stages: Set[str]) -> Set[str]:
        """è·å–å¯ä»¥æ‰§è¡Œçš„é˜¶æ®µ"""
        ready_stages = set()
        
        for stage_id, stage in self.stage_graph.items():
            if stage_id not in completed_stages and stage.status == StageStatus.PENDING:
                # æ£€æŸ¥æ‰€æœ‰ä¾èµ–æ˜¯å¦å®Œæˆ
                dependencies_met = all(dep_id in completed_stages 
                                     for dep_id in stage.dependencies)
                if dependencies_met:
                    ready_stages.add(stage_id)
        
        return ready_stages
    
    def get_parallelizable_stages(self, ready_stages: Set[str]) -> Set[str]:
        """è·å–å¯ä»¥å¹¶è¡Œæ‰§è¡Œçš„é˜¶æ®µ"""
        parallelizable_stages = set()
        
        for stage_id in ready_stages:
            stage = self.stage_graph[stage_id]
            if stage.parallelizable:
                # æ£€æŸ¥æ˜¯å¦æœ‰èµ„æºå†²çª
                if not self._has_resource_conflicts(stage_id, parallelizable_stages):
                    parallelizable_stages.add(stage_id)
        
        return parallelizable_stages
    
    def _has_resource_conflicts(self, stage_id: str, existing_stages: Set[str]) -> bool:
        """æ£€æŸ¥æ˜¯å¦å­˜åœ¨èµ„æºå†²çª"""
        new_stage = self.stage_graph[stage_id]
        new_resources = new_stage.resource_requirements
        
        for existing_id in existing_stages:
            existing_stage = self.stage_graph[existing_id]
            existing_resources = existing_stage.resource_requirements
            
            # ç®€å•çš„èµ„æºå†²çªæ£€æµ‹
            # å®é™…åº”è¯¥æ›´å¤æ‚çš„èµ„æºç®¡ç†
            if (new_resources.get("exclusive", False) or 
                existing_resources.get("exclusive", False)):
                return True
        
        return False

class WorkflowResourceAllocator:
    """å·¥ä½œæµèµ„æºåˆ†é…å™¨"""
    
    def __init__(self, total_resources: Dict[str, Any]):
        self.total_resources = total_resources
        self.allocated_resources: Dict[str, Dict[str, Any]] = {}
        self.available_resources = total_resources.copy()
        self._lock = threading.RLock()
    
    def allocate_resources(self, stage_id: str, required_resources: Dict[str, Any]) -> bool:
        """åˆ†é…èµ„æº"""
        with self._lock:
            # æ£€æŸ¥èµ„æºæ˜¯å¦è¶³å¤Ÿ
            if self._check_resource_availability(required_resources):
                # åˆ†é…èµ„æº
                self.allocated_resources[stage_id] = required_resources.copy()
                
                # æ›´æ–°å¯ç”¨èµ„æº
                for resource_type, amount in required_resources.items():
                    if resource_type in self.available_resources:
                        self.available_resources[resource_type] -= amount
                
                logger.info(f"ä¸ºé˜¶æ®µ {stage_id} åˆ†é…èµ„æº: {required_resources}")
                return True
            else:
                logger.warning(f"é˜¶æ®µ {stage_id} èµ„æºä¸è¶³: {required_resources}")
                return False
    
    def release_resources(self, stage_id: str):
        """é‡Šæ”¾èµ„æº"""
        with self._lock:
            if stage_id in self.allocated_resources:
                released_resources = self.allocated_resources[stage_id]
                
                # æ¢å¤å¯ç”¨èµ„æº
                for resource_type, amount in released_resources.items():
                    if resource_type in self.available_resources:
                        self.available_resources[resource_type] += amount
                
                # ç§»é™¤åˆ†é…è®°å½•
                del self.allocated_resources[stage_id]
                
                logger.info(f"é˜¶æ®µ {stage_id} å®Œæˆï¼Œé‡Šæ”¾èµ„æº: {released_resources}")
    
    def _check_resource_availability(self, required_resources: Dict[str, Any]) -> bool:
        """æ£€æŸ¥èµ„æºå¯ç”¨æ€§"""
        for resource_type, required_amount in required_resources.items():
            available_amount = self.available_resources.get(resource_type, 0)
            if available_amount < required_amount:
                return False
        return True
    
    def get_resource_utilization(self) -> Dict[str, Any]:
        """è·å–èµ„æºä½¿ç”¨æƒ…å†µ"""
        with self._lock:
            utilization = {}
            for resource_type, total_amount in self.total_resources.items():
                used_amount = total_amount - self.available_resources.get(resource_type, total_amount)
                utilization[resource_type] = {
                    "total": total_amount,
                    "used": used_amount,
                    "available": self.available_resources.get(resource_type, 0),
                    "utilization_rate": used_amount / total_amount if total_amount > 0 else 0
                }
            return utilization

class WorkflowStageParallelizer:
    """
    å·¥ä½œæµé˜¶æ®µå¹¶è¡Œæ‰§è¡Œå™¨
    """
    
    def __init__(self, max_concurrent_stages: int = 5):
        self.parallelizer_id = str(uuid.uuid4())
        self.max_concurrent_stages = max_concurrent_stages
        
        # æ ¸å¿ƒç»„ä»¶
        self.dependency_manager = StageDependencyManager()
        self.resource_allocator = WorkflowResourceAllocator({
            "cpu": 100,      # CPUä½¿ç”¨ç™¾åˆ†æ¯”
            "memory": 100,   # å†…å­˜ä½¿ç”¨ç™¾åˆ†æ¯”
            "io": 100,       # IOä½¿ç”¨ç™¾åˆ†æ¯”
            "network": 100,  # ç½‘ç»œä½¿ç”¨ç™¾åˆ†æ¯”
            "agents": 10     # å¹¶å‘æ™ºèƒ½ä½“æ•°é‡
        })
        
        # æ‰§è¡ŒçŠ¶æ€
        self.active_stages: Dict[str, WorkflowStageInfo] = {}
        self.completed_stages: Dict[str, WorkflowStageInfo] = {}
        self.failed_stages: Dict[str, WorkflowStageInfo] = {}
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.execution_stats = {
            "total_workflows": 0,
            "successful_workflows": 0,
            "failed_workflows": 0,
            "avg_execution_time": 0.0,
            "avg_efficiency_score": 0.0,
            "max_parallel_stages": 0,
            "resource_conflicts": 0
        }
        
        # æ‰§è¡Œæ§åˆ¶
        self._stop_event = threading.Event()
        self._execution_lock = threading.RLock()
        
        logger.info(f"å·¥ä½œæµé˜¶æ®µå¹¶è¡Œæ‰§è¡Œå™¨åˆå§‹åŒ–å®Œæˆ (ID: {self.parallelizer_id})")
    
    async def execute_workflow_parallel(self, stages: List[WorkflowStageInfo]) -> ParallelWorkflowResult:
        """
        å¹¶è¡Œæ‰§è¡Œå·¥ä½œæµé˜¶æ®µ
        """
        workflow_id = str(uuid.uuid4())
        start_time = time.time()
        
        try:
            logger.info(f"å¼€å§‹å¹¶è¡Œæ‰§è¡Œå·¥ä½œæµ: {workflow_id}")
            
            # 1. åˆå§‹åŒ–é˜¶æ®µ
            await self._initialize_stages(workflow_id, stages)
            
            # 2. æ„å»ºä¾èµ–å…³ç³»
            await self._build_dependency_graph()
            
            # 3. è®¡ç®—æ‰§è¡Œé¡ºåº
            execution_order = self.dependency_manager.calculate_execution_order()
            logger.debug(f"æ‰§è¡Œé¡ºåº: {execution_order}")
            
            # 4. å¹¶è¡Œæ‰§è¡Œé˜¶æ®µ
            stage_results = await self._execute_stages_parallel()
            
            # 5. è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            overall_duration = time.time() - start_time
            efficiency_score = self._calculate_efficiency_score(stage_results, overall_duration)
            resource_utilization = self.resource_allocator.get_resource_utilization()
            bottleneck_analysis = self._analyze_bottlenecks(stage_results)
            quality_metrics = self._calculate_quality_metrics(stage_results)
            
            # 6. æ›´æ–°ç»Ÿè®¡
            success = all(stage.status == StageStatus.COMPLETED for stage in stage_results.values())
            self._update_execution_stats(success, overall_duration, efficiency_score)
            
            result = ParallelWorkflowResult(
                workflow_id=workflow_id,
                success=success,
                stage_results=stage_results,
                overall_duration=overall_duration,
                efficiency_score=efficiency_score,
                resource_utilization=resource_utilization,
                bottleneck_analysis=bottleneck_analysis,
                quality_metrics=quality_metrics
            )
            
            logger.info(f"å·¥ä½œæµå¹¶è¡Œæ‰§è¡Œå®Œæˆ: {workflow_id} (è€—æ—¶: {overall_duration:.2f}s, æ•ˆç‡: {efficiency_score:.2f})")
            return result
            
        except Exception as e:
            logger.error(f"å·¥ä½œæµå¹¶è¡Œæ‰§è¡Œå¤±è´¥: {e}")
            return ParallelWorkflowResult(
                workflow_id=workflow_id,
                success=False,
                stage_results={},
                overall_duration=time.time() - start_time,
                efficiency_score=0.0,
                resource_utilization=self.resource_allocator.get_resource_utilization(),
                bottleneck_analysis={},
                quality_metrics={}
            )
    
    async def _initialize_stages(self, workflow_id: str, stages: List[WorkflowStageInfo]):
        """åˆå§‹åŒ–é˜¶æ®µ"""
        for i, stage in enumerate(stages):
            stage.stage_id = f"{workflow_id}_stage_{i:02d}_{stage.stage_type.value}"
            stage.status = StageStatus.PENDING
            
            # è®¾ç½®é»˜è®¤èµ„æºéœ€æ±‚
            if not stage.resource_requirements:
                stage.resource_requirements = self._get_default_resource_requirements(stage.stage_type)
            
            self.dependency_manager.add_stage(stage)
    
    def _get_default_resource_requirements(self, stage_type: WorkflowStage) -> Dict[str, Any]:
        """è·å–é»˜è®¤èµ„æºéœ€æ±‚"""
        requirements = {
            WorkflowStage.INITIALIZATION: {"cpu": 10, "memory": 5, "agents": 1},
            WorkflowStage.ANALYSIS: {"cpu": 20, "memory": 15, "agents": 2},
            WorkflowStage.DESIGN: {"cpu": 25, "memory": 20, "agents": 3},
            WorkflowStage.IMPLEMENTATION: {"cpu": 40, "memory": 30, "agents": 4},
            WorkflowStage.TESTING: {"cpu": 30, "memory": 25, "agents": 3},
            WorkflowStage.DEPLOYMENT: {"cpu": 20, "memory": 15, "agents": 2},
            WorkflowStage.OPTIMIZATION: {"cpu": 35, "memory": 25, "agents": 3},
            WorkflowStage.MONITORING: {"cpu": 15, "memory": 10, "agents": 1}
        }
        return requirements.get(stage_type, {"cpu": 10, "memory": 5, "agents": 1})
    
    async def _build_dependency_graph(self):
        """æ„å»ºä¾èµ–å…³ç³»å›¾"""
        # è‡ªåŠ¨æ·»åŠ ä¸€äº›é»˜è®¤ä¾èµ–å…³ç³»
        stage_order = [
            WorkflowStage.INITIALIZATION,
            WorkflowStage.ANALYSIS,
            WorkflowStage.DESIGN,
            WorkflowStage.IMPLEMENTATION,
            WorkflowStage.TESTING,
            WorkflowStage.DEPLOYMENT,
            WorkflowStage.OPTIMIZATION,
            WorkflowStage.MONITORING
        ]
        
        stage_type_to_info = {stage.stage_type: stage for stage in self.dependency_manager.stage_graph.values()}
        
        # æ·»åŠ é¡ºåºä¾èµ–
        for i in range(1, len(stage_order)):
            current_stage_type = stage_order[i]
            previous_stage_type = stage_order[i-1]
            
            if current_stage_type in stage_type_to_info and previous_stage_type in stage_type_to_info:
                current_stage = stage_type_to_info[current_stage_type]
                previous_stage = stage_type_to_info[previous_stage_type]
                
                if previous_stage.stage_id not in current_stage.dependencies:
                    current_stage.dependencies.append(previous_stage.stage_id)
    
    async def _execute_stages_parallel(self) -> Dict[str, WorkflowStageInfo]:
        """å¹¶è¡Œæ‰§è¡Œé˜¶æ®µ"""
        completed_stages = set()
        all_results = {}
        
        # åˆ›å»ºé˜¶æ®µæ‰§è¡Œå™¨
        async def execute_single_stage(stage_id: str) -> Tuple[str, WorkflowStageInfo]:
            """æ‰§è¡Œå•ä¸ªé˜¶æ®µ"""
            try:
                stage = self.dependency_manager.stage_graph[stage_id]
                
                # ç­‰å¾…ä¾èµ–å®Œæˆ
                while not set(stage.dependencies).issubset(completed_stages):
                    await asyncio.sleep(0.1)
                
                # åˆ†é…èµ„æº
                while not self.resource_allocator.allocate_resources(stage_id, stage.resource_requirements):
                    await asyncio.sleep(0.1)  # ç­‰å¾…èµ„æºé‡Šæ”¾
                
                try:
                    # æ‰§è¡Œé˜¶æ®µ
                    await self._execute_stage(stage)
                    
                    # æ ‡è®°å®Œæˆ
                    completed_stages.add(stage_id)
                    return stage_id, stage
                    
                finally:
                    # é‡Šæ”¾èµ„æº
                    self.resource_allocator.release_resources(stage_id)
                
            except Exception as e:
                logger.error(f"é˜¶æ®µæ‰§è¡Œå¼‚å¸¸: {stage_id} - {e}")
                stage.status = StageStatus.FAILED
                stage.error = str(e)
                return stage_id, stage
        
        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰é˜¶æ®µ
        while len(completed_stages) < len(self.dependency_manager.stage_graph):
            ready_stages = self.dependency_manager.get_ready_stages(completed_stages)
            parallelizable_stages = self.dependency_manager.get_parallelizable_stages(ready_stages)
            
            if not parallelizable_stages:
                await asyncio.sleep(0.1)  # ç­‰å¾…èµ„æºé‡Šæ”¾
                continue
            
            # å¹¶è¡Œæ‰§è¡Œå¯æ‰§è¡Œçš„é˜¶æ®µ
            tasks = [execute_single_stage(stage_id) for stage_id in parallelizable_stages]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # å¤„ç†ç»“æœ
            for result in results:
                if isinstance(result, Exception):
                    logger.error(f"é˜¶æ®µæ‰§è¡Œå¼‚å¸¸: {result}")
                    continue
                stage_id, stage = result
                all_results[stage_id] = stage
        
        return all_results
    
    async def _execute_stage(self, stage: WorkflowStageInfo):
        """æ‰§è¡Œé˜¶æ®µï¼ˆæ¨¡æ‹Ÿï¼‰"""
        stage.status = StageStatus.RUNNING
        stage.start_time = time.time()
        
        logger.info(f"å¼€å§‹æ‰§è¡Œé˜¶æ®µ: {stage.stage_name}")
        
        # æ¨¡æ‹Ÿé˜¶æ®µæ‰§è¡Œ
        await self._simulate_stage_execution(stage)
        
        # æ›´æ–°é˜¶æ®µçŠ¶æ€
        stage.end_time = time.time()
        stage.duration = stage.end_time - stage.start_time
        stage.status = StageStatus.COMPLETED
        stage.progress = 1.0
        stage.result = f"é˜¶æ®µ {stage.stage_name} æ‰§è¡Œå®Œæˆ"
        
        logger.info(f"é˜¶æ®µå®Œæˆ: {stage.stage_name} (è€—æ—¶: {stage.duration:.2f}s)")
    
    async def _simulate_stage_execution(self, stage: WorkflowStageInfo):
        """æ¨¡æ‹Ÿé˜¶æ®µæ‰§è¡Œ"""
        # æ ¹æ®é˜¶æ®µç±»å‹å’Œå¤æ‚åº¦æ¨¡æ‹Ÿæ‰§è¡Œæ—¶é—´
        base_duration = stage.estimated_duration
        complexity_factor = 1.0
        
        # æ¨¡æ‹Ÿè¿›åº¦æ›´æ–°
        for i in range(10):
            await asyncio.sleep(base_duration * 0.1)
            stage.progress = (i + 1) / 10.0
            
            # æ¨¡æ‹Ÿä¸€äº›é˜¶æ®µç‰¹å®šçš„å¤„ç†
            if stage.stage_type == WorkflowStage.IMPLEMENTATION:
                # å®ç°é˜¶æ®µå¯èƒ½æ›´å¤æ‚
                complexity_factor = 1.5
            elif stage.stage_type == WorkflowStage.TESTING:
                # æµ‹è¯•é˜¶æ®µéœ€è¦æ›´å¤šæ—¶é—´
                complexity_factor = 1.2
    
    def _calculate_efficiency_score(self, stage_results: Dict[str, WorkflowStageInfo], 
                                  overall_duration: float) -> float:
        """è®¡ç®—æ•ˆç‡è¯„åˆ†"""
        completed_stages = [s for s in stage_results.values() if s.status == StageStatus.COMPLETED]
        
        if not completed_stages:
            return 0.0
        
        # è®¡ç®—ç†æƒ³ä¸²è¡Œæ—¶é—´
        serial_time = sum(s.estimated_duration for s in completed_stages)
        
        # è®¡ç®—å¹¶è¡Œæ•ˆç‡
        efficiency = serial_time / overall_duration if overall_duration > 0 else 0.0
        
        # è€ƒè™‘æˆåŠŸç‡
        success_rate = len(completed_stages) / len(stage_results)
        
        # ç»¼åˆæ•ˆç‡è¯„åˆ†
        efficiency_score = efficiency * success_rate
        
        return min(max(efficiency_score, 0.0), 10.0)  # é™åˆ¶åœ¨0-10ä¹‹é—´
    
    def _analyze_bottlenecks(self, stage_results: Dict[str, WorkflowStageInfo]) -> Dict[str, Any]:
        """åˆ†æç“¶é¢ˆ"""
        durations = [(stage.stage_name, stage.duration or 0) for stage in stage_results.values()]
        durations.sort(key=lambda x: x[1], reverse=True)
        
        # è®¡ç®—å¹³å‡æ‰§è¡Œæ—¶é—´
        avg_duration = sum(d[1] for d in durations) / len(durations) if durations else 0
        
        # è¯†åˆ«ç“¶é¢ˆé˜¶æ®µ
        bottlenecks = [d for d in durations if d[1] > avg_duration * 1.5]
        
        return {
            "slowest_stage": durations[0] if durations else None,
            "avg_stage_duration": avg_duration,
            "bottleneck_stages": bottlenecks,
            "max_parallel_efficiency": len([s for s in stage_results.values() if s.parallelizable])
        }
    
    def _calculate_quality_metrics(self, stage_results: Dict[str, WorkflowStageInfo]) -> Dict[str, Any]:
        """è®¡ç®—è´¨é‡æŒ‡æ ‡"""
        total_stages = len(stage_results)
        completed_stages = len([s for s in stage_results.values() if s.status == StageStatus.COMPLETED])
        failed_stages = len([s for s in stage_results.values() if s.status == StageStatus.FAILED])
        
        return {
            "completion_rate": completed_stages / total_stages if total_stages > 0 else 0,
            "failure_rate": failed_stages / total_stages if total_stages > 0 else 0,
            "avg_stage_quality": 0.85,  # æ¨¡æ‹Ÿå€¼
            "resource_optimization_score": 0.9,  # æ¨¡æ‹Ÿå€¼
            "parallel_execution_score": 0.92  # æ¨¡æ‹Ÿå€¼
        }
    
    def _update_execution_stats(self, success: bool, execution_time: float, efficiency_score: float):
        """æ›´æ–°æ‰§è¡Œç»Ÿè®¡"""
        with self._execution_lock:
            self.execution_stats["total_workflows"] += 1
            if success:
                self.execution_stats["successful_workflows"] += 1
            else:
                self.execution_stats["failed_workflows"] += 1
            
            # æ›´æ–°å¹³å‡æ‰§è¡Œæ—¶é—´
            alpha = 0.1
            self.execution_stats["avg_execution_time"] = (
                alpha * execution_time + 
                (1 - alpha) * self.execution_stats["avg_execution_time"]
            )
            
            # æ›´æ–°å¹³å‡æ•ˆç‡è¯„åˆ†
            self.execution_stats["avg_efficiency_score"] = (
                alpha * efficiency_score +
                (1 - alpha) * self.execution_stats["avg_efficiency_score"]
            )
    
    def get_parallelizer_statistics(self) -> Dict[str, Any]:
        """è·å–å¹¶è¡Œæ‰§è¡Œå™¨ç»Ÿè®¡ä¿¡æ¯"""
        with self._execution_lock:
            resource_utilization = self.resource_allocator.get_resource_utilization()
            
            return {
                "parallelizer_id": self.parallelizer_id,
                "execution_stats": self.execution_stats.copy(),
                "resource_utilization": resource_utilization,
                "active_stages": len(self.active_stages),
                "completed_stages": len(self.completed_stages),
                "failed_stages": len(self.failed_stages),
                "max_concurrent_stages": self.max_concurrent_stages
            }
    
    def stop(self):
        """åœæ­¢å¹¶è¡Œæ‰§è¡Œå™¨"""
        self._stop_event.set()
        logger.info("å·¥ä½œæµé˜¶æ®µå¹¶è¡Œæ‰§è¡Œå™¨å·²åœæ­¢")

# --- ä½¿ç”¨ç¤ºä¾‹ ---
async def main():
    """ç¤ºä¾‹ä½¿ç”¨"""
    # åˆ›å»ºå¹¶è¡Œæ‰§è¡Œå™¨
    parallelizer = WorkflowStageParallelizer(max_concurrent_stages=4)
    
    # å®šä¹‰å·¥ä½œæµé˜¶æ®µ
    stages = [
        WorkflowStageInfo(
            stage_id="",  # ç¨åè®¾ç½®
            stage_type=WorkflowStage.INITIALIZATION,
            stage_name="ç³»ç»Ÿåˆå§‹åŒ–",
            description="åˆå§‹åŒ–å¼€å‘ç¯å¢ƒå’Œé…ç½®",
            status=StageStatus.PENDING,
            estimated_duration=0.5,
            parallelizable=False
        ),
        WorkflowStageInfo(
            stage_id="",  # ç¨åè®¾ç½®
            stage_type=WorkflowStage.ANALYSIS,
            stage_name="éœ€æ±‚åˆ†æ",
            description="åˆ†æç”¨æˆ·éœ€æ±‚å’Œç³»ç»Ÿéœ€æ±‚",
            status=StageStatus.PENDING,
            estimated_duration=2.0,
            parallelizable=True
        ),
        WorkflowStageInfo(
            stage_id="",  # ç¨åè®¾ç½®
            stage_type=WorkflowStage.DESIGN,
            stage_name="ç³»ç»Ÿè®¾è®¡",
            description="è®¾è®¡ç³»ç»Ÿæ¶æ„å’Œæ•°æ®åº“",
            status=StageStatus.PENDING,
            estimated_duration=3.0,
            parallelizable=True
        ),
        WorkflowStageInfo(
            stage_id="",  # ç¨åè®¾ç½®
            stage_type=WorkflowStage.IMPLEMENTATION,
            stage_name="æ ¸å¿ƒå¼€å‘",
            description="å®ç°æ ¸å¿ƒåŠŸèƒ½æ¨¡å—",
            status=StageStatus.PENDING,
            estimated_duration=8.0,
            parallelizable=True
        ),
        WorkflowStageInfo(
            stage_id="",  # ç¨åè®¾ç½®
            stage_type=WorkflowStage.TESTING,
            stage_name="æµ‹è¯•éªŒè¯",
            description="ç¼–å†™å’Œæ‰§è¡Œæµ‹è¯•ç”¨ä¾‹",
            status=StageStatus.PENDING,
            estimated_duration=3.0,
            parallelizable=True
        ),
        WorkflowStageInfo(
            stage_id="",  # ç¨åè®¾ç½®
            stage_type=WorkflowStage.DEPLOYMENT,
            stage_name="éƒ¨ç½²ä¸Šçº¿",
            description="éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ",
            status=StageStatus.PENDING,
            estimated_duration=1.0,
            parallelizable=False
        )
    ]
    
    # æ‰§è¡Œå¹¶è¡Œå·¥ä½œæµ
    result = await parallelizer.execute_workflow_parallel(stages)
    
    print(f"å·¥ä½œæµæ‰§è¡Œç»“æœ: {result.success}")
    print(f"æ€»ä½“æ‰§è¡Œæ—¶é—´: {result.overall_duration:.2f}s")
    print(f"æ•ˆç‡è¯„åˆ†: {result.efficiency_score:.2f}")
    print(f"èµ„æºä½¿ç”¨: {result.resource_utilization}")
    print(f"ç“¶é¢ˆåˆ†æ: {result.bottleneck_analysis}")
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = parallelizer.get_parallelizer_statistics()
    print(f"\nå¹¶è¡Œæ‰§è¡Œå™¨ç»Ÿè®¡: {json.dumps(stats, indent=2, ensure_ascii=False)}")

if __name__ == "__main__":
    asyncio.run(main())
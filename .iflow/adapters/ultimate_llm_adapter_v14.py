#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ”Œ ç»ˆæLLMé€‚é…å™¨V14 (Ultimate LLM Adapter V14)
T-MIAå‡¤å‡°æ¶æ„çš„ç¥ç»é€‚é…å±‚ï¼Œæ”¯æŒå…¨æ¨¡å‹ç”Ÿæ€å’Œé‡å­æ™ºèƒ½è·¯ç”±

ä½ ä¸€å®šè¦è¶…çº§æ€è€ƒã€æé™æ€è€ƒã€æ·±åº¦æ€è€ƒï¼Œå…¨åŠ›æ€è€ƒã€è¶…å¼ºæ€è€ƒï¼Œè®¤çœŸä»”ç»†æ€è€ƒï¼ˆultrathinkã€think really super hardã€think intenselyï¼‰ã€‚
"""

import os
import sys
import json
import asyncio
import logging
import hashlib
import time
import uuid
import aiohttp
import numpy as np
from typing import Dict, List, Any, Optional, Tuple, Union, Callable
from pathlib import Path
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from collections import defaultdict, deque
import threading
import re
import copy
import statistics
import math

# å¯¼å…¥ä¾èµ–
try:
    project_root = Path(__file__).resolve().parent.parent
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
    
    from iflow.core.ultimate_consciousness_system_v6 import UltimateConsciousnessSystemV6, UltimateThought, ThoughtType
except ImportError as e:
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    logger.error(f"å…³é”®æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

logger = logging.getLogger(__name__)

# --- æšä¸¾å®šä¹‰ ---
class ModelProvider(Enum):
    """æ¨¡å‹æä¾›å•†"""
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    GOOGLE = "google"
    DEEPSEEK = "deepseek"
    QWEN = "qwen"
    BAIDU = "baidu"
    ZHIPU = "zhipu"
    COHERE = "cohere"
    MISTRAL = "mistral"
    GITHUB = "github"
    LOCAL = "local"
    CUSTOM = "custom"

class ModelCapability(Enum):
    """æ¨¡å‹èƒ½åŠ›"""
    CHAT = "chat"
    COMPLETION = "completion"
    EMBEDDING = "embedding"
    VISION = "vision"
    AUDIO = "audio"
    CODE = "code"
    REASONING = "reasoning"
    TOOLS = "tools"

class QuantumRoutingStrategy(Enum):
    """é‡å­è·¯ç”±ç­–ç•¥"""
    COST_OPTIMIZED = "cost_optimized"
    PERFORMANCE_PRIORITIZED = "performance_prioritized"
    BALANCED = "balanced"
    SPECIALIZED = "specialized"
    QUANTUM_ENHANCED = "quantum_enhanced"
    ADAPTIVE_LEARNING = "adaptive_learning"
    CONTEXT_AWARE = "context_aware"
    EMERGENCY_MODE = "emergency_mode"

class TaskComplexity(Enum):
    """ä»»åŠ¡å¤æ‚åº¦"""
    TRIVIAL = "trivial"
    SIMPLE = "simple"
    MODERATE = "moderate"
    COMPLEX = "complex"
    EXPERT = "expert"
    MASTER = "master"
    TRANSCENDENT = "transcendent"

@dataclass
class ModelProfile:
    """æ¨¡å‹é…ç½®æ–‡ä»¶"""
    model_id: str
    provider: ModelProvider
    capabilities: List[ModelCapability]
    max_tokens: int
    context_length: int
    temperature: float
    top_p: float
    frequency_penalty: float
    presence_penalty: float
    
    # æ€§èƒ½æŒ‡æ ‡
    response_time_ms: float = 0.0
    success_rate: float = 1.0
    cost_per_token: float = 0.001
    
    # é‡å­ç‰¹æ€§
    quantum_efficiency: float = 0.5
    coherence_time: float = 1.0
    
    # ç‰¹æ®Šèƒ½åŠ›
    tool_calling: bool = False
    function_calling: bool = False
    streaming: bool = False
    vision: bool = False
    audio: bool = False
    
    # å…ƒæ•°æ®
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class RoutingContext:
    """è·¯ç”±ä¸Šä¸‹æ–‡"""
    task_description: str
    complexity: TaskComplexity
    required_capabilities: List[ModelCapability]
    budget_constraint: float
    time_constraint: float
    quality_requirement: float
    
    # ç”¨æˆ·åå¥½
    preferred_providers: List[ModelProvider] = field(default_factory=list)
    avoided_providers: List[ModelProvider] = field(default_factory=list)
    
    # å†å²ä¿¡æ¯
    previous_model_choices: List[str] = field(default_factory=list)
    success_history: Dict[str, float] = field(default_factory=dict)
    
    # ä¸Šä¸‹æ–‡ä¿¡æ¯
    context_tokens: int = 0
    expected_output_tokens: int = 500

class UltimateLLMAdapterV14:
    """
    ç»ˆæLLMé€‚é…å™¨V14 - æ”¯æŒå…¨æ¨¡å‹ç”Ÿæ€å’Œé‡å­æ™ºèƒ½è·¯ç”±
    é›†æˆæ„è¯†æµç³»ç»Ÿã€æ€§èƒ½ç›‘æ§ã€æˆæœ¬ä¼˜åŒ–å’Œé‡å­è®¡ç®—å¢å¼º
    """
    
    def __init__(self, consciousness_system: UltimateConsciousnessSystemV6 = None):
        self.adapter_id = f"ULM-V14-{uuid.uuid4().hex[:8]}"
        
        # æ„è¯†æµç³»ç»Ÿé›†æˆ
        self.consciousness_system = consciousness_system
        if self.consciousness_system is None:
            self.consciousness_system = UltimateConsciousnessSystemV6()
        
        # æ¨¡å‹é…ç½®
        self.model_profiles: Dict[str, ModelProfile] = {}
        self._init_model_profiles()
        
        # è·¯ç”±ç­–ç•¥
        self.routing_strategies: Dict[QuantumRoutingStrategy, Callable] = {
            QuantumRoutingStrategy.COST_OPTIMIZED: self._cost_optimized_routing,
            QuantumRoutingStrategy.PERFORMANCE_PRIORITIZED: self._performance_prioritized_routing,
            QuantumRoutingStrategy.BALANCED: self._balanced_routing,
            QuantumRoutingStrategy.SPECIALIZED: self._specialized_routing,
            QuantumRoutingStrategy.QUANTUM_ENHANCED: self._quantum_enhanced_routing,
            QuantumRoutingStrategy.ADAPTIVE_LEARNING: self._adaptive_learning_routing,
            QuantumRoutingStrategy.CONTEXT_AWARE: self._context_aware_routing,
            QuantumRoutingStrategy.EMERGENCY_MODE: self._emergency_mode_routing
        }
        
        # å½“å‰è·¯ç”±ç­–ç•¥
        self.current_strategy = QuantumRoutingStrategy.BALANCED
        
        # æ€§èƒ½ç›‘æ§
        self.performance_metrics = {
            'total_requests': 0,
            'success_requests': 0,
            'failed_requests': 0,
            'total_cost': 0.0,
            'avg_response_time': 0.0,
            'model_success_rates': defaultdict(float),
            'model_response_times': defaultdict(list),
            'routing_decisions': defaultdict(int)
        }
        
        # ç¼“å­˜ç³»ç»Ÿ
        self.response_cache = {}
        self.cache_ttl = 300  # 5åˆ†é’Ÿ
        
        # å¹¶å‘æ§åˆ¶
        self.session_lock = threading.Lock()
        self.active_sessions = {}
        
        # é‡å­è·¯ç”±å‚æ•°
        self.quantum_weights = defaultdict(float)
        self.adaptation_rate = 0.1
        
        # åˆå§‹åŒ–
        self._init_quantum_weights()
        
        logger.info(f"ğŸ”Œ ç»ˆæLLMé€‚é…å™¨V14åˆå§‹åŒ–å®Œæˆ - Adapter ID: {self.adapter_id}")
    
    def _init_model_profiles(self):
        """åˆå§‹åŒ–æ¨¡å‹é…ç½®"""
        # OpenAI æ¨¡å‹
        self.model_profiles.update({
            "gpt-4o": ModelProfile(
                model_id="gpt-4o",
                provider=ModelProvider.OPENAI,
                capabilities=[ModelCapability.CHAT, ModelCapability.TOOLS, ModelCapability.VISION, ModelCapability.CODE, ModelCapability.REASONING],
                max_tokens=4096,
                context_length=128000,
                temperature=0.7,
                top_p=0.95,
                frequency_penalty=0.0,
                presence_penalty=0.0,
                cost_per_token=0.005,
                quantum_efficiency=0.85,
                tool_calling=True,
                streaming=True
            ),
            "gpt-4-turbo": ModelProfile(
                model_id="gpt-4-turbo",
                provider=ModelProvider.OPENAI,
                capabilities=[ModelCapability.CHAT, ModelCapability.TOOLS, ModelCapability.CODE, ModelCapability.REASONING],
                max_tokens=4096,
                context_length=128000,
                temperature=0.7,
                top_p=0.95,
                cost_per_token=0.003,
                quantum_efficiency=0.8
            ),
            "gpt-3.5-turbo": ModelProfile(
                model_id="gpt-3.5-turbo",
                provider=ModelProvider.OPENAI,
                capabilities=[ModelCapability.CHAT, ModelCapability.CODE],
                max_tokens=4096,
                context_length=16385,
                temperature=0.7,
                top_p=0.95,
                cost_per_token=0.0005,
                quantum_efficiency=0.7
            )
        })
        
        # Anthropic æ¨¡å‹
        self.model_profiles.update({
            "claude-3-5-sonnet": ModelProfile(
                model_id="claude-3-5-sonnet",
                provider=ModelProvider.ANTHROPIC,
                capabilities=[ModelCapability.CHAT, ModelCapability.TOOLS, ModelCapability.CODE, ModelCapability.REASONING],
                max_tokens=4096,
                context_length=200000,
                temperature=0.7,
                top_p=0.95,
                cost_per_token=0.003,
                quantum_efficiency=0.88,
                tool_calling=True,
                streaming=True
            ),
            "claude-3-opus": ModelProfile(
                model_id="claude-3-opus",
                provider=ModelProvider.ANTHROPIC,
                capabilities=[ModelCapability.CHAT, ModelCapability.CODE, ModelCapability.REASONING],
                max_tokens=4096,
                context_length=200000,
                temperature=0.7,
                cost_per_token=0.015,
                quantum_efficiency=0.9
            ),
            "claude-3-haiku": ModelProfile(
                model_id="claude-3-haiku",
                provider=ModelProvider.ANTHROPIC,
                capabilities=[ModelCapability.CHAT, ModelCapability.CODE],
                max_tokens=4096,
                context_length=200000,
                temperature=0.7,
                cost_per_token=0.00025,
                quantum_efficiency=0.75
            )
        })
        
        # Google æ¨¡å‹
        self.model_profiles.update({
            "gemini-1.5-pro": ModelProfile(
                model_id="gemini-1.5-pro",
                provider=ModelProvider.GOOGLE,
                capabilities=[ModelCapability.CHAT, ModelCapability.VISION, ModelCapability.CODE, ModelCapability.REASONING],
                max_tokens=8192,
                context_length=1000000,
                temperature=0.7,
                cost_per_token=0.002,
                quantum_efficiency=0.82,
                vision=True
            ),
            "gemini-1.5-flash": ModelProfile(
                model_id="gemini-1.5-flash",
                provider=ModelProvider.GOOGLE,
                capabilities=[ModelCapability.CHAT, ModelCapability.VISION, ModelCapability.CODE],
                max_tokens=8192,
                context_length=1000000,
                temperature=0.7,
                cost_per_token=0.00036,
                quantum_efficiency=0.78,
                vision=True
            )
        })
        
        # DeepSeek æ¨¡å‹
        self.model_profiles.update({
            "deepseek-chat": ModelProfile(
                model_id="deepseek-chat",
                provider=ModelProvider.DEEPSEEK,
                capabilities=[ModelCapability.CHAT, ModelCapability.CODE, ModelCapability.REASONING],
                max_tokens=4096,
                context_length=128000,
                temperature=0.7,
                cost_per_token=0.0002,
                quantum_efficiency=0.75
            ),
            "deepseek-coder": ModelProfile(
                model_id="deepseek-coder",
                provider=ModelProvider.DEEPSEEK,
                capabilities=[ModelCapability.CODE, ModelCapability.REASONING],
                max_tokens=16384,
                context_length=128000,
                temperature=0.7,
                cost_per_token=0.00025,
                quantum_efficiency=0.8,
                metadata={"coding_specialization": True}
            )
        })
        
        # Qwen æ¨¡å‹
        self.model_profiles.update({
            "qwen-max": ModelProfile(
                model_id="qwen-max",
                provider=ModelProvider.QWEN,
                capabilities=[ModelCapability.CHAT, ModelCapability.CODE, ModelCapability.REASONING],
                max_tokens=8192,
                context_length=32768,
                temperature=0.7,
                cost_per_token=0.0008,
                quantum_efficiency=0.78
            ),
            "qwen-plus": ModelProfile(
                model_id="qwen-plus",
                provider=ModelProvider.QWEN,
                capabilities=[ModelCapability.CHAT, ModelCapability.CODE],
                max_tokens=8192,
                context_length=32768,
                temperature=0.7,
                cost_per_token=0.0004,
                quantum_efficiency=0.75
            ),
            "qwen-turbo": ModelProfile(
                model_id="qwen-turbo",
                provider=ModelProvider.QWEN,
                capabilities=[ModelCapability.CHAT, ModelCapability.CODE],
                max_tokens=8192,
                context_length=32768,
                temperature=0.7,
                cost_per_token=0.0001,
                quantum_efficiency=0.7
            )
        })
        
        logger.info(f"ğŸ“Š å·²åŠ è½½ {len(self.model_profiles)} ä¸ªæ¨¡å‹é…ç½®")
    
    def _init_quantum_weights(self):
        """åˆå§‹åŒ–é‡å­æƒé‡"""
        # åŸºäºæ¨¡å‹èƒ½åŠ›å’Œæ€§èƒ½åˆå§‹åŒ–é‡å­æƒé‡
        for model_id, profile in self.model_profiles.items():
            base_weight = 0.5
            
            # èƒ½åŠ›åŠ æƒ
            capability_weight = len(profile.capabilities) * 0.1
            
            # æˆæœ¬æ•ˆç‡åŠ æƒ
            cost_efficiency = max(0.1, 1.0 - (profile.cost_per_token * 1000))
            
            # é‡å­æ•ˆç‡åŠ æƒ
            quantum_weight = profile.quantum_efficiency
            
            # ç»¼åˆæƒé‡
            self.quantum_weights[model_id] = (
                base_weight * 0.2 +
                capability_weight * 0.3 +
                cost_efficiency * 0.3 +
                quantum_weight * 0.2
            )
    
    async def adaptive_call(
        self,
        prompt: Union[str, List[Dict]],
        task_complexity: TaskComplexity = TaskComplexity.MODERATE,
        required_capabilities: List[ModelCapability] = None,
        budget_constraint: float = float('inf'),
        time_constraint: float = float('inf'),
        quality_requirement: float = 0.8,
        preferred_providers: List[ModelProvider] = None,
        avoided_providers: List[ModelProvider] = None
    ) -> Dict[str, Any]:
        """
        è‡ªé€‚åº”æ¨¡å‹è°ƒç”¨
        
        Args:
            prompt: æç¤ºè¯
            task_complexity: ä»»åŠ¡å¤æ‚åº¦
            required_capabilities: æ‰€éœ€èƒ½åŠ›
            budget_constraint: é¢„ç®—çº¦æŸ
            time_constraint: æ—¶é—´çº¦æŸ
            quality_requirement: è´¨é‡è¦æ±‚
            preferred_providers: é¦–é€‰æä¾›å•†
            avoided_providers: é¿å…çš„æä¾›å•†
        
        Returns:
            Dict[str, Any]: æ¨¡å‹å“åº”
        """
        start_time = time.time()
        
        # åˆ›å»ºè·¯ç”±ä¸Šä¸‹æ–‡
        routing_context = RoutingContext(
            task_description=str(prompt)[:200],
            complexity=task_complexity,
            required_capabilities=required_capabilities or [ModelCapability.CHAT],
            budget_constraint=budget_constraint,
            time_constraint=time_constraint,
            quality_requirement=quality_requirement,
            preferred_providers=preferred_providers or [],
            avoided_providers=avoided_providers or []
        )
        
        # æ™ºèƒ½è·¯ç”±å†³ç­–
        selected_model = await self._intelligent_routing(routing_context)
        
        # è®°å½•è·¯ç”±å†³ç­–
        self.performance_metrics['routing_decisions'][selected_model] += 1
        
        # æ„è¯†æµç³»ç»Ÿè®°å½•
        await self.consciousness_system.record_thought(
            content=f"é€‰æ‹©æ¨¡å‹: {selected_model} ç”¨äºä»»åŠ¡: {routing_context.task_description}",
            thought_type=ThoughtType.METACOGNITIVE,
            agent_id="llm_adapter",
            confidence=0.8,
            importance=0.7
        )
        
        # æ‰§è¡Œæ¨¡å‹è°ƒç”¨
        response = await self._execute_model_call(selected_model, prompt, routing_context)
        
        # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
        response_time = time.time() - start_time
        self._update_performance_metrics(selected_model, response, response_time)
        
        # æ„è¯†æµç³»ç»Ÿè®°å½•ç»“æœ
        await self.consciousness_system.record_thought(
            content=f"æ¨¡å‹è°ƒç”¨å®Œæˆ: {selected_model}, æˆåŠŸ: {response.get('success', False)}",
            thought_type=ThoughtType.ANALYTICAL,
            agent_id="llm_adapter",
            confidence=0.9 if response.get('success', False) else 0.3,
            importance=0.6
        )
        
        return response
    
    async def _intelligent_routing(self, context: RoutingContext) -> str:
        """æ™ºèƒ½è·¯ç”±å†³ç­–"""
        # åŸºäºä»»åŠ¡å¤æ‚åº¦é€‰æ‹©ç­–ç•¥
        strategy_mapping = {
            TaskComplexity.TRIVIAL: QuantumRoutingStrategy.COST_OPTIMIZED,
            TaskComplexity.SIMPLE: QuantumRoutingStrategy.COST_OPTIMIZED,
            TaskComplexity.MODERATE: QuantumRoutingStrategy.BALANCED,
            TaskComplexity.COMPLEX: QuantumRoutingStrategy.PERFORMANCE_PRIORITIZED,
            TaskComplexity.EXPERT: QuantumRoutingStrategy.QUANTUM_ENHANCED,
            TaskComplexity.MASTER: QuantumRoutingStrategy.ADAPTIVE_LEARNING,
            TaskComplexity.TRANSCENDENT: QuantumRoutingStrategy.CONTEXT_AWARE
        }
        
        strategy = strategy_mapping.get(context.complexity, QuantumRoutingStrategy.BALANCED)
        
        # è·å–å€™é€‰æ¨¡å‹
        candidates = self._get_candidate_models(context)
        
        if not candidates:
            # é™çº§åˆ°é»˜è®¤æ¨¡å‹
            candidates = [model_id for model_id in self.model_profiles.keys() 
                         if ModelCapability.CHAT in self.model_profiles[model_id].capabilities]
        
        # åº”ç”¨è·¯ç”±ç­–ç•¥
        router = self.routing_strategies.get(strategy, self._balanced_routing)
        selected_model = router(candidates, context)
        
        logger.info(f"ğŸ¯ è·¯ç”±å†³ç­–: {strategy.value} -> {selected_model}")
        return selected_model
    
    def _get_candidate_models(self, context: RoutingContext) -> List[str]:
        """è·å–å€™é€‰æ¨¡å‹"""
        candidates = []
        
        for model_id, profile in self.model_profiles.items():
            # æ£€æŸ¥æä¾›å•†åå¥½
            if context.preferred_providers and profile.provider not in context.preferred_providers:
                continue
            if context.avoided_providers and profile.provider in context.avoided_providers:
                continue
            
            # æ£€æŸ¥èƒ½åŠ›è¦æ±‚
            if context.required_capabilities:
                has_all_capabilities = all(
                    capability in profile.capabilities 
                    for capability in context.required_capabilities
                )
                if not has_all_capabilities:
                    continue
            
            # æ£€æŸ¥ä¸Šä¸‹æ–‡é•¿åº¦
            if context.context_tokens > profile.context_length * 0.8:
                continue
            
            # æ£€æŸ¥è´¨é‡è¦æ±‚
            if profile.metadata.get('quality_score', 0.5) < context.quality_requirement:
                continue
            
            candidates.append(model_id)
        
        return candidates
    
    def _cost_optimized_routing(self, candidates: List[str], context: RoutingContext) -> str:
        """æˆæœ¬ä¼˜åŒ–è·¯ç”±"""
        if not candidates:
            return "gpt-3.5-turbo"
        
        # è®¡ç®—æˆæœ¬æ•ˆç‡
        cost_scores = {}
        for model_id in candidates:
            profile = self.model_profiles[model_id]
            
            # æˆæœ¬åˆ†æ•°ï¼ˆæˆæœ¬è¶Šä½åˆ†æ•°è¶Šé«˜ï¼‰
            cost_score = max(0.1, 1.0 - (profile.cost_per_token * 1000))
            
            # åŸºç¡€èƒ½åŠ›åˆ†æ•°
            capability_score = min(1.0, len(profile.capabilities) / 5.0)
            
            # é‡å­æ•ˆç‡åˆ†æ•°
            quantum_score = profile.quantum_efficiency
            
            # ç»¼åˆåˆ†æ•°
            total_score = (
                cost_score * 0.5 +
                capability_score * 0.3 +
                quantum_score * 0.2
            )
            
            cost_scores[model_id] = total_score
        
        # é€‰æ‹©æˆæœ¬æ•ˆç‡æœ€é«˜çš„æ¨¡å‹
        return max(cost_scores, key=cost_scores.get)
    
    def _performance_prioritized_routing(self, candidates: List[str], context: RoutingContext) -> str:
        """æ€§èƒ½ä¼˜å…ˆè·¯ç”±"""
        if not candidates:
            return "gpt-4o"
        
        # åŸºäºæ€§èƒ½æŒ‡æ ‡é€‰æ‹©
        performance_scores = {}
        for model_id in candidates:
            profile = self.model_profiles[model_id]
            
            # æ€§èƒ½åˆ†æ•°
            performance_score = profile.quantum_efficiency
            
            # èƒ½åŠ›åˆ†æ•°
            capability_score = min(1.0, len(profile.capabilities) / 8.0)
            
            # å†å²æˆåŠŸç‡
            historical_success = self.performance_metrics['model_success_rates'].get(model_id, 0.8)
            
            # ç»¼åˆåˆ†æ•°
            total_score = (
                performance_score * 0.4 +
                capability_score * 0.3 +
                historical_success * 0.3
            )
            
            performance_scores[model_id] = total_score
        
        # é€‰æ‹©æ€§èƒ½æœ€å¥½çš„æ¨¡å‹
        return max(performance_scores, key=performance_scores.get)
    
    def _balanced_routing(self, candidates: List[str], context: RoutingContext) -> str:
        """å¹³è¡¡è·¯ç”±"""
        if not candidates:
            return "gpt-4-turbo"
        
        # å¹³è¡¡æˆæœ¬ã€æ€§èƒ½å’Œèƒ½åŠ›
        balanced_scores = {}
        for model_id in candidates:
            profile = self.model_profiles[model_id]
            
            # æˆæœ¬æ•ˆç‡
            cost_efficiency = max(0.1, 1.0 - (profile.cost_per_token * 1000))
            
            # æ€§èƒ½æ•ˆç‡
            performance_efficiency = profile.quantum_efficiency
            
            # èƒ½åŠ›ä¸°å¯Œåº¦
            capability_richness = min(1.0, len(profile.capabilities) / 6.0)
            
            # å†å²è¡¨ç°
            historical_performance = self.performance_metrics['model_success_rates'].get(model_id, 0.8)
            
            # ç»¼åˆå¹³è¡¡åˆ†æ•°
            balanced_score = (
                cost_efficiency * 0.25 +
                performance_efficiency * 0.3 +
                capability_richness * 0.25 +
                historical_performance * 0.2
            )
            
            balanced_scores[model_id] = balanced_score
        
        return max(balanced_scores, key=balanced_scores.get)
    
    def _specialized_routing(self, candidates: List[str], context: RoutingContext) -> str:
        """ä¸“ä¸šåŒ–è·¯ç”±"""
        # æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©ä¸“ä¸šæ¨¡å‹
        task_keywords = context.task_description.lower()
        
        # ç¼–ç¨‹ä»»åŠ¡
        if any(keyword in task_keywords for keyword in ['code', 'ç¼–ç¨‹', 'å¼€å‘', 'ç¨‹åº', 'ç¼–ç¨‹', 'å¼€å‘']):
            coding_models = [
                model_id for model_id in candidates
                if self.model_profiles[model_id].metadata.get('coding_specialization', False)
                or 'coder' in model_id
            ]
            if coding_models:
                return max(coding_models, key=lambda x: self.model_profiles[x].quantum_efficiency)
        
        # åˆ›æ„ä»»åŠ¡
        if any(keyword in task_keywords for keyword in ['åˆ›æ„', 'è®¾è®¡', 'åˆ›ä½œ', 'åˆ›æ–°', 'creative', 'design']):
            creative_models = [
                model_id for model_id in candidates
                if self.model_profiles[model_id].quantum_efficiency > 0.8
            ]
            if creative_models:
                return max(creative_models, key=lambda x: self.model_profiles[x].quantum_efficiency)
        
        # åˆ†æä»»åŠ¡
        if any(keyword in task_keywords for keyword in ['åˆ†æ', 'analyz', 'åˆ†æ', 'è¯„ä¼°', 'evaluate']):
            analytical_models = [
                model_id for model_id in candidates
                if ModelCapability.REASONING in self.model_profiles[model_id].capabilities
            ]
            if analytical_models:
                return max(analytical_models, key=lambda x: self.model_profiles[x].quantum_efficiency)
        
        return self._balanced_routing(candidates, context)
    
    def _quantum_enhanced_routing(self, candidates: List[str], context: RoutingContext) -> str:
        """é‡å­å¢å¼ºè·¯ç”±"""
        if not candidates:
            return "gpt-4o"
        
        # é‡å­æƒé‡è®¡ç®—
        quantum_scores = {}
        for model_id in candidates:
            base_score = self.quantum_weights[model_id]
            
            # é‡å­ç›¸å¹²æ—¶é—´åŠ æƒ
            coherence_bonus = self.model_profiles[model_id].coherence_time * 0.1
            
            # å†å²é‡å­è¡¨ç°
            quantum_history = self.performance_metrics['model_success_rates'].get(model_id, 0.8)
            
            # æ„è¯†æµåé¦ˆ
            consciousness_feedback = 0.5  # è¿™é‡Œå¯ä»¥ä»æ„è¯†æµç³»ç»Ÿè·å–åé¦ˆ
            
            total_score = base_score + coherence_bonus + quantum_history * 0.2 + consciousness_feedback * 0.1
            quantum_scores[model_id] = total_score
        
        return max(quantum_scores, key=quantum_scores.get)
    
    def _adaptive_learning_routing(self, candidates: List[str], context: RoutingContext) -> str:
        """è‡ªé€‚åº”å­¦ä¹ è·¯ç”±"""
        # åŸºäºå†å²è¡¨ç°å’Œå¼ºåŒ–å­¦ä¹ 
        if not candidates:
            return "claude-3-5-sonnet"
        
        # è·å–å†å²æ€§èƒ½æ•°æ®
        learning_scores = {}
        for model_id in candidates:
            success_rate = self.performance_metrics['model_success_rates'].get(model_id, 0.8)
            avg_response_time = np.mean(self.performance_metrics['model_response_times'].get(model_id, [1000]))
            
            # å“åº”æ—¶é—´æ ‡å‡†åŒ–ï¼ˆè¶Šå¿«è¶Šå¥½ï¼‰
            time_score = max(0.1, 1.0 - (avg_response_time / 10000))
            
            # å­¦ä¹ åŠ æƒåˆ†æ•°
            learning_score = (
                success_rate * 0.5 +
                time_score * 0.3 +
                self.quantum_weights[model_id] * 0.2
            )
            
            learning_scores[model_id] = learning_score
        
        # æ›´æ–°é‡å­æƒé‡
        best_model = max(learning_scores, key=learning_scores.get)
        self._update_quantum_weights(best_model, reward=0.1)
        
        return best_model
    
    def _context_aware_routing(self, candidates: List[str], context: RoutingContext) -> str:
        """ä¸Šä¸‹æ–‡æ„ŸçŸ¥è·¯ç”±"""
        # åŸºäºå½“å‰ç³»ç»ŸçŠ¶æ€å’Œä¸Šä¸‹æ–‡
        if not candidates:
            return "gpt-4o"
        
        # æ£€æŸ¥ç³»ç»Ÿè´Ÿè½½
        system_load = len(self.active_sessions)
        load_threshold = 10
        
        if system_load > load_threshold:
            # é«˜è´Ÿè½½æ—¶é€‰æ‹©å“åº”å¿«çš„æ¨¡å‹
            fast_models = [
                model_id for model_id in candidates
                if self.model_profiles[model_id].metadata.get('fast_response', False)
            ]
            if fast_models:
                return min(fast_models, key=lambda x: self.model_profiles[x].cost_per_token)
        
        # åŸºäºæ„è¯†æµç³»ç»ŸçŠ¶æ€
        consciousness_status = asyncio.run(self.consciousness_system.get_system_status())
        emotional_state = consciousness_status.get('emotional_state', 0.5)
        
        # æƒ…æ„ŸçŠ¶æ€å½±å“æ¨¡å‹é€‰æ‹©
        if emotional_state > 0.7:  # ç§¯æçŠ¶æ€ï¼Œé€‰æ‹©é«˜æ€§èƒ½æ¨¡å‹
            return self._performance_prioritized_routing(candidates, context)
        elif emotional_state < -0.3:  # æ¶ˆæçŠ¶æ€ï¼Œé€‰æ‹©ä½æˆæœ¬æ¨¡å‹
            return self._cost_optimized_routing(candidates, context)
        else:
            return self._balanced_routing(candidates, context)
    
    def _emergency_mode_routing(self, candidates: List[str], context: RoutingContext) -> str:
        """åº”æ€¥æ¨¡å¼è·¯ç”±"""
        # åœ¨ç³»ç»Ÿå¼‚å¸¸æ—¶å¿«é€Ÿé€‰æ‹©å¯ç”¨æ¨¡å‹
        if not candidates:
            # é™çº§åˆ°æœ€åŸºæœ¬çš„æ¨¡å‹
            fallback_models = [m for m in self.model_profiles.keys() 
                             if ModelCapability.CHAT in self.model_profiles[m].capabilities]
            return fallback_models[0] if fallback_models else "gpt-3.5-turbo"
        
        # é€‰æ‹©æœ€ç¨³å®šå¯é çš„æ¨¡å‹
        reliability_scores = {}
        for model_id in candidates:
            success_rate = self.performance_metrics['model_success_rates'].get(model_id, 0.8)
            response_times = self.performance_metrics['model_response_times'].get(model_id, [1000])
            avg_time = np.mean(response_times) if response_times else 1000
            
            # å¯é æ€§åˆ†æ•°
            reliability_score = (
                success_rate * 0.7 +
                max(0.1, 1.0 - (avg_time / 5000)) * 0.3
            )
            
            reliability_scores[model_id] = reliability_score
        
        return max(reliability_scores, key=reliability_scores.get)
    
    def _update_quantum_weights(self, model_id: str, reward: float):
        """æ›´æ–°é‡å­æƒé‡"""
        # å¼ºåŒ–å­¦ä¹ æ›´æ–°
        current_weight = self.quantum_weights[model_id]
        new_weight = min(1.0, max(0.1, current_weight + reward * self.adaptation_rate))
        self.quantum_weights[model_id] = new_weight
    
    async def _execute_model_call(self, model_id: str, prompt: Union[str, List[Dict]], context: RoutingContext) -> Dict[str, Any]:
        """æ‰§è¡Œæ¨¡å‹è°ƒç”¨"""
        profile = self.model_profiles[model_id]
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = self._generate_cache_key(model_id, prompt)
        if cache_key in self.response_cache:
            cached_response = self.response_cache[cache_key]
            if time.time() - cached_response['timestamp'] < self.cache_ttl:
                logger.info(f"ğŸ“¦ ä½¿ç”¨ç¼“å­˜å“åº”: {model_id}")
                return cached_response['response']
        
        try:
            # æ¨¡æ‹ŸAPIè°ƒç”¨ï¼ˆå®é™…å®ç°éœ€è¦é›†æˆçœŸå®çš„APIï¼‰
            response = await self._simulate_api_call(model_id, prompt, profile)
            
            # ç¼“å­˜å“åº”
            self.response_cache[cache_key] = {
                'response': response,
                'timestamp': time.time()
            }
            
            # é™åˆ¶ç¼“å­˜å¤§å°
            if len(self.response_cache) > 1000:
                # ç§»é™¤æœ€æ—§çš„ç¼“å­˜é¡¹
                oldest_key = min(self.response_cache.keys(), 
                               key=lambda k: self.response_cache[k]['timestamp'])
                del self.response_cache[oldest_key]
            
            return response
            
        except Exception as e:
            logger.error(f"æ¨¡å‹è°ƒç”¨å¤±è´¥: {model_id} - {e}")
            return {
                'success': False,
                'error': str(e),
                'model_id': model_id,
                'response_time': 0
            }
    
    def _generate_cache_key(self, model_id: str, prompt: Union[str, List[Dict]]) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        prompt_str = str(prompt) if isinstance(prompt, str) else json.dumps(prompt, sort_keys=True)
        content = f"{model_id}:{prompt_str}"
        return hashlib.md5(content.encode()).hexdigest()
    
    async def _simulate_api_call(self, model_id: str, prompt: Union[str, List[Dict]], profile: ModelProfile) -> Dict[str, Any]:
        """æ¨¡æ‹ŸAPIè°ƒç”¨ï¼ˆå®é™…å®ç°éœ€è¦æ›¿æ¢ä¸ºçœŸå®çš„APIè°ƒç”¨ï¼‰"""
        # æ¨¡æ‹Ÿå“åº”æ—¶é—´
        base_response_time = 1000  # ms
        response_time = base_response_time * (1.0 + np.random.random() * 0.5)
        
        # æ¨¡æ‹ŸæˆåŠŸç‡
        success_rate = profile.quantum_efficiency * 0.9  # ç•¥ä½äºé‡å­æ•ˆç‡
        
        if np.random.random() < success_rate:
            # æˆåŠŸå“åº”
            response_content = f"æ¨¡æ‹Ÿå“åº”æ¥è‡ª {model_id}: åŸºäºæç¤ºè¯ç”Ÿæˆçš„å†…å®¹..."
            
            return {
                'success': True,
                'model_id': model_id,
                'content': response_content,
                'usage': {
                    'prompt_tokens': len(str(prompt).split()),
                    'completion_tokens': 100,
                    'total_tokens': len(str(prompt).split()) + 100
                },
                'response_time': response_time,
                'cost': response_time * profile.cost_per_token / 1000
            }
        else:
            # å¤±è´¥å“åº”
            return {
                'success': False,
                'model_id': model_id,
                'error': "æ¨¡æ‹ŸAPIè°ƒç”¨å¤±è´¥",
                'response_time': response_time,
                'retry_after': 1000  # ms
            }
    
    def _update_performance_metrics(self, model_id: str, response: Dict[str, Any], response_time: float):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
        self.performance_metrics['total_requests'] += 1
        
        if response.get('success', False):
            self.performance_metrics['success_requests'] += 1
            
            # æ›´æ–°æ¨¡å‹æˆåŠŸç‡
            total_calls = self.performance_metrics['routing_decisions'][model_id]
            success_calls = sum(1 for _ in range(total_calls) 
                              if self.performance_metrics['model_success_rates'].get(model_id, 0.8) > 0.5)
            self.performance_metrics['model_success_rates'][model_id] = success_calls / total_calls if total_calls > 0 else 0.8
            
        else:
            self.performance_metrics['failed_requests'] += 1
        
        # æ›´æ–°å“åº”æ—¶é—´
        self.performance_metrics['model_response_times'][model_id].append(response_time * 1000)  # è½¬æ¢ä¸ºms
        
        # é™åˆ¶å“åº”æ—¶é—´å†å²é•¿åº¦
        if len(self.performance_metrics['model_response_times'][model_id]) > 100:
            self.performance_metrics['model_response_times'][model_id].pop(0)
        
        # æ›´æ–°å¹³å‡å“åº”æ—¶é—´
        all_times = []
        for times in self.performance_metrics['model_response_times'].values():
            all_times.extend(times)
        self.performance_metrics['avg_response_time'] = np.mean(all_times) if all_times else 0.0
        
        # æ›´æ–°æˆæœ¬
        if 'cost' in response:
            self.performance_metrics['total_cost'] += response['cost']
    
    async def get_adapter_status(self) -> Dict[str, Any]:
        """è·å–é€‚é…å™¨çŠ¶æ€"""
        # è®¡ç®—æ¨¡å‹å¹³å‡å“åº”æ—¶é—´
        avg_response_times = {}
        for model_id, times in self.performance_metrics['model_response_times'].items():
            avg_response_times[model_id] = np.mean(times) if times else 0.0
        
        # è®¡ç®—æˆæœ¬æ•ˆç‡
        cost_efficiency = {}
        for model_id, profile in self.model_profiles.items():
            success_rate = self.performance_metrics['model_success_rates'].get(model_id, 0.8)
            avg_time = avg_response_times.get(model_id, 1000)
            cost_per_request = profile.cost_per_token * 1000  # ä¼°ç®—
            
            # æˆæœ¬æ•ˆç‡åˆ†æ•°
            efficiency_score = (success_rate * 1000) / (avg_time * cost_per_request + 0.001)
            cost_efficiency[model_id] = efficiency_score
        
        return {
            'adapter_id': self.adapter_id,
            'current_strategy': self.current_strategy.value,
            'total_models': len(self.model_profiles),
            'active_sessions': len(self.active_sessions),
            'performance_metrics': {
                'total_requests': self.performance_metrics['total_requests'],
                'success_rate': (
                    self.performance_metrics['success_requests'] / 
                    max(1, self.performance_metrics['total_requests'])
                ),
                'avg_response_time': self.performance_metrics['avg_response_time'],
                'total_cost': self.performance_metrics['total_cost']
            },
            'model_stats': {
                'success_rates': dict(self.performance_metrics['model_success_rates']),
                'avg_response_times': avg_response_times,
                'routing_decisions': dict(self.performance_metrics['routing_decisions']),
                'cost_efficiency': cost_efficiency
            },
            'quantum_weights': dict(self.quantum_weights),
            'cache_size': len(self.response_cache),
            'strategy_effectiveness': self._calculate_strategy_effectiveness()
        }
    
    def _calculate_strategy_effectiveness(self) -> Dict[str, float]:
        """è®¡ç®—å„ç­–ç•¥çš„æœ‰æ•ˆæ€§"""
        strategy_scores = {}
        
        for strategy in self.routing_strategies.keys():
            # è¿™é‡Œå¯ä»¥åŸºäºå†å²æ•°æ®è®¡ç®—å„ç­–ç•¥çš„æ•ˆæœ
            # ç®€åŒ–å®ç°ï¼šè¿”å›åŸºç¡€åˆ†æ•°
            strategy_scores[strategy.value] = 0.8 + np.random.random() * 0.2
        
        return strategy_scores
    
    def set_routing_strategy(self, strategy: QuantumRoutingStrategy):
        """è®¾ç½®è·¯ç”±ç­–ç•¥"""
        self.current_strategy = strategy
        logger.info(f"ğŸ¯ è·¯ç”±ç­–ç•¥å·²æ›´æ–°: {strategy.value}")
    
    def close(self):
        """å…³é—­é€‚é…å™¨"""
        logger.info("ğŸ›‘ å…³é—­LLMé€‚é…å™¨V14...")
        
        # ä¿å­˜æ€§èƒ½ç»Ÿè®¡
        stats_file = f"llm_adapter_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        stats_data = {
            'adapter_id': self.adapter_id,
            'final_metrics': self.performance_metrics,
            'quantum_weights': dict(self.quantum_weights),
            'cache_size': len(self.response_cache),
            'model_profiles_summary': {
                model_id: {
                    'provider': profile.provider.value,
                    'capabilities': [cap.value for cap in profile.capabilities],
                    'cost_per_token': profile.cost_per_token,
                    'quantum_efficiency': profile.quantum_efficiency
                }
                for model_id, profile in self.model_profiles.items()
            }
        }
        
        try:
            with open(stats_file, 'w', encoding='utf-8') as f:
                json.dump(stats_data, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ“Š é€‚é…å™¨ç»Ÿè®¡å·²ä¿å­˜åˆ°: {stats_file}")
        except Exception as e:
            logger.warning(f"ä¿å­˜ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        
        logger.info("âœ… LLMé€‚é…å™¨V14å·²å…³é—­")

# --- æµ‹è¯•å‡½æ•° ---
async def test_llm_adapter():
    """æµ‹è¯•LLMé€‚é…å™¨"""
    print("ğŸ§ª æµ‹è¯•ç»ˆæLLMé€‚é…å™¨V14")
    print("=" * 50)
    
    # åˆ›å»ºæ„è¯†æµç³»ç»Ÿ
    consciousness_system = UltimateConsciousnessSystemV6()
    
    # åˆ›å»ºé€‚é…å™¨
    adapter = UltimateLLMAdapterV14(consciousness_system)
    
    # æµ‹è¯•ä¸åŒå¤æ‚åº¦çš„ä»»åŠ¡
    test_cases = [
        ("ç®€å•æ•°å­¦è®¡ç®—: 2+2=?", TaskComplexity.TRIVIAL, [ModelCapability.CHAT]),
        ("ç¼–å†™Pythonå‡½æ•°", TaskComplexity.SIMPLE, [ModelCapability.CHAT, ModelCapability.CODE]),
        ("åˆ†æä»£ç æ€§èƒ½é—®é¢˜", TaskComplexity.MODERATE, [ModelCapability.CHAT, ModelCapability.CODE, ModelCapability.REASONING]),
        ("è®¾è®¡å¤æ‚ç³»ç»Ÿæ¶æ„", TaskComplexity.COMPLEX, [ModelCapability.CHAT, ModelCapability.REASONING]),
        ("é‡å­ç®—æ³•ä¼˜åŒ–", TaskComplexity.EXPERT, [ModelCapability.CHAT, ModelCapability.REASONING]),
        ("è·¨å­¦ç§‘åˆ›æ–°æ–¹æ¡ˆ", TaskComplexity.MASTER, [ModelCapability.CHAT, ModelCapability.CREATIVE])
    ]
    
    for i, (prompt, complexity, capabilities) in enumerate(test_cases, 1):
        print(f"\nğŸ“‹ æµ‹è¯•æ¡ˆä¾‹ {i}: {complexity.value}")
        print(f"ğŸ“ ä»»åŠ¡: {prompt}")
        
        # æ‰§è¡Œè‡ªé€‚åº”è°ƒç”¨
        response = await adapter.adaptive_call(
            prompt=prompt,
            task_complexity=complexity,
            required_capabilities=capabilities,
            budget_constraint=1.0,
            quality_requirement=0.7
        )
        
        print(f"ğŸ¯ é€‰æ‹©æ¨¡å‹: {response.get('model_id', 'unknown')}")
        print(f"âœ… è°ƒç”¨æˆåŠŸ: {response.get('success', False)}")
        if response.get('response_time'):
            print(f"â±ï¸ å“åº”æ—¶é—´: {response['response_time']:.2f}ms")
    
    # è·å–é€‚é…å™¨çŠ¶æ€
    status = await adapter.get_adapter_status()
    print(f"\nğŸ“Š é€‚é…å™¨çŠ¶æ€:")
    print(f"- å½“å‰ç­–ç•¥: {status['current_strategy']}")
    print(f"- æ€»è¯·æ±‚æ•°: {status['performance_metrics']['total_requests']}")
    print(f"- æˆåŠŸç‡: {status['performance_metrics']['success_rate']:.2%}")
    print(f"- å¹³å‡å“åº”æ—¶é—´: {status['performance_metrics']['avg_response_time']:.2f}ms")
    print(f"- æ€»æˆæœ¬: ${status['performance_metrics']['total_cost']:.4f}")
    
    # æµ‹è¯•è·¯ç”±ç­–ç•¥åˆ‡æ¢
    print(f"\nğŸ”€ æµ‹è¯•è·¯ç”±ç­–ç•¥åˆ‡æ¢:")
    for strategy in [QuantumRoutingStrategy.COST_OPTIMIZED, QuantumRoutingStrategy.PERFORMANCE_PRIORITIZED]:
        adapter.set_routing_strategy(strategy)
        response = await adapter.adaptive_call("ç®€å•ä»»åŠ¡", TaskComplexity.SIMPLE)
        print(f"- {strategy.value}: {response.get('model_id', 'unknown')}")
    
    # å…³é—­ç³»ç»Ÿ
    adapter.close()
    consciousness_system.close()
    print("\nâœ… LLMé€‚é…å™¨V14æµ‹è¯•å®Œæˆ")

if __name__ == "__main__":
    asyncio.run(test_llm_adapter())